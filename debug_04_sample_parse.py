#!/usr/bin/env python3
"""
ãƒ‡ãƒãƒƒã‚°ã‚¹ã‚¯ãƒªãƒ—ãƒˆ04: ã‚µãƒ³ãƒ—ãƒ«ãƒ‘ãƒ¼ã‚¹å‡¦ç†ï¼ˆ100ä»¶ãƒ†ã‚¹ãƒˆï¼‰
ãƒ‘ãƒ¼ã‚µãƒ¼ã®å‹•ä½œç¢ºèªã¨å•é¡Œç‚¹ã®æ—©æœŸç™ºè¦‹ã‚’ç›®çš„ã¨ã—ãŸè»½é‡ç‰ˆãƒ‘ãƒ¼ã‚¹å‡¦ç†
"""

import logging
import sqlite3
from pathlib import Path
import yaml
from datetime import datetime
import sys
import pandas as pd
import os

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’sys.pathã«è¿½åŠ 
project_root = Path(__file__).resolve().parent / "keibaai"
sys.path.insert(0, str(project_root))

from src import pipeline_core
from src.modules.parsers import results_parser, shutuba_parser, horse_info_parser, pedigree_parser

# ã‚µãƒ³ãƒ—ãƒ«ä»¶æ•°ï¼ˆãƒ†ã‚¹ãƒˆç”¨ã«å°‘æ•°ã«è¨­å®šï¼‰
SAMPLE_SIZE = 100

def load_config():
    """è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ã™ã‚‹"""
    config_path = Path("keibaai/configs")
    with open(config_path / "default.yaml", "r", encoding="utf-8") as f:
        default_cfg = yaml.safe_load(f)

    data_root = Path("keibaai") / default_cfg['data_path']
    default_cfg['raw_data_path'] = str(data_root / 'raw')
    default_cfg['parsed_data_path'] = str(data_root / 'parsed')
    default_cfg['database']['path'] = str(data_root / 'metadata' / 'db.sqlite3')

    return {"default": default_cfg}

def setup_logging():
    """ãƒ­ã‚°è¨­å®š"""
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s - %(levelname)s - %(message)s",
        handlers=[logging.StreamHandler()]
    )

def parse_sample_races(sample_size=SAMPLE_SIZE):
    """ãƒ¬ãƒ¼ã‚¹çµæœã®ã‚µãƒ³ãƒ—ãƒ«ãƒ‘ãƒ¼ã‚¹"""
    print("\n" + "=" * 80)
    print(f"ğŸ‡ ãƒ¬ãƒ¼ã‚¹çµæœã®ã‚µãƒ³ãƒ—ãƒ«ãƒ‘ãƒ¼ã‚¹ï¼ˆ{sample_size}ä»¶ï¼‰")
    print("=" * 80)

    cfg = load_config()
    raw_race_html_dir = Path(cfg["default"]["raw_data_path"]) / "html" / "race"

    # HTMLãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—
    race_html_files = sorted(list(raw_race_html_dir.glob("*.bin")))[:sample_size]

    print(f"\nğŸ“Š å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(race_html_files)} ä»¶")

    # ãƒ‘ãƒ¼ã‚¹å‡¦ç†
    db_path = Path(cfg["default"]["database"]["path"])
    db_path.parent.mkdir(parents=True, exist_ok=True)
    conn = sqlite3.connect(db_path)

    success_count = 0
    error_count = 0
    all_results_df = []
    error_details = []

    start_time = datetime.now()

    for i, html_file in enumerate(race_html_files, 1):
        try:
            df = results_parser.parse_results_html(str(html_file))

            if df is not None and not df.empty:
                all_results_df.append(df)
                success_count += 1
                print(f"  âœ… [{i:>3}/{len(race_html_files)}] {html_file.name}: {len(df)}è¡Œ")
            else:
                error_count += 1
                error_details.append((html_file.name, "ç©ºã®DataFrame"))
                print(f"  âš ï¸ [{i:>3}/{len(race_html_files)}] {html_file.name}: ç©ºãƒ‡ãƒ¼ã‚¿")

        except Exception as e:
            error_count += 1
            error_msg = str(e)[:100]
            error_details.append((html_file.name, error_msg))
            print(f"  âŒ [{i:>3}/{len(race_html_files)}] {html_file.name}: {error_msg}")

    elapsed = (datetime.now() - start_time).total_seconds()

    conn.close()

    # çµæœã‚µãƒãƒªãƒ¼
    print(f"\nğŸ“Š ãƒ‘ãƒ¼ã‚¹çµæœ:")
    print(f"  æˆåŠŸ: {success_count:>6} ä»¶ ({success_count/len(race_html_files)*100:>5.1f}%)")
    print(f"  å¤±æ•—: {error_count:>6} ä»¶ ({error_count/len(race_html_files)*100:>5.1f}%)")
    print(f"  å‡¦ç†æ™‚é–“: {elapsed:.2f} ç§’")
    print(f"  å¹³å‡å‡¦ç†æ™‚é–“: {elapsed/len(race_html_files):.3f} ç§’/ä»¶")

    # ã‚¨ãƒ©ãƒ¼è©³ç´°
    if error_details:
        print(f"\nâš ï¸ ã‚¨ãƒ©ãƒ¼è©³ç´°ï¼ˆæœ€åˆã®10ä»¶ï¼‰:")
        for file_name, error_msg in error_details[:10]:
            print(f"  - {file_name}: {error_msg}")

    # æˆåŠŸã—ãŸãƒ‡ãƒ¼ã‚¿ã®å†…å®¹ç¢ºèª
    if all_results_df:
        final_df = pd.concat(all_results_df, ignore_index=True)
        print(f"\nâœ… ãƒ‘ãƒ¼ã‚¹æˆåŠŸãƒ‡ãƒ¼ã‚¿:")
        print(f"  ç·è¡Œæ•°: {len(final_df):,} è¡Œ")
        print(f"  åˆ—æ•°: {len(final_df.columns)} åˆ—")
        print(f"\n  ã‚«ãƒ©ãƒ ä¸€è¦§:")
        for col in final_df.columns:
            null_rate = final_df[col].isna().sum() / len(final_df) * 100
            print(f"    - {col:35s} (æ¬ æç‡: {null_rate:>5.1f}%)")

        return final_df, success_count, error_count

    return None, success_count, error_count

def parse_sample_shutuba(sample_size=SAMPLE_SIZE):
    """å‡ºé¦¬è¡¨ã®ã‚µãƒ³ãƒ—ãƒ«ãƒ‘ãƒ¼ã‚¹"""
    print("\n" + "=" * 80)
    print(f"ğŸ“‹ å‡ºé¦¬è¡¨ã®ã‚µãƒ³ãƒ—ãƒ«ãƒ‘ãƒ¼ã‚¹ï¼ˆ{sample_size}ä»¶ï¼‰")
    print("=" * 80)

    cfg = load_config()
    raw_shutuba_html_dir = Path(cfg["default"]["raw_data_path"]) / "html" / "shutuba"

    shutuba_html_files = sorted(list(raw_shutuba_html_dir.glob("*.bin")))[:sample_size]

    print(f"\nğŸ“Š å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(shutuba_html_files)} ä»¶")

    db_path = Path(cfg["default"]["database"]["path"])
    conn = sqlite3.connect(db_path)

    success_count = 0
    error_count = 0
    all_shutuba_df = []
    error_details = []

    start_time = datetime.now()

    for i, html_file in enumerate(shutuba_html_files, 1):
        try:
            df = shutuba_parser.parse_shutuba_html(str(html_file))

            if df is not None and not df.empty:
                all_shutuba_df.append(df)
                success_count += 1

                # morning_oddsã®æœ‰ç„¡ã‚’ç¢ºèª
                has_morning_odds = 'morning_odds' in df.columns and df['morning_odds'].notna().sum() > 0
                odds_status = "âœ“ ã‚ªãƒƒã‚ºã‚ã‚Š" if has_morning_odds else "âœ— ã‚ªãƒƒã‚ºãªã—"

                print(f"  âœ… [{i:>3}/{len(shutuba_html_files)}] {html_file.name}: {len(df)}è¡Œ {odds_status}")
            else:
                error_count += 1
                error_details.append((html_file.name, "ç©ºã®DataFrame"))
                print(f"  âš ï¸ [{i:>3}/{len(shutuba_html_files)}] {html_file.name}: ç©ºãƒ‡ãƒ¼ã‚¿")

        except Exception as e:
            error_count += 1
            error_msg = str(e)[:100]
            error_details.append((html_file.name, error_msg))
            print(f"  âŒ [{i:>3}/{len(shutuba_html_files)}] {html_file.name}: {error_msg}")

    elapsed = (datetime.now() - start_time).total_seconds()

    conn.close()

    print(f"\nğŸ“Š ãƒ‘ãƒ¼ã‚¹çµæœ:")
    print(f"  æˆåŠŸ: {success_count:>6} ä»¶ ({success_count/len(shutuba_html_files)*100:>5.1f}%)")
    print(f"  å¤±æ•—: {error_count:>6} ä»¶ ({error_count/len(shutuba_html_files)*100:>5.1f}%)")
    print(f"  å‡¦ç†æ™‚é–“: {elapsed:.2f} ç§’")

    if all_shutuba_df:
        final_df = pd.concat(all_shutuba_df, ignore_index=True)
        print(f"\nâœ… ãƒ‘ãƒ¼ã‚¹æˆåŠŸãƒ‡ãƒ¼ã‚¿:")
        print(f"  ç·è¡Œæ•°: {len(final_df):,} è¡Œ")

        # é‡è¦ã‚«ãƒ©ãƒ ã®æ¬ æçŠ¶æ³
        important_cols = ['morning_odds', 'morning_popularity', 'owner_name', 'prize_total']
        print(f"\n  é‡è¦ã‚«ãƒ©ãƒ ã®æ¬ æçŠ¶æ³:")
        for col in important_cols:
            if col in final_df.columns:
                null_count = final_df[col].isna().sum()
                null_rate = null_count / len(final_df) * 100
                status = "âŒ" if null_rate > 90 else "âš ï¸" if null_rate > 50 else "âœ…"
                print(f"    {status} {col:25s}: {null_rate:>5.1f}% æ¬ æ")
            else:
                print(f"    âŒ {col:25s}: ã‚«ãƒ©ãƒ è‡ªä½“ãŒå­˜åœ¨ã—ãªã„")

        return final_df, success_count, error_count

    return None, success_count, error_count

def parse_sample_horses(sample_size=SAMPLE_SIZE):
    """é¦¬ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«ã®ã‚µãƒ³ãƒ—ãƒ«ãƒ‘ãƒ¼ã‚¹"""
    print("\n" + "=" * 80)
    print(f"ğŸ´ é¦¬ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«ã®ã‚µãƒ³ãƒ—ãƒ«ãƒ‘ãƒ¼ã‚¹ï¼ˆ{sample_size}ä»¶ï¼‰")
    print("=" * 80)

    cfg = load_config()
    raw_horse_html_dir = Path(cfg["default"]["raw_data_path"]) / "html" / "horse"

    # ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿ã‚’å–å¾—
    horse_profile_files = sorted([
        f for f in raw_horse_html_dir.glob("*_profile.bin")
    ])[:sample_size]

    print(f"\nğŸ“Š å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(horse_profile_files)} ä»¶")

    db_path = Path(cfg["default"]["database"]["path"])
    conn = sqlite3.connect(db_path)

    success_count = 0
    error_count = 0
    all_horses_data = []
    error_details = []

    start_time = datetime.now()

    for i, html_file in enumerate(horse_profile_files, 1):
        try:
            data = horse_info_parser.parse_horse_profile(str(html_file))

            if data and 'horse_id' in data and data['horse_id']:
                all_horses_data.append(data)
                success_count += 1

                # è¡€çµ±æƒ…å ±ã®æœ‰ç„¡ã‚’ç¢ºèª
                has_sire = 'sire_id' in data and data['sire_id']
                pedigree_status = "âœ“ è¡€çµ±ã‚ã‚Š" if has_sire else "âœ— è¡€çµ±ãªã—"

                print(f"  âœ… [{i:>3}/{len(horse_profile_files)}] {html_file.name}: {pedigree_status}")
            else:
                error_count += 1
                error_details.append((html_file.name, "horse_idå–å¾—å¤±æ•—"))
                print(f"  âš ï¸ [{i:>3}/{len(horse_profile_files)}] {html_file.name}: ãƒ‡ãƒ¼ã‚¿ä¸å®Œå…¨")

        except Exception as e:
            error_count += 1
            error_msg = str(e)[:100]
            error_details.append((html_file.name, error_msg))
            print(f"  âŒ [{i:>3}/{len(horse_profile_files)}] {html_file.name}: {error_msg}")

    elapsed = (datetime.now() - start_time).total_seconds()

    conn.close()

    print(f"\nğŸ“Š ãƒ‘ãƒ¼ã‚¹çµæœ:")
    print(f"  æˆåŠŸ: {success_count:>6} ä»¶ ({success_count/len(horse_profile_files)*100:>5.1f}%)")
    print(f"  å¤±æ•—: {error_count:>6} ä»¶ ({error_count/len(horse_profile_files)*100:>5.1f}%)")
    print(f"  å‡¦ç†æ™‚é–“: {elapsed:.2f} ç§’")

    if all_horses_data:
        final_df = pd.DataFrame(all_horses_data)
        print(f"\nâœ… ãƒ‘ãƒ¼ã‚¹æˆåŠŸãƒ‡ãƒ¼ã‚¿:")
        print(f"  ç·è¡Œæ•°: {len(final_df):,} è¡Œ")
        print(f"\n  ã‚«ãƒ©ãƒ ä¸€è¦§:")
        for col in final_df.columns:
            print(f"    - {col}")

        # è¡€çµ±æƒ…å ±ã®ç¢ºèª
        pedigree_cols = ['sire_id', 'dam_id', 'damsire_id']
        print(f"\n  è¡€çµ±ã‚«ãƒ©ãƒ ã®çŠ¶æ³:")
        for col in pedigree_cols:
            if col in final_df.columns:
                null_rate = final_df[col].isna().sum() / len(final_df) * 100
                status = "âœ…" if null_rate < 10 else "âš ï¸"
                print(f"    {status} {col:15s}: {null_rate:>5.1f}% æ¬ æ")
            else:
                print(f"    âŒ {col:15s}: ã‚«ãƒ©ãƒ è‡ªä½“ãŒå­˜åœ¨ã—ãªã„")

        return final_df, success_count, error_count

    return None, success_count, error_count

def estimate_full_parse_time(sample_results):
    """å…¨ä»¶ãƒ‘ãƒ¼ã‚¹å‡¦ç†ã®æ¨å®šæ™‚é–“ã‚’è¨ˆç®—"""
    print("\n" + "=" * 80)
    print("â±ï¸ å…¨ä»¶ãƒ‘ãƒ¼ã‚¹å‡¦ç†ã®æ¨å®šæ™‚é–“")
    print("=" * 80)

    race_time_per_file = sample_results['race']['time'] / sample_results['race']['sample_size']
    shutuba_time_per_file = sample_results['shutuba']['time'] / sample_results['shutuba']['sample_size']
    horse_time_per_file = sample_results['horse']['time'] / sample_results['horse']['sample_size']

    # å…¨ä»¶æ•°
    total_race_files = 20157
    total_shutuba_files = 20157
    total_horse_files = 27150

    estimated_race_time = race_time_per_file * total_race_files
    estimated_shutuba_time = shutuba_time_per_file * total_shutuba_files
    estimated_horse_time = horse_time_per_file * total_horse_files

    total_estimated_time = estimated_race_time + estimated_shutuba_time + estimated_horse_time

    print(f"\nğŸ“Š æ¨å®šæ‰€è¦æ™‚é–“:")
    print(f"  ãƒ¬ãƒ¼ã‚¹çµæœ: {estimated_race_time/60:>8.1f} åˆ†")
    print(f"  å‡ºé¦¬è¡¨:     {estimated_shutuba_time/60:>8.1f} åˆ†")
    print(f"  é¦¬æƒ…å ±:     {estimated_horse_time/60:>8.1f} åˆ†")
    print(f"  -----------------------------")
    print(f"  åˆè¨ˆ:       {total_estimated_time/60:>8.1f} åˆ† ({total_estimated_time/3600:.1f} æ™‚é–“)")

    print(f"\nğŸ’¡ æ¨å¥¨:")
    if total_estimated_time < 600:  # 10åˆ†æœªæº€
        print(f"  å‡¦ç†æ™‚é–“ãŒçŸ­ã„ã®ã§ã€ã™ãã«å…¨ä»¶ãƒ‘ãƒ¼ã‚¹å‡¦ç†ã‚’å®Ÿè¡Œã§ãã¾ã™")
    elif total_estimated_time < 3600:  # 1æ™‚é–“æœªæº€
        print(f"  å‡¦ç†æ™‚é–“ã¯ä¸­ç¨‹åº¦ã§ã™ã€‚æ™‚é–“ã«ä½™è£•ãŒã‚ã‚‹ã¨ãã«å®Ÿè¡Œã—ã¦ãã ã•ã„")
    else:
        print(f"  å‡¦ç†æ™‚é–“ãŒé•·ã„ãŸã‚ã€å¤œé–“ãƒãƒƒãƒå‡¦ç†ã§ã®å®Ÿè¡Œã‚’æ¨å¥¨ã—ã¾ã™")

def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    print("\n")
    print("ğŸ” KeibaAI_v2 ã‚µãƒ³ãƒ—ãƒ«ãƒ‘ãƒ¼ã‚¹å‡¦ç†ï¼ˆå‹•ä½œç¢ºèªï¼‰")
    print(f"â° å®Ÿè¡Œæ™‚åˆ»: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"ğŸ“Š ã‚µãƒ³ãƒ—ãƒ«ä»¶æ•°: {SAMPLE_SIZE} ä»¶")
    print("\n")

    setup_logging()

    sample_results = {
        'race': {},
        'shutuba': {},
        'horse': {}
    }

    # 1. ãƒ¬ãƒ¼ã‚¹çµæœã®ã‚µãƒ³ãƒ—ãƒ«ãƒ‘ãƒ¼ã‚¹
    start = datetime.now()
    race_df, race_success, race_error = parse_sample_races(SAMPLE_SIZE)
    sample_results['race'] = {
        'time': (datetime.now() - start).total_seconds(),
        'sample_size': SAMPLE_SIZE,
        'success': race_success,
        'error': race_error
    }

    # 2. å‡ºé¦¬è¡¨ã®ã‚µãƒ³ãƒ—ãƒ«ãƒ‘ãƒ¼ã‚¹
    start = datetime.now()
    shutuba_df, shutuba_success, shutuba_error = parse_sample_shutuba(SAMPLE_SIZE)
    sample_results['shutuba'] = {
        'time': (datetime.now() - start).total_seconds(),
        'sample_size': SAMPLE_SIZE,
        'success': shutuba_success,
        'error': shutuba_error
    }

    # 3. é¦¬ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«ã®ã‚µãƒ³ãƒ—ãƒ«ãƒ‘ãƒ¼ã‚¹
    start = datetime.now()
    horse_df, horse_success, horse_error = parse_sample_horses(SAMPLE_SIZE)
    sample_results['horse'] = {
        'time': (datetime.now() - start).total_seconds(),
        'sample_size': SAMPLE_SIZE,
        'success': horse_success,
        'error': horse_error
    }

    # 4. å…¨ä»¶ãƒ‘ãƒ¼ã‚¹æ¨å®šæ™‚é–“
    estimate_full_parse_time(sample_results)

    # æœ€çµ‚ã‚µãƒãƒªãƒ¼
    print("\n" + "=" * 80)
    print("ğŸ“‹ ã‚µãƒ³ãƒ—ãƒ«ãƒ‘ãƒ¼ã‚¹å®Œäº†")
    print("=" * 80)

    total_success = race_success + shutuba_success + horse_success
    total_files = SAMPLE_SIZE * 3
    success_rate = total_success / total_files * 100

    print(f"\nç·åˆæˆåŠŸç‡: {success_rate:.1f}% ({total_success}/{total_files})")

    if success_rate > 90:
        print(f"\nâœ… ãƒ‘ãƒ¼ã‚µãƒ¼ã¯æ­£å¸¸ã«å‹•ä½œã—ã¦ã„ã¾ã™ï¼")
        print(f"ğŸ’¡ æ¬¡ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³: å…¨ä»¶ãƒ‘ãƒ¼ã‚¹å‡¦ç†ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„")
        print(f"   â†’ python keibaai/src/run_parsing_pipeline_local.py")
    elif success_rate > 50:
        print(f"\nâš ï¸ ãƒ‘ãƒ¼ã‚µãƒ¼ã«ä¸€éƒ¨å•é¡ŒãŒã‚ã‚Šã¾ã™ãŒã€ãŠãŠã‚€ã­å‹•ä½œã—ã¦ã„ã¾ã™")
        print(f"ğŸ’¡ æ¬¡ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³: ã‚¨ãƒ©ãƒ¼å†…å®¹ã‚’ç¢ºèªã—ã€å¿…è¦ã«å¿œã˜ã¦ä¿®æ­£ã—ã¦ãã ã•ã„")
    else:
        print(f"\nğŸš¨ ãƒ‘ãƒ¼ã‚µãƒ¼ã«é‡å¤§ãªå•é¡ŒãŒã‚ã‚Šã¾ã™ï¼")
        print(f"ğŸ’¡ æ¬¡ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³: ã‚¨ãƒ©ãƒ¼å†…å®¹ã‚’è©³ã—ãèª¿æŸ»ã—ã€ãƒ‘ãƒ¼ã‚µãƒ¼ã‚’ä¿®æ­£ã—ã¦ãã ã•ã„")

    print("\n" + "=" * 80)
    print("\n")

if __name__ == "__main__":
    main()
