# 07_æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«

**æœ€çµ‚æ›´æ–°æ—¥**: 2025-11-18

---

## ğŸ“‘ ç›®æ¬¡

1. [ãƒ¢ãƒ‡ãƒ«æ¦‚è¦](#ãƒ¢ãƒ‡ãƒ«æ¦‚è¦)
2. [3ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¢ãƒ‡ãƒ«ï¼ˆÎ¼, Ïƒ, Î½ï¼‰](#3ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¢ãƒ‡ãƒ«)
3. [LightGBMè¨­å®š](#lightgbmè¨­å®š)
4. [ç¢ºç‡æ ¡æ­£](#ç¢ºç‡æ ¡æ­£)
5. [è¨“ç·´ãƒ•ãƒ­ãƒ¼](#è¨“ç·´ãƒ•ãƒ­ãƒ¼)
6. [è©•ä¾¡æŒ‡æ¨™](#è©•ä¾¡æŒ‡æ¨™)

---

## ğŸ¯ ãƒ¢ãƒ‡ãƒ«æ¦‚è¦

### äºˆæ¸¬ç›®æ¨™

å¾“æ¥ã®ç€é †äºˆæ¸¬ï¼ˆåˆ†é¡å•é¡Œï¼‰ã§ã¯ãªãã€**ç¢ºç‡åˆ†å¸ƒã®æ¨å®š**ï¼š

```python
# âŒ å¾“æ¥ã®åˆ†é¡ãƒ¢ãƒ‡ãƒ«
y_pred = [1, 5, 3, 2, ...]  # å„é¦¬ã®äºˆæ¸¬ç€é †

# âœ… KeibaAI_v2ã®ç¢ºç‡ãƒ¢ãƒ‡ãƒ«
Î¼ = [72.3, 73.1, 71.8, ...]  # æœŸå¾…å®Œèµ°æ™‚é–“ï¼ˆç§’ï¼‰
Ïƒ = [1.2, 2.5, 0.8, ...]     # é¦¬å›ºæœ‰ã®ä¸ç¢ºå®Ÿæ€§ï¼ˆç§’ï¼‰
Î½ = 5.0                      # ãƒ¬ãƒ¼ã‚¹å…¨ä½“ã®æ··æ²Œåº¦ï¼ˆè‡ªç”±åº¦ï¼‰

# â†’ tåˆ†å¸ƒã§ãƒ¢ãƒ‡ãƒ«åŒ–
# â†’ ãƒ¢ãƒ³ãƒ†ã‚«ãƒ«ãƒ­ã§å‹ç‡æ¨å®š
P(win) = [0.35, 0.12, 0.42, ...]
```

---

## ğŸ”¬ 3ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¢ãƒ‡ãƒ«ï¼ˆÎ¼, Ïƒ, Î½ï¼‰

### æ•°å­¦çš„èƒŒæ™¯

å„é¦¬iã®å®Œèµ°æ™‚é–“ã‚’**tåˆ†å¸ƒ**ã§ãƒ¢ãƒ‡ãƒ«åŒ–ï¼š

```
T_i ~ t_Î½(Î¼_i, Ïƒ_iÂ²)

where:
  Î¼_i: é¦¬iã®æœŸå¾…å®Œèµ°æ™‚é–“ï¼ˆlocation parameterï¼‰
  Ïƒ_i: é¦¬iã®æ¨™æº–åå·®ï¼ˆscale parameterï¼‰
  Î½:   ãƒ¬ãƒ¼ã‚¹å…¨ä½“ã®è‡ªç”±åº¦ï¼ˆdegrees of freedomï¼‰
```

**tåˆ†å¸ƒã‚’é¸ã¶ç†ç”±**:
1. å¤–ã‚Œå€¤ã«é ‘å¥ï¼ˆheavy-tailedï¼‰
2. Î½ãŒå°ã•ã„ â†’ äºˆæƒ³å¤–ã®å±•é–‹ãŒèµ·ãã‚„ã™ã„ï¼ˆè’ã‚Œã‚‹ãƒ¬ãƒ¼ã‚¹ï¼‰
3. Î½ãŒå¤§ãã„ â†’ æ­£è¦åˆ†å¸ƒã«è¿‘ã¥ãï¼ˆå …ã„ãƒ¬ãƒ¼ã‚¹ï¼‰

### ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®è§£é‡ˆ

| ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ | æ„å‘³ | å…¸å‹å€¤ | å½±éŸ¿è¦å›  |
|-----------|------|-------|---------|
| **Î¼ï¼ˆmuï¼‰** | æœŸå¾…å®Œèµ°æ™‚é–“ | 70-150ç§’ | è·é›¢ã€é¦¬ã®èƒ½åŠ›ã€é¨æ‰‹ |
| **Ïƒï¼ˆsigmaï¼‰** | é¦¬å›ºæœ‰ã®ä¸ç¢ºå®Ÿæ€§ | 0.5-3.0ç§’ | é¦¬ã®å®‰å®šæ€§ã€çµŒé¨“ |
| **Î½ï¼ˆnuï¼‰** | ãƒ¬ãƒ¼ã‚¹å…¨ä½“ã®æ··æ²Œåº¦ | 3-10 | ãƒ¬ãƒ¼ã‚¹ã‚¯ãƒ©ã‚¹ã€é¦¬å ´çŠ¶æ…‹ |

---

### Î¼ãƒ¢ãƒ‡ãƒ«ï¼ˆæœŸå¾…å®Œèµ°æ™‚é–“ï¼‰

**ãƒ•ã‚¡ã‚¤ãƒ«**: `keibaai/src/modules/models/model_train.py`

#### ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ

**2æ®µéšå­¦ç¿’**:
1. **Ranking**: ç›¸å¯¾çš„ãªå¼·ã•ã‚’å­¦ç¿’ï¼ˆLambdaRankï¼‰
2. **Regression**: çµ¶å¯¾çš„ãªã‚¿ã‚¤ãƒ ã‚’å­¦ç¿’ï¼ˆRMSEæœ€å°åŒ–ï¼‰

```python
from lightgbm import LGBMRanker, LGBMRegressor

class MuEstimator:
    def __init__(self):
        # Stage 1: Ranking
        self.ranker = LGBMRanker(
            objective='lambdarank',
            metric='ndcg',
            n_estimators=2000,
            learning_rate=0.01
        )

        # Stage 2: Regression
        self.regressor = LGBMRegressor(
            objective='regression',
            metric='rmse',
            n_estimators=2000,
            learning_rate=0.01
        )

    def fit(self, X, y, group):
        """
        Args:
            X: ç‰¹å¾´é‡ [N x D]
            y: å®Œèµ°æ™‚é–“ï¼ˆç§’ï¼‰ [N]
            group: ãƒ¬ãƒ¼ã‚¹ã”ã¨ã®é¦¬æ•° [num_races]
        """
        # Stage 1
        self.ranker.fit(X, y, group=group)
        rank_scores = self.ranker.predict(X)

        # Stage 2
        X_with_rank = np.c_[X, rank_scores]  # Rank scoreã‚’ç‰¹å¾´é‡ã«è¿½åŠ 
        self.regressor.fit(X_with_rank, y)

    def predict(self, X):
        rank_scores = self.ranker.predict(X)
        X_with_rank = np.c_[X, rank_scores]
        mu = self.regressor.predict(X_with_rank)
        return mu
```

---

### Ïƒãƒ¢ãƒ‡ãƒ«ï¼ˆåˆ†æ•£æ¨å®šï¼‰

**ãƒ•ã‚¡ã‚¤ãƒ«**: `keibaai/src/modules/models/sigma_estimator.py`ï¼ˆæœªå®Ÿè£…ï¼‰

#### ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ

**æ®‹å·®ã®åˆ†æ•£ã‚’å­¦ç¿’**:

```python
# Î¼ãƒ¢ãƒ‡ãƒ«ã§äºˆæ¸¬
mu_pred = mu_model.predict(X)

# å®Ÿéš›ã®æ®‹å·®
residuals = y_true - mu_pred

# æ®‹å·®ã®çµ¶å¯¾å€¤ã‚’å­¦ç¿’ï¼ˆÏƒã®ä»£ç†å¤‰æ•°ï¼‰
sigma_model = LGBMRegressor(
    objective='regression',
    metric='rmse'
)
sigma_model.fit(X, np.abs(residuals))

# Ïƒã®äºˆæ¸¬
sigma_pred = sigma_model.predict(X_test)
```

**ç‰¹å¾´é‡ã®å·¥å¤«**:
- é¦¬ã®éå»æˆç¸¾ã®ã°ã‚‰ã¤ã
- é¨æ‰‹ã®å®‰å®šæ€§
- çµŒé¨“ãƒ¬ãƒ¼ã‚¹æ•°ï¼ˆå°‘ãªã„ã»ã©ä¸ç¢ºå®Ÿï¼‰

---

### Î½ãƒ¢ãƒ‡ãƒ«ï¼ˆæ··æ²Œåº¦æ¨å®šï¼‰

**ãƒ•ã‚¡ã‚¤ãƒ«**: `keibaai/src/modules/models/nu_estimator.py`ï¼ˆæœªå®Ÿè£…ï¼‰

#### ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ

**ãƒ¬ãƒ¼ã‚¹ãƒ¬ãƒ™ãƒ«ã®ç‰¹å¾´é‡ã‹ã‚‰æ¨å®š**:

```python
# ãƒ¬ãƒ¼ã‚¹ã”ã¨ã«1ã¤ã®Î½
race_features = [
    'head_count',        # å‡ºèµ°é ­æ•°ï¼ˆå¤šã„ â†’ æ··æ²Œï¼‰
    'race_class',        # ã‚¯ãƒ©ã‚¹ï¼ˆä½ã„ â†’ æ··æ²Œï¼‰
    'track_condition',   # é¦¬å ´çŠ¶æ…‹ï¼ˆæ‚ªã„ â†’ æ··æ²Œï¼‰
    'prize_1st',         # è³é‡‘ï¼ˆä½ã„ â†’ æ··æ²Œï¼‰
]

nu_model = LGBMRegressor(
    objective='regression',
    metric='mae'
)
nu_model.fit(race_features, nu_true)
```

**Î½ã®çœŸå€¤ã®è¨ˆç®—**ï¼ˆè¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ï¼‰:
```python
from scipy.stats import t

def estimate_nu_from_residuals(residuals):
    """
    æ®‹å·®ã‹ã‚‰Î½ã‚’MLEæ¨å®š
    """
    def neg_log_likelihood(nu):
        return -np.sum(t.logpdf(residuals, df=nu))

    result = minimize_scalar(
        neg_log_likelihood,
        bounds=(2, 30),
        method='bounded'
    )
    return result.x
```

---

## âš™ï¸ LightGBMè¨­å®š

### ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿

**Î¼ãƒ¢ãƒ‡ãƒ«ï¼ˆRankerï¼‰**:
```yaml
# configs/models.yaml
models:
  lgbm_ranker:
    objective: lambdarank
    metric: ndcg
    boosting_type: gbdt
    n_estimators: 2000
    learning_rate: 0.01
    num_leaves: 31
    max_depth: -1
    feature_fraction: 0.8
    bagging_fraction: 0.8
    bagging_freq: 5
    lambda_l1: 0.0
    lambda_l2: 0.0
    min_data_in_leaf: 20
    verbose: -1
```

**Î¼ãƒ¢ãƒ‡ãƒ«ï¼ˆRegressorï¼‰**:
```yaml
  lgbm_regressor:
    objective: regression
    metric: rmse
    boosting_type: gbdt
    n_estimators: 2000
    learning_rate: 0.01
    num_leaves: 31
    max_depth: -1
    feature_fraction: 0.8
    bagging_fraction: 0.8
    bagging_freq: 5
    lambda_l1: 0.0
    lambda_l2: 0.0
    min_data_in_leaf: 20
    verbose: -1
```

### å­¦ç¿’ç‡ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚°

```python
callbacks = [
    lgb.early_stopping(stopping_rounds=50),
    lgb.log_evaluation(period=100),
]

model = lgb.train(
    params,
    train_data,
    num_boost_round=2000,
    valid_sets=[valid_data],
    callbacks=callbacks
)
```

---

## ğŸ“Š ç¢ºç‡æ ¡æ­£

### Temperature Scaling

**å•é¡Œ**: LightGBMã®ç”Ÿå‡ºåŠ›ã¯ç¢ºç‡ã¨ã—ã¦æ ¡æ­£ã•ã‚Œã¦ã„ãªã„

**è§£æ±º**: Temperature Scaling

```python
from sklearn.calibration import CalibratedClassifierCV

# Î¼, Ïƒ, Î½ â†’ å‹ç‡ã¸ã®å¤‰æ›ï¼ˆã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å¾Œï¼‰
win_probs_uncalibrated = simulate_race(mu, sigma, nu)

# Isotonic Regressionã§æ ¡æ­£
calibrator = CalibratedClassifierCV(
    base_estimator=None,  # ã™ã§ã«ç¢ºç‡ãŒå‡ºã¦ã„ã‚‹
    method='isotonic',    # or 'sigmoid'
    cv='prefit'           # äº‹å‰ã«è¨“ç·´æ¸ˆã¿
)

calibrator.fit(win_probs_uncalibrated.reshape(-1, 1), y_true)

# æ ¡æ­£æ¸ˆã¿ç¢ºç‡
win_probs_calibrated = calibrator.predict_proba(
    win_probs_uncalibrated.reshape(-1, 1)
)[:, 1]
```

### Platt Scalingï¼ˆä»£æ›¿æ‰‹æ³•ï¼‰

```python
from sklearn.linear_model import LogisticRegression

# ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°ã§æ ¡æ­£
platt_scaler = LogisticRegression()
platt_scaler.fit(
    win_probs_uncalibrated.reshape(-1, 1),
    y_true
)

win_probs_calibrated = platt_scaler.predict_proba(
    win_probs_uncalibrated.reshape(-1, 1)
)[:, 1]
```

---

## ğŸ‹ï¸ è¨“ç·´ãƒ•ãƒ­ãƒ¼

### æ™‚ç³»åˆ—åˆ†å‰²

```python
from sklearn.model_selection import TimeSeriesSplit

# âŒ NG: ãƒ©ãƒ³ãƒ€ãƒ åˆ†å‰²ï¼ˆãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯ï¼‰
from sklearn.model_selection import KFold
kfold = KFold(n_splits=5, shuffle=True)

# âœ… OK: æ™‚ç³»åˆ—åˆ†å‰²
tscv = TimeSeriesSplit(n_splits=5)

for train_idx, valid_idx in tscv.split(X):
    X_train, X_valid = X.iloc[train_idx], X.iloc[valid_idx]
    y_train, y_valid = y.iloc[train_idx], y.iloc[valid_idx]

    # è¨“ç·´
    model.fit(X_train, y_train)

    # æ¤œè¨¼
    y_pred = model.predict(X_valid)
```

### ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ï¼ˆãƒ¬ãƒ¼ã‚¹å˜ä½ï¼‰

```python
# ãƒ¬ãƒ¼ã‚¹ã”ã¨ã«ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
groups = df.groupby('race_id').size().values

# LightGBM Rankerã®è¨“ç·´
ranker.fit(X, y, group=groups)
```

### è¨“ç·´ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

```bash
# Î¼ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´
python keibaai/src/models/train_mu_model.py \
  --start_date 2020-01-01 \
  --end_date 2023-12-31 \
  --output_dir data/models/20231015_lgbm

# Ïƒ, Î½ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´
python keibaai/src/models/train_sigma_nu_models.py \
  --mu_model_path data/models/20231015_lgbm/mu_model.txt \
  --output_dir data/models/20231015_lgbm
```

---

## ğŸ“ è©•ä¾¡æŒ‡æ¨™

### 1. Brier Scoreï¼ˆç¢ºç‡ç²¾åº¦ï¼‰

```python
from sklearn.metrics import brier_score_loss

# P(win)ã¨å®Ÿéš›ã®å‹æ•—ï¼ˆ0 or 1ï¼‰
brier = brier_score_loss(y_true, win_probs)
# 0.0 (perfect) ~ 1.0 (worst)
```

**è§£é‡ˆ**:
- Brier Score < 0.20: å„ªç§€
- 0.20 - 0.25: è‰¯å¥½
- 0.25 - 0.30: æ”¹å–„ã®ä½™åœ°ã‚ã‚Š

### 2. Expected Calibration Error (ECE)

```python
def calculate_ece(y_true, y_prob, n_bins=10):
    """
    Expected Calibration Error

    ç¢ºç‡ã®äºˆæ¸¬å€¤ã¨å®Ÿéš›ã®å‹ç‡ã®ã‚ºãƒ¬ã‚’æ¸¬å®š
    """
    bins = np.linspace(0, 1, n_bins + 1)
    bin_indices = np.digitize(y_prob, bins) - 1

    ece = 0.0
    for i in range(n_bins):
        mask = bin_indices == i
        if mask.sum() > 0:
            avg_prob = y_prob[mask].mean()
            avg_true = y_true[mask].mean()
            ece += mask.sum() / len(y_true) * abs(avg_prob - avg_true)

    return ece
```

**è§£é‡ˆ**:
- ECE < 0.05: éå¸¸ã«ã‚ˆãæ ¡æ­£ã•ã‚Œã¦ã„ã‚‹
- 0.05 - 0.10: è¨±å®¹ç¯„å›²
- > 0.10: æ ¡æ­£ãŒå¿…è¦

### 3. ROIï¼ˆæŠ•è³‡åç›Šç‡ï¼‰

```python
def calculate_roi(bets, outcomes, odds):
    """
    Return on Investment

    Args:
        bets: è³­ã‘é‡‘é…åˆ† [N]
        outcomes: çµæœï¼ˆ1=çš„ä¸­, 0=å¤–ã‚Œï¼‰ [N]
        odds: ã‚ªãƒƒã‚º [N]

    Returns:
        ROI: æŠ•è³‡åç›Šç‡
    """
    total_bet = bets.sum()
    total_return = (bets * outcomes * odds).sum()

    roi = total_return / total_bet
    return roi
```

**è§£é‡ˆ**:
- ROI > 1.0: åˆ©ç›Š
- ROI = 1.0: æç›Šåˆ†å²ç‚¹
- ROI < 1.0: æå¤±

### 4. Hit Rateï¼ˆçš„ä¸­ç‡ï¼‰

```python
# 1ç€äºˆæ¸¬ã®çš„ä¸­ç‡
hit_rate = (predicted_winners == actual_winners).mean()
```

---

## ğŸ“ ãƒ¢ãƒ‡ãƒ«æ”¹å–„ã®ã‚¢ã‚¤ãƒ‡ã‚¢

### 1. ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«

```python
# è¤‡æ•°ãƒ¢ãƒ‡ãƒ«ã®å¹³å‡
mu_ensemble = (mu_lgbm + mu_xgboost + mu_catboost) / 3
```

### 2. Stacking

```python
# Level 0: Base models
mu_lgbm = lgbm_model.predict(X)
mu_xgb = xgb_model.predict(X)

# Level 1: Meta model
X_meta = np.c_[mu_lgbm, mu_xgb]
meta_model = LinearRegression()
meta_model.fit(X_meta, y_true)

mu_final = meta_model.predict(X_meta_test)
```

### 3. ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯

```python
import tensorflow as tf

model = tf.keras.Sequential([
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dropout(0.3),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dropout(0.3),
    tf.keras.layers.Dense(1, activation='linear')  # Î¼ã®äºˆæ¸¬
])

model.compile(optimizer='adam', loss='mse')
model.fit(X_train, y_train, epochs=50)
```

---

**æ¬¡ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ**: [08_ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã¨æœ€é©åŒ–.md](./08_ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã¨æœ€é©åŒ–.md)
