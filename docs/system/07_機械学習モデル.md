# 07_æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«

**æœ€çµ‚æ›´æ–°æ—¥**: 2025-11-20

---

## ğŸ“‘ ç›®æ¬¡

1. [ãƒ¢ãƒ‡ãƒ«æ¦‚è¦](#ãƒ¢ãƒ‡ãƒ«æ¦‚è¦)
2. [3ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¢ãƒ‡ãƒ«ï¼ˆÎ¼, Ïƒ, Î½ï¼‰](#3ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¢ãƒ‡ãƒ«)
3. [LightGBMè¨­å®š](#lightgbmè¨­å®š)
4. [ç¢ºç‡æ ¡æ­£](#ç¢ºç‡æ ¡æ­£)
5. [è¨“ç·´ãƒ•ãƒ­ãƒ¼](#è¨“ç·´ãƒ•ãƒ­ãƒ¼)
6. [è©•ä¾¡æŒ‡æ¨™](#è©•ä¾¡æŒ‡æ¨™)

---

## ğŸ¯ ãƒ¢ãƒ‡ãƒ«æ¦‚è¦

### äºˆæ¸¬ç›®æ¨™

å¾“æ¥ã®ç€é †äºˆæ¸¬ï¼ˆåˆ†é¡å•é¡Œï¼‰ã§ã¯ãªãã€**ç¢ºç‡åˆ†å¸ƒã®æ¨å®š**ï¼š

```python
# âŒ å¾“æ¥ã®åˆ†é¡ãƒ¢ãƒ‡ãƒ«
y_pred = [1, 5, 3, 2, ...]  # å„é¦¬ã®äºˆæ¸¬ç€é †

# âœ… KeibaAI_v2ã®ç¢ºç‡ãƒ¢ãƒ‡ãƒ«
Î¼ = [72.3, 73.1, 71.8, ...]  # æœŸå¾…å®Œèµ°æ™‚é–“ï¼ˆç§’ï¼‰
Ïƒ = [1.2, 2.5, 0.8, ...]     # é¦¬å›ºæœ‰ã®ä¸ç¢ºå®Ÿæ€§ï¼ˆç§’ï¼‰
Î½ = 5.0                      # ãƒ¬ãƒ¼ã‚¹å…¨ä½“ã®æ··æ²Œåº¦ï¼ˆè‡ªç”±åº¦ï¼‰

# â†’ tåˆ†å¸ƒã§ãƒ¢ãƒ‡ãƒ«åŒ–
# â†’ ãƒ¢ãƒ³ãƒ†ã‚«ãƒ«ãƒ­ã§å‹ç‡æ¨å®š
P(win) = [0.35, 0.12, 0.42, ...]
```

---

## ğŸ”¬ 3ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¢ãƒ‡ãƒ«ï¼ˆÎ¼, Ïƒ, Î½ï¼‰

### æ•°å­¦çš„èƒŒæ™¯

å„é¦¬iã®å®Œèµ°æ™‚é–“ã‚’**tåˆ†å¸ƒ**ã§ãƒ¢ãƒ‡ãƒ«åŒ–ï¼š

```
T_i ~ t_Î½(Î¼_i, Ïƒ_iÂ²)

where:
  Î¼_i: é¦¬iã®æœŸå¾…å®Œèµ°æ™‚é–“ï¼ˆlocation parameterï¼‰
  Ïƒ_i: é¦¬iã®æ¨™æº–åå·®ï¼ˆscale parameterï¼‰
  Î½:   ãƒ¬ãƒ¼ã‚¹å…¨ä½“ã®è‡ªç”±åº¦ï¼ˆdegrees of freedomï¼‰
```

**tåˆ†å¸ƒã‚’é¸ã¶ç†ç”±**:
1. å¤–ã‚Œå€¤ã«é ‘å¥ï¼ˆheavy-tailedï¼‰
2. Î½ãŒå°ã•ã„ â†’ äºˆæƒ³å¤–ã®å±•é–‹ãŒèµ·ãã‚„ã™ã„ï¼ˆè’ã‚Œã‚‹ãƒ¬ãƒ¼ã‚¹ï¼‰
3. Î½ãŒå¤§ãã„ â†’ æ­£è¦åˆ†å¸ƒã«è¿‘ã¥ãï¼ˆå …ã„ãƒ¬ãƒ¼ã‚¹ï¼‰

### ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®è§£é‡ˆ

| ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ | æ„å‘³ | å…¸å‹å€¤ | å½±éŸ¿è¦å›  |
|-----------|------|-------|---------|
| **Î¼ï¼ˆmuï¼‰** | æœŸå¾…å®Œèµ°æ™‚é–“ | 70-150ç§’ | è·é›¢ã€é¦¬ã®èƒ½åŠ›ã€é¨æ‰‹ |
| **Ïƒï¼ˆsigmaï¼‰** | é¦¬å›ºæœ‰ã®ä¸ç¢ºå®Ÿæ€§ | 0.5-3.0ç§’ | é¦¬ã®å®‰å®šæ€§ã€çµŒé¨“ |
| **Î½ï¼ˆnuï¼‰** | ãƒ¬ãƒ¼ã‚¹å…¨ä½“ã®æ··æ²Œåº¦ | 3-10 | ãƒ¬ãƒ¼ã‚¹ã‚¯ãƒ©ã‚¹ã€é¦¬å ´çŠ¶æ…‹ |

---

### Î¼ãƒ¢ãƒ‡ãƒ«ï¼ˆæœŸå¾…å®Œèµ°æ™‚é–“ï¼‰

**ãƒ•ã‚¡ã‚¤ãƒ«**: `keibaai/src/modules/models/model_train.py`

#### ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ

**2æ®µéšå­¦ç¿’**:
1. **Ranking**: ç›¸å¯¾çš„ãªå¼·ã•ã‚’å­¦ç¿’ï¼ˆLambdaRankï¼‰
2. **Regression**: çµ¶å¯¾çš„ãªã‚¿ã‚¤ãƒ ã‚’å­¦ç¿’ï¼ˆRMSEæœ€å°åŒ–ï¼‰

```python
import numpy as np
from lightgbm import LGBMRanker, LGBMRegressor

class MuEstimator:
    def __init__(self, ranker_params=None, regressor_params=None):
        # Stage 1: Rankingï¼ˆç›¸å¯¾çš„ãªå¼·ã•ã‚’å­¦ç¿’ï¼‰
        self.ranker_params = ranker_params or {
            'objective': 'lambdarank',
            'metric': 'ndcg',
            'n_estimators': 1000,
            'learning_rate': 0.01,
            'num_leaves': 31,
            'verbose': -1
        }
        
        # Stage 2: Regressionï¼ˆçµ¶å¯¾çš„ãªã‚¿ã‚¤ãƒ ã‚’å­¦ç¿’ï¼‰
        self.regressor_params = regressor_params or {
            'objective': 'regression',
            'metric': 'rmse',
            'n_estimators': 1000,
            'learning_rate': 0.01,
            'num_leaves': 31,
            'verbose': -1
        }
        
        self.ranker = None
        self.regressor = None

    def fit(self, X, y, group):
        """
        Args:
            X: ç‰¹å¾´é‡ [N x D]
            y: å®Œèµ°æ™‚é–“ï¼ˆç§’ï¼‰ [N]
            group: ãƒ¬ãƒ¼ã‚¹ã”ã¨ã®é¦¬æ•° [num_races]
        """
        # Stage 1: Rankerã®å­¦ç¿’
        # yã¯å®Œèµ°æ™‚é–“ï¼ˆæµ®å‹•å°æ•°ç‚¹ï¼‰ãªã®ã§ã€æ•´æ•°ã®relevanceã‚¹ã‚³ã‚¢ã«å¤‰æ›
        rank_target = []
        start_idx = 0
        for g in group:
            group_y = y[start_idx:start_idx+g]
            # ã‚¿ã‚¤ãƒ ã‹ã‚‰ãƒ©ãƒ³ã‚¯ã‚’è¨ˆç®—ï¼ˆ0-indexedï¼‰
            ranks = np.argsort(np.argsort(group_y))
            # relevance: æœ€å¤§ãƒ©ãƒ³ã‚¯ - ç¾åœ¨ã®ãƒ©ãƒ³ã‚¯ï¼ˆ1ç€ãŒæœ€é«˜ã‚¹ã‚³ã‚¢ï¼‰
            relevance = (g - 1) - ranks
            rank_target.extend(relevance)
            start_idx += g
        rank_target = np.array(rank_target, dtype=int)
        
        # Rankerã®å­¦ç¿’
        self.ranker = LGBMRanker(**self.ranker_params)
        self.ranker.fit(X, rank_target, group=group)
        rank_scores = self.ranker.predict(X)

        # Stage 2: Regressorã®å­¦ç¿’
        X_aug = X.copy()
        X_aug['rank_score'] = rank_scores  # Rankerã®ã‚¹ã‚³ã‚¢ã‚’ç‰¹å¾´é‡ã«è¿½åŠ 
        
        self.regressor = LGBMRegressor(**self.regressor_params)
        self.regressor.fit(X_aug, y)

    def predict(self, X):
        rank_scores = self.ranker.predict(X)
        X_aug = X.copy()
        X_aug['rank_score'] = rank_scores
        mu = self.regressor.predict(X_aug)
        return mu
```

**é‡è¦ãªå®Ÿè£…ãƒã‚¤ãƒ³ãƒˆ**:
- `LGBMRanker`ã¯æ•´æ•°å‹ã®relevanceã‚¹ã‚³ã‚¢ã‚’è¦æ±‚ã™ã‚‹ãŸã‚ã€å®Œèµ°æ™‚é–“ï¼ˆæµ®å‹•å°æ•°ç‚¹ï¼‰ã‹ã‚‰å„ãƒ¬ãƒ¼ã‚¹å†…ã§ãƒ©ãƒ³ã‚¯ã‚’è¨ˆç®—
- `(æœ€å¤§ãƒ©ãƒ³ã‚¯ - ç¾åœ¨ã®ãƒ©ãƒ³ã‚¯)`ã§1ç€ãŒæœ€é«˜ã‚¹ã‚³ã‚¢ã«ãªã‚‹ã‚ˆã†ã«å¤‰æ›
- Rankerã®ã‚¹ã‚³ã‚¢ã‚’ç‰¹å¾´é‡ã«è¿½åŠ ã™ã‚‹ã“ã¨ã§ã€ç›¸å¯¾çš„ãªå¼·ã•ã¨çµ¶å¯¾çš„ãªã‚¿ã‚¤ãƒ ã®ä¸¡æ–¹ã‚’å­¦ç¿’

---

### Ïƒãƒ¢ãƒ‡ãƒ«ï¼ˆåˆ†æ•£æ¨å®šï¼‰

**ãƒ•ã‚¡ã‚¤ãƒ«**: `keibaai/src/modules/models/sigma_estimator.py`ï¼ˆå®Ÿè£…æ¸ˆã¿ï¼‰

> **Note**: Muãƒ¢ãƒ‡ãƒ«ã®æ®‹å·®ã‹ã‚‰å„é¦¬ã®ä¸ç¢ºå®Ÿæ€§ã‚’å­¦ç¿’ã™ã‚‹LightGBMãƒ¢ãƒ‡ãƒ«ã‚’å®Ÿè£…æ¸ˆã¿ã§ã™ã€‚

#### ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ

**æ®‹å·®ã®åˆ†æ•£ã‚’å­¦ç¿’**:

```python
# Î¼ãƒ¢ãƒ‡ãƒ«ã§äºˆæ¸¬
mu_pred = mu_model.predict(X)

# å®Ÿéš›ã®æ®‹å·®
residuals = y_true - mu_pred

# æ®‹å·®ã®çµ¶å¯¾å€¤ã‚’å­¦ç¿’ï¼ˆÏƒã®ä»£ç†å¤‰æ•°ï¼‰
sigma_model = LGBMRegressor(
    objective='regression',
    metric='rmse'
)
sigma_model.fit(X, np.abs(residuals))

# Ïƒã®äºˆæ¸¬
sigma_pred = sigma_model.predict(X_test)
```

**ç‰¹å¾´é‡ã®å·¥å¤«**:
- é¦¬ã®éå»æˆç¸¾ã®ã°ã‚‰ã¤ã
- é¨æ‰‹ã®å®‰å®šæ€§
- çµŒé¨“ãƒ¬ãƒ¼ã‚¹æ•°ï¼ˆå°‘ãªã„ã»ã©ä¸ç¢ºå®Ÿï¼‰

---

### Î½ãƒ¢ãƒ‡ãƒ«ï¼ˆæ··æ²Œåº¦æ¨å®šï¼‰

**ãƒ•ã‚¡ã‚¤ãƒ«**: `keibaai/src/modules/models/nu_estimator.py`ï¼ˆå®Ÿè£…æ¸ˆã¿ï¼‰

> **Note**: ãƒ¬ãƒ¼ã‚¹ãƒ¬ãƒ™ãƒ«ã®ç‰¹å¾´é‡ã‹ã‚‰MLEæ¨å®šã—ãŸÎ½ã‚’å­¦ç¿’ã™ã‚‹LightGBMãƒ¢ãƒ‡ãƒ«ã‚’å®Ÿè£…æ¸ˆã¿ã§ã™ã€‚

#### ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ

**ãƒ¬ãƒ¼ã‚¹ãƒ¬ãƒ™ãƒ«ã®ç‰¹å¾´é‡ã‹ã‚‰æ¨å®š**:

```python
# ãƒ¬ãƒ¼ã‚¹ã”ã¨ã«1ã¤ã®Î½
race_features = [
    'head_count',        # å‡ºèµ°é ­æ•°ï¼ˆå¤šã„ â†’ æ··æ²Œï¼‰
    'race_class',        # ã‚¯ãƒ©ã‚¹ï¼ˆä½ã„ â†’ æ··æ²Œï¼‰
    'track_condition',   # é¦¬å ´çŠ¶æ…‹ï¼ˆæ‚ªã„ â†’ æ··æ²Œï¼‰
    'prize_1st',         # è³é‡‘ï¼ˆä½ã„ â†’ æ··æ²Œï¼‰
]

nu_model = LGBMRegressor(
    objective='regression',
    metric='mae'
)
nu_model.fit(race_features, nu_true)
```

**Î½ã®çœŸå€¤ã®è¨ˆç®—**ï¼ˆè¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ï¼‰:
```python
from scipy.stats import t

def estimate_nu_from_residuals(residuals):
    """
    æ®‹å·®ã‹ã‚‰Î½ã‚’MLEæ¨å®š
    """
    def neg_log_likelihood(nu):
        return -np.sum(t.logpdf(residuals, df=nu))

    result = minimize_scalar(
        neg_log_likelihood,
        bounds=(2, 30),
        method='bounded'
    )
    return result.x
```

---

## âš™ï¸ LightGBMè¨­å®š

### ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿

**Î¼ãƒ¢ãƒ‡ãƒ«ï¼ˆRankerï¼‰**:
```yaml
# configs/models.yaml
models:
  lgbm_ranker:
    objective: lambdarank
    metric: ndcg
    boosting_type: gbdt
    n_estimators: 2000
    learning_rate: 0.01
    num_leaves: 31
    max_depth: -1
    feature_fraction: 0.8
    bagging_fraction: 0.8
    bagging_freq: 5
    lambda_l1: 0.0
    lambda_l2: 0.0
    min_data_in_leaf: 20
    verbose: -1
```

**Î¼ãƒ¢ãƒ‡ãƒ«ï¼ˆRegressorï¼‰**:
```yaml
  lgbm_regressor:
    objective: regression
    metric: rmse
    boosting_type: gbdt
    n_estimators: 2000
    learning_rate: 0.01
    num_leaves: 31
    max_depth: -1
    feature_fraction: 0.8
    bagging_fraction: 0.8
    bagging_freq: 5
    lambda_l1: 0.0
    lambda_l2: 0.0
    min_data_in_leaf: 20
    verbose: -1
```

### å­¦ç¿’ç‡ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚°

```python
callbacks = [
    lgb.early_stopping(stopping_rounds=50),
    lgb.log_evaluation(period=100),
]

model = lgb.train(
    params,
    train_data,
    num_boost_round=2000,
    valid_sets=[valid_data],
    callbacks=callbacks
)
```

---

## ğŸ“Š ç¢ºç‡æ ¡æ­£

### Temperature Scaling

**å•é¡Œ**: LightGBMã®ç”Ÿå‡ºåŠ›ã¯ç¢ºç‡ã¨ã—ã¦æ ¡æ­£ã•ã‚Œã¦ã„ãªã„

**è§£æ±º**: Temperature Scaling

```python
from sklearn.calibration import CalibratedClassifierCV

# Î¼, Ïƒ, Î½ â†’ å‹ç‡ã¸ã®å¤‰æ›ï¼ˆã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å¾Œï¼‰
win_probs_uncalibrated = simulate_race(mu, sigma, nu)

# Isotonic Regressionã§æ ¡æ­£
calibrator = CalibratedClassifierCV(
    base_estimator=None,  # ã™ã§ã«ç¢ºç‡ãŒå‡ºã¦ã„ã‚‹
    method='isotonic',    # or 'sigmoid'
    cv='prefit'           # äº‹å‰ã«è¨“ç·´æ¸ˆã¿
)

calibrator.fit(win_probs_uncalibrated.reshape(-1, 1), y_true)

# æ ¡æ­£æ¸ˆã¿ç¢ºç‡
win_probs_calibrated = calibrator.predict_proba(
    win_probs_uncalibrated.reshape(-1, 1)
)[:, 1]
```

### Platt Scalingï¼ˆä»£æ›¿æ‰‹æ³•ï¼‰

```python
from sklearn.linear_model import LogisticRegression

# ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°ã§æ ¡æ­£
platt_scaler = LogisticRegression()
platt_scaler.fit(
    win_probs_uncalibrated.reshape(-1, 1),
    y_true
)

win_probs_calibrated = platt_scaler.predict_proba(
    win_probs_uncalibrated.reshape(-1, 1)
)[:, 1]
```

---

## ğŸ‹ï¸ è¨“ç·´ãƒ•ãƒ­ãƒ¼

### æ™‚ç³»åˆ—åˆ†å‰²

```python
from sklearn.model_selection import TimeSeriesSplit

# âŒ NG: ãƒ©ãƒ³ãƒ€ãƒ åˆ†å‰²ï¼ˆãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯ï¼‰
from sklearn.model_selection import KFold
kfold = KFold(n_splits=5, shuffle=True)

# âœ… OK: æ™‚ç³»åˆ—åˆ†å‰²
tscv = TimeSeriesSplit(n_splits=5)

for train_idx, valid_idx in tscv.split(X):
    X_train, X_valid = X.iloc[train_idx], X.iloc[valid_idx]
    y_train, y_valid = y.iloc[train_idx], y.iloc[valid_idx]

    # è¨“ç·´
    model.fit(X_train, y_train)

    # æ¤œè¨¼
    y_pred = model.predict(X_valid)
```

### ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ï¼ˆãƒ¬ãƒ¼ã‚¹å˜ä½ï¼‰

```python
# ãƒ¬ãƒ¼ã‚¹ã”ã¨ã«ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
groups = df.groupby('race_id').size().values

# LightGBM Rankerã®è¨“ç·´
ranker.fit(X, y, group=groups)
```

### è¨“ç·´ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

```bash
# Î¼ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´
python keibaai/src/models/train_mu_model.py \
  --start_date 2020-01-01 \
  --end_date 2023-12-31 \
  --output_dir data/models/20231015_lgbm

# Ïƒ, Î½ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´
python keibaai/src/models/train_sigma_nu_models.py \
  --mu_model_path data/models/20231015_lgbm/mu_model.txt \
  --output_dir data/models/20231015_lgbm
```

---

## ğŸ“ è©•ä¾¡æŒ‡æ¨™

### 1. Brier Scoreï¼ˆç¢ºç‡ç²¾åº¦ï¼‰

```python
from sklearn.metrics import brier_score_loss

# P(win)ã¨å®Ÿéš›ã®å‹æ•—ï¼ˆ0 or 1ï¼‰
brier = brier_score_loss(y_true, win_probs)
# 0.0 (perfect) ~ 1.0 (worst)
```

**è§£é‡ˆ**:
- Brier Score < 0.20: å„ªç§€
- 0.20 - 0.25: è‰¯å¥½
- 0.25 - 0.30: æ”¹å–„ã®ä½™åœ°ã‚ã‚Š

### 2. Expected Calibration Error (ECE)

```python
def calculate_ece(y_true, y_prob, n_bins=10):
    """
    Expected Calibration Error

    ç¢ºç‡ã®äºˆæ¸¬å€¤ã¨å®Ÿéš›ã®å‹ç‡ã®ã‚ºãƒ¬ã‚’æ¸¬å®š
    """
    bins = np.linspace(0, 1, n_bins + 1)
    bin_indices = np.digitize(y_prob, bins) - 1

    ece = 0.0
    for i in range(n_bins):
        mask = bin_indices == i
        if mask.sum() > 0:
            avg_prob = y_prob[mask].mean()
            avg_true = y_true[mask].mean()
            ece += mask.sum() / len(y_true) * abs(avg_prob - avg_true)

    return ece
```

**è§£é‡ˆ**:
- ECE < 0.05: éå¸¸ã«ã‚ˆãæ ¡æ­£ã•ã‚Œã¦ã„ã‚‹
- 0.05 - 0.10: è¨±å®¹ç¯„å›²
- > 0.10: æ ¡æ­£ãŒå¿…è¦

### 3. ROIï¼ˆæŠ•è³‡åç›Šç‡ï¼‰

```python
def calculate_roi(bets, outcomes, odds):
    """
    Return on Investment

    Args:
        bets: è³­ã‘é‡‘é…åˆ† [N]
        outcomes: çµæœï¼ˆ1=çš„ä¸­, 0=å¤–ã‚Œï¼‰ [N]
        odds: ã‚ªãƒƒã‚º [N]

    Returns:
        ROI: æŠ•è³‡åç›Šç‡
    """
    total_bet = bets.sum()
    total_return = (bets * outcomes * odds).sum()

    roi = total_return / total_bet
    return roi
```

**è§£é‡ˆ**:
- ROI > 1.0: åˆ©ç›Š
- ROI = 1.0: æç›Šåˆ†å²ç‚¹
- ROI < 1.0: æå¤±

### 4. Hit Rateï¼ˆçš„ä¸­ç‡ï¼‰

```python
# 1ç€äºˆæ¸¬ã®çš„ä¸­ç‡
hit_rate = (predicted_winners == actual_winners).mean()
```

---

## ğŸ”§ ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–ï¼ˆOptunaTunerï¼‰

**ãƒ•ã‚¡ã‚¤ãƒ«**: `keibaai/src/modules/models/optuna_tuner.py`

LightGBMãƒ¢ãƒ‡ãƒ«ã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è‡ªå‹•ã§æœ€é©åŒ–ã™ã‚‹ãŸã‚ã®Optunaãƒ™ãƒ¼ã‚¹ã®ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã§ã™ã€‚

### ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ

**2ã¤ã®ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ¢ãƒ¼ãƒ‰**:
1. **Regressorãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°**: æ™‚ç³»åˆ—ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆTimeSeriesSplitï¼‰
2. **Rankerãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°**: ã‚°ãƒ«ãƒ¼ãƒ—å¢ƒç•Œã‚’è€ƒæ…®ã—ãŸãƒ›ãƒ¼ãƒ«ãƒ‰ã‚¢ã‚¦ãƒˆæ³•

```python
from keibaai.src.modules.models.optuna_tuner import OptunaTuner

# ãƒãƒ¥ãƒ¼ãƒŠãƒ¼ã®åˆæœŸåŒ–
tuner = OptunaTuner(n_trials=50, timeout=3600)

# Regressorã®ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°
best_params_reg = tuner.tune_lgbm_regressor(X, y)

# Rankerã®ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°
best_params_rank = tuner.tune_lgbm_ranker(X, rank_target, groups)
```

### æœ€é©åŒ–ã•ã‚Œã‚‹ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿

| ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ | æ¢ç´¢ç¯„å›² | èª¬æ˜ |
|-----------|---------|------|
| `lambda_l1` | 1e-8 ~ 10.0 (log) | L1æ­£å‰‡åŒ– |
| `lambda_l2` | 1e-8 ~ 10.0 (log) | L2æ­£å‰‡åŒ– |
| `num_leaves` | 2 ~ 256 | è‘‰ã®æœ€å¤§æ•° |
| `feature_fraction` | 0.4 ~ 1.0 | ç‰¹å¾´é‡ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ç‡ |
| `bagging_fraction` | 0.4 ~ 1.0 | ãƒ‡ãƒ¼ã‚¿ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ç‡ |
| `bagging_freq` | 1 ~ 7 | ãƒã‚®ãƒ³ã‚°é »åº¦ |
| `min_child_samples` | 5 ~ 100 | è‘‰ãƒãƒ¼ãƒ‰ã®æœ€å°ã‚µãƒ³ãƒ—ãƒ«æ•° |
| `learning_rate` | 0.001 ~ 0.1 (log) | å­¦ç¿’ç‡ |

### ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³æˆ¦ç•¥

**Regressor**: `TimeSeriesSplit(n_splits=3)`ã§æ™‚ç³»åˆ—æ€§ã‚’ä¿æŒ

**Ranker**: ãƒ¬ãƒ¼ã‚¹å˜ä½ã®ã‚°ãƒ«ãƒ¼ãƒ—ã‚’ç¶­æŒã™ã‚‹ãŸã‚ã€æœ€å¾Œã®20%ã‚’ãƒ›ãƒ¼ãƒ«ãƒ‰ã‚¢ã‚¦ãƒˆ
```python
# ã‚°ãƒ«ãƒ¼ãƒ—å¢ƒç•Œã§åˆ†å‰²ï¼ˆãƒ¬ãƒ¼ã‚¹ã‚’é€”ä¸­ã§åˆ†å‰²ã—ãªã„ï¼‰
adjusted_split_idx = group_cumsum[split_group_idx]
```

---

## ğŸ† ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«å­¦ç¿’ï¼ˆStackingEnsembleï¼‰

**ãƒ•ã‚¡ã‚¤ãƒ«**: `keibaai/src/modules/models/stacking_ensemble.py`

è¤‡æ•°ã®ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬ã‚’çµ„ã¿åˆã‚ã›ã¦ã€ã‚ˆã‚Šé ‘å¥ã§é«˜ç²¾åº¦ãªäºˆæ¸¬ã‚’å®Ÿç¾ã—ã¾ã™ã€‚

### ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

```
Level 0 (Base Models):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Model 1  â”‚  â”‚ Model 2  â”‚  â”‚ Model 3  â”‚
â”‚(LightGBM)â”‚  â”‚(LightGBM)â”‚  â”‚(LightGBM)â”‚
â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
      â”‚             â”‚             â”‚
      â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
             â”‚  Out-of-Fold â”‚
             â”‚  Predictions â”‚
             â–¼             â–¼
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚ Level 1 (Meta Model)  â”‚
      â”‚  Ridge Regression     â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
                  â–¼
           Final Prediction
```

### ä½¿ç”¨æ–¹æ³•

```python
from keibaai.src.modules.models.stacking_ensemble import StackingEnsemble
from lightgbm import LGBMRegressor

# ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã®å®šç¾©ï¼ˆç•°ãªã‚‹è¨­å®šï¼‰
base_models = [
    LGBMRegressor(n_estimators=1000, learning_rate=0.01, num_leaves=31),
    LGBMRegressor(n_estimators=1500, learning_rate=0.005, num_leaves=63),
    LGBMRegressor(n_estimators=800, learning_rate=0.02, num_leaves=15)
]

# ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’
ensemble = StackingEnsemble(base_models=base_models, n_folds=5)
ensemble.fit(X_train, y_train)

# äºˆæ¸¬
y_pred = ensemble.predict(X_test)
```

### ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ã®åˆ©ç‚¹

1. **åˆ†æ•£ã®å‰Šæ¸›**: è¤‡æ•°ãƒ¢ãƒ‡ãƒ«ã®å¹³å‡åŒ–ã«ã‚ˆã‚Šéå­¦ç¿’ã‚’æŠ‘åˆ¶
2. **ãƒã‚¤ã‚¢ã‚¹ã®è£œæ­£**: ãƒ¡ã‚¿ãƒ¢ãƒ‡ãƒ«ãŒå„ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã®å¼±ç‚¹ã‚’è£œå®Œ
3. **é ‘å¥æ€§ã®å‘ä¸Š**: å˜ä¸€ãƒ¢ãƒ‡ãƒ«ã®å¤±æ•—ãƒªã‚¹ã‚¯ã‚’è»½æ¸›

---

## ğŸ“Š çµ±åˆãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³

**ãƒ•ã‚¡ã‚¤ãƒ«**: `keibaai/src/models/train_full_pipeline.py`

Mu, Sigma, Nu ã®3ã¤ã®ãƒ¢ãƒ‡ãƒ«ã‚’ä¸€æ‹¬ã§å­¦ç¿’ã—ã€ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–ã¨ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«å­¦ç¿’ã‚’çµ±åˆã—ãŸãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã§ã™ã€‚

### å®Ÿè¡Œã‚³ãƒãƒ³ãƒ‰

```bash
# åŸºæœ¬çš„ãªå­¦ç¿’
python keibaai/src/models/train_full_pipeline.py

# ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æœ‰åŠ¹åŒ–
python keibaai/src/models/train_full_pipeline.py --tune

# ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«å­¦ç¿’æœ‰åŠ¹åŒ–
python keibaai/src/models/train_full_pipeline.py --ensemble

# ä¸¡æ–¹ã‚’æœ‰åŠ¹åŒ–
python keibaai/src/models/train_full_pipeline.py --tune --ensemble
```

### ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®æµã‚Œ

1. **ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿**: `features.parquet`ã‹ã‚‰ç‰¹å¾´é‡ã‚’å–å¾—
2. **Muãƒ¢ãƒ‡ãƒ«å­¦ç¿’**:
   - `--tune`ãŒæœ‰åŠ¹ãªå ´åˆã€Optunaã§Rankerã¨Regressorã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æœ€é©åŒ–
   - `--ensemble`ãŒæœ‰åŠ¹ãªå ´åˆã€StackingEnsembleã§è¤‡æ•°ãƒ¢ãƒ‡ãƒ«ã‚’çµåˆ
3. **Sigmaãƒ¢ãƒ‡ãƒ«å­¦ç¿’**: Muãƒ¢ãƒ‡ãƒ«ã®æ®‹å·®ã‹ã‚‰å„é¦¬ã®ä¸ç¢ºå®Ÿæ€§ã‚’å­¦ç¿’
4. **Nuãƒ¢ãƒ‡ãƒ«å­¦ç¿’**: ãƒ¬ãƒ¼ã‚¹ç‰¹å¾´é‡ã‹ã‚‰æ··æ²Œåº¦ã‚’å­¦ç¿’
5. **ãƒ¢ãƒ‡ãƒ«ä¿å­˜**: `artifacts/models/`ã«ä¿å­˜

---

## ğŸ“ ãƒ¢ãƒ‡ãƒ«æ”¹å–„ã®ã‚¢ã‚¤ãƒ‡ã‚¢

### 1. ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ï¼ˆå®Ÿè£…æ¸ˆã¿ï¼‰

> **Note**: `StackingEnsemble`ã¨ã—ã¦å®Ÿè£…æ¸ˆã¿ã§ã™ï¼ˆä¸Šè¨˜å‚ç…§ï¼‰ã€‚

```python
# è¤‡æ•°ãƒ¢ãƒ‡ãƒ«ã®å¹³å‡
mu_ensemble = (mu_lgbm + mu_xgboost + mu_catboost) / 3
```

### 2. Stackingï¼ˆå®Ÿè£…æ¸ˆã¿ï¼‰

> **Note**: `StackingEnsemble`ã‚¯ãƒ©ã‚¹ã¨ã—ã¦å®Ÿè£…æ¸ˆã¿ã§ã™ï¼ˆä¸Šè¨˜å‚ç…§ï¼‰ã€‚

```python
# Level 0: Base models
mu_lgbm = lgbm_model.predict(X)
mu_xgb = xgb_model.predict(X)

# Level 1: Meta model
X_meta = np.c_[mu_lgbm, mu_xgb]
meta_model = LinearRegression()
meta_model.fit(X_meta, y_true)

mu_final = meta_model.predict(X_meta_test)
```

### 3. ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ï¼ˆFuture Enhancementï¼‰

```python
import tensorflow as tf

model = tf.keras.Sequential([
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dropout(0.3),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dropout(0.3),
    tf.keras.layers.Dense(1, activation='linear')  # Î¼ã®äºˆæ¸¬
])

model.compile(optimizer='adam', loss='mse')
model.fit(X_train, y_train, epochs=50)
```

---

**æ¬¡ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ**: [08_ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã¨æœ€é©åŒ–.md](./08_ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã¨æœ€é©åŒ–.md)
