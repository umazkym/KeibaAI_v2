#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å†é–‹å¯èƒ½ãªãƒ‘ãƒ¼ã‚¹ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
é€”ä¸­ã§åœæ­¢ã—ã¦ã‚‚ã€æ—¢ã«ãƒ‘ãƒ¼ã‚¹æ¸ˆã¿ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¦ç¶šè¡Œå¯èƒ½
"""

import logging
import sqlite3
from pathlib import Path
import yaml
from datetime import datetime
import sys
import pandas as pd
import os
from tqdm import tqdm
import argparse

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’sys.pathã«è¿½åŠ 
project_root = Path(__file__).resolve().parent / 'keibaai'
sys.path.append(str(project_root))

from src import pipeline_core
from src.modules.parsers import results_parser, shutuba_parser, horse_info_parser, pedigree_parser


def load_config():
    """è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ã™ã‚‹"""
    config_path = Path(__file__).resolve().parent / "keibaai" / "configs"
    with open(config_path / "default.yaml", "r", encoding="utf-8") as f:
        default_cfg = yaml.safe_load(f)

    data_root = Path(__file__).resolve().parent / "keibaai" / default_cfg['data_path']
    default_cfg['raw_data_path'] = str(data_root / 'raw')
    default_cfg['parsed_data_path'] = str(data_root / 'parsed')
    default_cfg['database']['path'] = str(data_root / 'metadata' / 'db.sqlite3')
    default_cfg['logging']['log_file'] = str(data_root / 'logs' / '{YYYY}' / '{MM}' / '{DD}' / 'parsing_resumable.log')

    config = {"default": default_cfg}
    return config


def setup_logging(log_path_template: str):
    """ãƒ­ã‚°è¨­å®šã‚’è¡Œã†"""
    now = datetime.now()
    log_path = log_path_template.format(YYYY=now.year, MM=f"{now.month:02}", DD=f"{now.day:02}")
    Path(log_path).parent.mkdir(parents=True, exist_ok=True)

    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
        handlers=[
            logging.FileHandler(log_path, encoding='utf-8'),
            logging.StreamHandler()
        ]
    )


def get_already_parsed_ids(parquet_path: Path, id_column: str) -> set:
    """
    æ—¢å­˜ã®Parquetãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰æ—¢ã«ãƒ‘ãƒ¼ã‚¹æ¸ˆã¿ã®IDã‚’å–å¾—

    Args:
        parquet_path: Parquetãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
        id_column: IDã‚«ãƒ©ãƒ å (ä¾‹: 'race_id', 'horse_id')

    Returns:
        æ—¢ã«ãƒ‘ãƒ¼ã‚¹æ¸ˆã¿ã®IDã®ã‚»ãƒƒãƒˆ
    """
    if not parquet_path.exists():
        return set()

    try:
        df = pd.read_parquet(parquet_path, columns=[id_column])
        parsed_ids = set(df[id_column].unique())
        return parsed_ids
    except Exception as e:
        logging.warning(f"æ—¢å­˜ã®Parquetãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼ ({parquet_path}): {e}")
        return set()


def get_error_files(conn, parser_name: str) -> set:
    """
    ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã‚’å–å¾—

    Args:
        conn: SQLiteæ¥ç¶š
        parser_name: ãƒ‘ãƒ¼ã‚µå (ä¾‹: 'results_parser', 'shutuba_parser')

    Returns:
        ã‚¨ãƒ©ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã®ã‚»ãƒƒãƒˆ
    """
    try:
        cursor = conn.cursor()
        cursor.execute('''
            SELECT DISTINCT source_file
            FROM parse_failures
            WHERE parser_name = ?
            ORDER BY failed_ts DESC
        ''', (parser_name,))

        error_files = {row[0] for row in cursor.fetchall()}
        return error_files
    except Exception as e:
        logging.warning(f"ã‚¨ãƒ©ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«å–å¾—å¤±æ•— ({parser_name}): {e}")
        return set()


def clear_error_record(conn, file_path: str, parser_name: str):
    """
    ãƒ‘ãƒ¼ã‚¹æˆåŠŸæ™‚ã«ã‚¨ãƒ©ãƒ¼ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’å‰Šé™¤

    Args:
        conn: SQLiteæ¥ç¶š
        file_path: ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        parser_name: ãƒ‘ãƒ¼ã‚µå
    """
    try:
        cursor = conn.cursor()
        cursor.execute('''
            DELETE FROM parse_failures
            WHERE source_file = ? AND parser_name = ?
        ''', (file_path, parser_name))
        conn.commit()

        deleted_count = cursor.rowcount
        if deleted_count > 0:
            logging.debug(f"ã‚¨ãƒ©ãƒ¼ãƒ¬ã‚³ãƒ¼ãƒ‰å‰Šé™¤: {file_path} ({deleted_count}ä»¶)")
    except Exception as e:
        logging.warning(f"ã‚¨ãƒ©ãƒ¼ãƒ¬ã‚³ãƒ¼ãƒ‰å‰Šé™¤å¤±æ•— ({file_path}): {e}")


def extract_race_id_from_filename(file_path: str) -> str:
    """ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰race_idã‚’æŠ½å‡º (ä¾‹: 202305040301.bin â†’ 202305040301)"""
    return Path(file_path).stem.replace('_perf', '').replace('_profile', '')


def parse_phase_races(cfg, conn, skip_existing: bool = False, retry_errors: bool = False):
    """ãƒ•ã‚§ãƒ¼ã‚º1: ãƒ¬ãƒ¼ã‚¹çµæœHTMLã®ãƒ‘ãƒ¼ã‚¹"""
    log = logging.getLogger(__name__)
    log.info("\n" + "=" * 80)
    log.info("ã€ãƒ•ã‚§ãƒ¼ã‚º1/5ã€‘ãƒ¬ãƒ¼ã‚¹çµæœHTMLã®ãƒ‘ãƒ¼ã‚¹å‡¦ç†ã‚’é–‹å§‹")
    log.info("=" * 80)

    raw_race_html_dir = Path(cfg["default"]["raw_data_path"]) / "html" / "race"
    parsed_race_parquet_dir = Path(cfg["default"]["parsed_data_path"]) / "parquet" / "races"
    parsed_race_parquet_dir.mkdir(parents=True, exist_ok=True)
    output_path = parsed_race_parquet_dir / "races.parquet"

    # å…¨HTMLãƒ•ã‚¡ã‚¤ãƒ«ã‚’åé›†
    race_html_files = []
    for root, _, files in os.walk(raw_race_html_dir):
        for file in files:
            if file.endswith((".html", ".bin")):
                race_html_files.append(os.path.join(root, file))

    log.info(f"  â†’ {len(race_html_files):,}ä»¶ã®ãƒ¬ãƒ¼ã‚¹çµæœHTMLãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸ")

    # ã‚¨ãƒ©ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—
    error_files = get_error_files(conn, 'results_parser')
    if error_files:
        log.info(f"  â†’ ã‚¨ãƒ©ãƒ¼å±¥æ­´: {len(error_files):,}ä»¶ (è‡ªå‹•çš„ã«å†å‡¦ç†å¯¾è±¡ã«å«ã‚ã¾ã™)")

    # ã‚¹ã‚­ãƒƒãƒ—å‡¦ç†
    if skip_existing and not retry_errors:
        already_parsed = get_already_parsed_ids(output_path, 'race_id')
        log.info(f"  â†’ æ—¢ã«ãƒ‘ãƒ¼ã‚¹æ¸ˆã¿: {len(already_parsed):,}ä»¶")

        # æ–°è¦ãƒ•ã‚¡ã‚¤ãƒ« + ã‚¨ãƒ©ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«
        race_html_files_to_process = [
            f for f in race_html_files
            if extract_race_id_from_filename(f) not in already_parsed or f in error_files
        ]

        new_files = len([f for f in race_html_files if extract_race_id_from_filename(f) not in already_parsed])
        error_retry = len([f for f in race_html_files_to_process if f in error_files])
        log.info(f"  â†’ å‡¦ç†å¯¾è±¡: {len(race_html_files_to_process):,}ä»¶ (æ–°è¦: {new_files:,}ä»¶, ã‚¨ãƒ©ãƒ¼å†å‡¦ç†: {error_retry:,}ä»¶)")
    elif retry_errors:
        # ã‚¨ãƒ©ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿
        race_html_files_to_process = [f for f in race_html_files if f in error_files]
        log.info(f"  â†’ å‡¦ç†å¯¾è±¡: {len(race_html_files_to_process):,}ä»¶ (ã‚¨ãƒ©ãƒ¼å†å‡¦ç†ã®ã¿)")
    else:
        race_html_files_to_process = race_html_files
        log.info(f"  â†’ å‡¦ç†å¯¾è±¡: {len(race_html_files_to_process):,}ä»¶ (å…¨ãƒ•ã‚¡ã‚¤ãƒ«)")

    # ãƒ‘ãƒ¼ã‚¹å®Ÿè¡Œ
    all_results_df = []
    success_count = 0
    error_count = 0
    error_cleared_count = 0

    for html_file in tqdm(race_html_files_to_process, desc="ãƒ¬ãƒ¼ã‚¹çµæœãƒ‘ãƒ¼ã‚¹", unit="ä»¶"):
        df = pipeline_core.parse_with_error_handling(
            str(html_file), "results_parser", results_parser.parse_results_html, conn
        )
        if df is not None and not df.empty:
            all_results_df.append(df)
            success_count += 1
            # ãƒ‘ãƒ¼ã‚¹æˆåŠŸã—ãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’è¨˜éŒ²ï¼ˆä¿å­˜æˆåŠŸå¾Œã«ã‚¨ãƒ©ãƒ¼ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’ã‚¯ãƒªã‚¢ã™ã‚‹ãŸã‚ï¼‰
            if html_file in error_files:
                # ä¸€æ—¦ã€æˆåŠŸã—ãŸã‚¨ãƒ©ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒªã‚¹ãƒˆã«è¿½åŠ ï¼ˆå¾Œã§ã‚¯ãƒªã‚¢ï¼‰
                pass  # å¾Œã§å‡¦ç†
        else:
            error_count += 1

    # ä¿å­˜ (æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã¨çµåˆ) - ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°å¼·åŒ–
    if all_results_df:
        try:
            new_results_df = pd.concat(all_results_df, ignore_index=True)

            if skip_existing and output_path.exists():
                # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚“ã§çµåˆ
                existing_df = pd.read_parquet(output_path)
                final_results_df = pd.concat([existing_df, new_results_df], ignore_index=True)
                # é‡è¤‡æ’é™¤ (race_id + horse_numberã§ä¸€æ„)
                final_results_df = final_results_df.drop_duplicates(
                    subset=['race_id', 'horse_number'], keep='last'
                )
                log.info(f"  â†’ æ—¢å­˜ {len(existing_df):,}ä»¶ + æ–°è¦ {len(new_results_df):,}ä»¶ = åˆè¨ˆ {len(final_results_df):,}ä»¶")
            else:
                final_results_df = new_results_df

            # ä¿å­˜å®Ÿè¡Œ
            final_results_df.to_parquet(output_path, index=False)
            log.info(f"  âœ“ ä¿å­˜å®Œäº†: {output_path} ({len(final_results_df):,}ãƒ¬ã‚³ãƒ¼ãƒ‰)")

            # ä¿å­˜æˆåŠŸå¾Œã€ãƒ‘ãƒ¼ã‚¹æˆåŠŸã—ãŸã‚¨ãƒ©ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’ã‚¯ãƒªã‚¢
            for html_file in race_html_files_to_process:
                if html_file in error_files:
                    # ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‡ãƒ¼ã‚¿ãŒfinal_results_dfã«å«ã¾ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª
                    race_id = extract_race_id_from_filename(html_file)
                    if race_id in final_results_df['race_id'].values:
                        clear_error_record(conn, html_file, 'results_parser')
                        error_cleared_count += 1

            # å‡¦ç†çµ±è¨ˆã‚’è¡¨ç¤º
            log.info(f"  ğŸ“Š å‡¦ç†çµ±è¨ˆ: æˆåŠŸ {success_count:,}ä»¶ / ã‚¨ãƒ©ãƒ¼ {error_count:,}ä»¶ / ã‚¨ãƒ©ãƒ¼è§£æ¶ˆ {error_cleared_count:,}ä»¶")

        except Exception as e:
            log.error(f"  âŒ Parquetä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
            log.warning(f"  âš ï¸ {len(all_results_df):,}ä»¶ã®ãƒ‘ãƒ¼ã‚¹çµæœãŒä¿å­˜ã§ãã¾ã›ã‚“ã§ã—ãŸ")
            log.warning(f"  ğŸ’¡ æ¬¡å›å®Ÿè¡Œæ™‚ã«å†å‡¦ç†ã•ã‚Œã¾ã™")
    else:
        log.warning("å‡¦ç†ã§ãã‚‹ãƒ¬ãƒ¼ã‚¹çµæœãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        if error_count > 0:
            log.info(f"  ğŸ“Š å‡¦ç†çµ±è¨ˆ: ã‚¨ãƒ©ãƒ¼ {error_count:,}ä»¶")


def parse_phase_shutuba(cfg, conn, skip_existing: bool = False, retry_errors: bool = False):
    """ãƒ•ã‚§ãƒ¼ã‚º2: å‡ºé¦¬è¡¨HTMLã®ãƒ‘ãƒ¼ã‚¹"""
    log = logging.getLogger(__name__)
    log.info("\n" + "=" * 80)
    log.info("ã€ãƒ•ã‚§ãƒ¼ã‚º2/5ã€‘å‡ºé¦¬è¡¨HTMLã®ãƒ‘ãƒ¼ã‚¹å‡¦ç†ã‚’é–‹å§‹")
    log.info("=" * 80)

    raw_shutuba_html_dir = Path(cfg["default"]["raw_data_path"]) / "html" / "shutuba"
    parsed_shutuba_parquet_dir = Path(cfg["default"]["parsed_data_path"]) / "parquet" / "shutuba"
    parsed_shutuba_parquet_dir.mkdir(parents=True, exist_ok=True)
    output_path = parsed_shutuba_parquet_dir / "shutuba.parquet"

    shutuba_html_files = []
    for root, _, files in os.walk(raw_shutuba_html_dir):
        for file in files:
            if file.endswith((".html", ".bin")):
                shutuba_html_files.append(os.path.join(root, file))

    log.info(f"  â†’ {len(shutuba_html_files):,}ä»¶ã®å‡ºé¦¬è¡¨HTMLãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸ")

    # ã‚¨ãƒ©ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—
    error_files = get_error_files(conn, 'shutuba_parser')
    if error_files:
        log.info(f"  â†’ ã‚¨ãƒ©ãƒ¼å±¥æ­´: {len(error_files):,}ä»¶ (è‡ªå‹•çš„ã«å†å‡¦ç†å¯¾è±¡ã«å«ã‚ã¾ã™)")

    if skip_existing and not retry_errors:
        already_parsed = get_already_parsed_ids(output_path, 'race_id')
        log.info(f"  â†’ æ—¢ã«ãƒ‘ãƒ¼ã‚¹æ¸ˆã¿: {len(already_parsed):,}ä»¶")

        shutuba_html_files_to_process = [
            f for f in shutuba_html_files
            if extract_race_id_from_filename(f) not in already_parsed or f in error_files
        ]

        new_files = len([f for f in shutuba_html_files if extract_race_id_from_filename(f) not in already_parsed])
        error_retry = len([f for f in shutuba_html_files_to_process if f in error_files])
        log.info(f"  â†’ å‡¦ç†å¯¾è±¡: {len(shutuba_html_files_to_process):,}ä»¶ (æ–°è¦: {new_files:,}ä»¶, ã‚¨ãƒ©ãƒ¼å†å‡¦ç†: {error_retry:,}ä»¶)")
    elif retry_errors:
        shutuba_html_files_to_process = [f for f in shutuba_html_files if f in error_files]
        log.info(f"  â†’ å‡¦ç†å¯¾è±¡: {len(shutuba_html_files_to_process):,}ä»¶ (ã‚¨ãƒ©ãƒ¼å†å‡¦ç†ã®ã¿)")
    else:
        shutuba_html_files_to_process = shutuba_html_files

    all_shutuba_df = []
    success_count = 0
    error_count = 0
    error_cleared_count = 0

    for html_file in tqdm(shutuba_html_files_to_process, desc="å‡ºé¦¬è¡¨ãƒ‘ãƒ¼ã‚¹", unit="ä»¶"):
        df = pipeline_core.parse_with_error_handling(
            str(html_file), "shutuba_parser", shutuba_parser.parse_shutuba_html, conn
        )
        if df is not None and not df.empty:
            all_shutuba_df.append(df)
            success_count += 1
        else:
            error_count += 1

    if all_shutuba_df:
        try:
            new_shutuba_df = pd.concat(all_shutuba_df, ignore_index=True)

            if skip_existing and output_path.exists():
                existing_df = pd.read_parquet(output_path)
                final_shutuba_df = pd.concat([existing_df, new_shutuba_df], ignore_index=True)
                final_shutuba_df = final_shutuba_df.drop_duplicates(
                    subset=['race_id', 'horse_number'], keep='last'
                )
                log.info(f"  â†’ æ—¢å­˜ {len(existing_df):,}ä»¶ + æ–°è¦ {len(new_shutuba_df):,}ä»¶ = åˆè¨ˆ {len(final_shutuba_df):,}ä»¶")
            else:
                final_shutuba_df = new_shutuba_df

            final_shutuba_df.to_parquet(output_path, index=False)
            log.info(f"  âœ“ ä¿å­˜å®Œäº†: {output_path} ({len(final_shutuba_df):,}ãƒ¬ã‚³ãƒ¼ãƒ‰)")

            # ä¿å­˜æˆåŠŸå¾Œã€ã‚¨ãƒ©ãƒ¼ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’ã‚¯ãƒªã‚¢
            for html_file in shutuba_html_files_to_process:
                if html_file in error_files:
                    race_id = extract_race_id_from_filename(html_file)
                    if race_id in final_shutuba_df['race_id'].values:
                        clear_error_record(conn, html_file, 'shutuba_parser')
                        error_cleared_count += 1

            log.info(f"  ğŸ“Š å‡¦ç†çµ±è¨ˆ: æˆåŠŸ {success_count:,}ä»¶ / ã‚¨ãƒ©ãƒ¼ {error_count:,}ä»¶ / ã‚¨ãƒ©ãƒ¼è§£æ¶ˆ {error_cleared_count:,}ä»¶")

        except Exception as e:
            log.error(f"  âŒ Parquetä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
            log.warning(f"  âš ï¸ {len(all_shutuba_df):,}ä»¶ã®ãƒ‘ãƒ¼ã‚¹çµæœãŒä¿å­˜ã§ãã¾ã›ã‚“ã§ã—ãŸ")
            log.warning(f"  ğŸ’¡ æ¬¡å›å®Ÿè¡Œæ™‚ã«å†å‡¦ç†ã•ã‚Œã¾ã™")
    else:
        log.warning("å‡¦ç†ã§ãã‚‹å‡ºé¦¬è¡¨ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        if error_count > 0:
            log.info(f"  ğŸ“Š å‡¦ç†çµ±è¨ˆ: ã‚¨ãƒ©ãƒ¼ {error_count:,}ä»¶")


def parse_phase_horses(cfg, conn, skip_existing: bool = False, retry_errors: bool = False):
    """ãƒ•ã‚§ãƒ¼ã‚º3: é¦¬ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«HTMLã®ãƒ‘ãƒ¼ã‚¹"""
    log = logging.getLogger(__name__)
    log.info("\n" + "=" * 80)
    log.info("ã€ãƒ•ã‚§ãƒ¼ã‚º3/5ã€‘é¦¬ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«HTMLã®ãƒ‘ãƒ¼ã‚¹å‡¦ç†ã‚’é–‹å§‹")
    log.info("=" * 80)

    raw_horse_html_dir = Path(cfg["default"]["raw_data_path"]) / "html" / "horse"
    parsed_horse_parquet_dir = Path(cfg["default"]["parsed_data_path"]) / "parquet" / "horses"
    parsed_horse_parquet_dir.mkdir(parents=True, exist_ok=True)
    output_path = parsed_horse_parquet_dir / "horses.parquet"

    horse_html_files = []
    for root, _, files in os.walk(raw_horse_html_dir):
        for file in files:
            if file.endswith((".html", ".bin")):
                full_path = os.path.join(root, file)
                if horse_info_parser.is_profile_file(full_path):
                    horse_html_files.append(full_path)

    log.info(f"  â†’ {len(horse_html_files):,}ä»¶ã®é¦¬ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«HTMLãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸ")

    # ã‚¨ãƒ©ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—
    error_files = get_error_files(conn, 'horse_info_parser')
    if error_files:
        log.info(f"  â†’ ã‚¨ãƒ©ãƒ¼å±¥æ­´: {len(error_files):,}ä»¶ (è‡ªå‹•çš„ã«å†å‡¦ç†å¯¾è±¡ã«å«ã‚ã¾ã™)")

    if skip_existing and not retry_errors:
        already_parsed = get_already_parsed_ids(output_path, 'horse_id')
        log.info(f"  â†’ æ—¢ã«ãƒ‘ãƒ¼ã‚¹æ¸ˆã¿: {len(already_parsed):,}é ­")

        horse_html_files_to_process = [
            f for f in horse_html_files
            if extract_race_id_from_filename(f) not in already_parsed or f in error_files
        ]

        new_files = len([f for f in horse_html_files if extract_race_id_from_filename(f) not in already_parsed])
        error_retry = len([f for f in horse_html_files_to_process if f in error_files])
        log.info(f"  â†’ å‡¦ç†å¯¾è±¡: {len(horse_html_files_to_process):,}é ­ (æ–°è¦: {new_files:,}é ­, ã‚¨ãƒ©ãƒ¼å†å‡¦ç†: {error_retry:,}é ­)")
    elif retry_errors:
        horse_html_files_to_process = [f for f in horse_html_files if f in error_files]
        log.info(f"  â†’ å‡¦ç†å¯¾è±¡: {len(horse_html_files_to_process):,}é ­ (ã‚¨ãƒ©ãƒ¼å†å‡¦ç†ã®ã¿)")
    else:
        horse_html_files_to_process = horse_html_files

    all_horses_data = []
    success_count = 0
    error_count = 0
    error_cleared_count = 0

    for html_file in tqdm(horse_html_files_to_process, desc="é¦¬ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‘ãƒ¼ã‚¹", unit="é ­"):
        data = pipeline_core.parse_with_error_handling(
            str(html_file), "horse_info_parser", horse_info_parser.parse_horse_profile, conn
        )
        if data and 'horse_id' in data and data['horse_id']:
            if not data.get('_is_empty', False):
                all_horses_data.append(data)
                success_count += 1
        else:
            error_count += 1

    if all_horses_data:
        try:
            new_horses_df = pd.DataFrame(all_horses_data)

            if skip_existing and output_path.exists():
                existing_df = pd.read_parquet(output_path)
                final_horses_df = pd.concat([existing_df, new_horses_df], ignore_index=True)
                final_horses_df = final_horses_df.drop_duplicates(subset='horse_id', keep='last')
                log.info(f"  â†’ æ—¢å­˜ {len(existing_df):,}é ­ + æ–°è¦ {len(new_horses_df):,}é ­ = åˆè¨ˆ {len(final_horses_df):,}é ­")
            else:
                final_horses_df = new_horses_df

            final_horses_df.to_parquet(output_path, index=False)
            log.info(f"  âœ“ ä¿å­˜å®Œäº†: {output_path} ({len(final_horses_df):,}ãƒ¬ã‚³ãƒ¼ãƒ‰)")

            # ä¿å­˜æˆåŠŸå¾Œã€ã‚¨ãƒ©ãƒ¼ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’ã‚¯ãƒªã‚¢
            for html_file in horse_html_files_to_process:
                if html_file in error_files:
                    horse_id = extract_race_id_from_filename(html_file)
                    if horse_id in final_horses_df['horse_id'].values:
                        clear_error_record(conn, html_file, 'horse_info_parser')
                        error_cleared_count += 1

            log.info(f"  ğŸ“Š å‡¦ç†çµ±è¨ˆ: æˆåŠŸ {success_count:,}é ­ / ã‚¨ãƒ©ãƒ¼ {error_count:,}é ­ / ã‚¨ãƒ©ãƒ¼è§£æ¶ˆ {error_cleared_count:,}é ­")

        except Exception as e:
            log.error(f"  âŒ Parquetä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
            log.warning(f"  âš ï¸ {len(all_horses_data):,}é ­ã®ãƒ‘ãƒ¼ã‚¹çµæœãŒä¿å­˜ã§ãã¾ã›ã‚“ã§ã—ãŸ")
            log.warning(f"  ğŸ’¡ æ¬¡å›å®Ÿè¡Œæ™‚ã«å†å‡¦ç†ã•ã‚Œã¾ã™")
    else:
        log.warning("å‡¦ç†ã§ãã‚‹é¦¬ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        if error_count > 0:
            log.info(f"  ğŸ“Š å‡¦ç†çµ±è¨ˆ: ã‚¨ãƒ©ãƒ¼ {error_count:,}é ­")


def parse_phase_pedigrees(cfg, conn, skip_existing: bool = False, retry_errors: bool = False):
    """ãƒ•ã‚§ãƒ¼ã‚º4: è¡€çµ±HTMLã®ãƒ‘ãƒ¼ã‚¹"""
    log = logging.getLogger(__name__)
    log.info("\n" + "=" * 80)
    log.info("ã€ãƒ•ã‚§ãƒ¼ã‚º4/5ã€‘è¡€çµ±HTMLã®ãƒ‘ãƒ¼ã‚¹å‡¦ç†ã‚’é–‹å§‹")
    log.info("=" * 80)

    raw_ped_html_dir = Path(cfg["default"]["raw_data_path"]) / "html" / "ped"
    parsed_ped_parquet_dir = Path(cfg["default"]["parsed_data_path"]) / "parquet" / "pedigrees"
    parsed_ped_parquet_dir.mkdir(parents=True, exist_ok=True)
    output_path = parsed_ped_parquet_dir / "pedigrees.parquet"

    ped_html_files = []
    for root, _, files in os.walk(raw_ped_html_dir):
        for file in files:
            if file.endswith((".html", ".bin")):
                ped_html_files.append(os.path.join(root, file))

    log.info(f"  â†’ {len(ped_html_files):,}ä»¶ã®è¡€çµ±HTMLãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸ")

    # ã‚¨ãƒ©ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—
    error_files = get_error_files(conn, 'pedigree_parser')
    if error_files:
        log.info(f"  â†’ ã‚¨ãƒ©ãƒ¼å±¥æ­´: {len(error_files):,}ä»¶ (è‡ªå‹•çš„ã«å†å‡¦ç†å¯¾è±¡ã«å«ã‚ã¾ã™)")

    if skip_existing and not retry_errors:
        already_parsed = get_already_parsed_ids(output_path, 'horse_id')
        log.info(f"  â†’ æ—¢ã«ãƒ‘ãƒ¼ã‚¹æ¸ˆã¿: {len(already_parsed):,}é ­")

        ped_html_files_to_process = [
            f for f in ped_html_files
            if extract_race_id_from_filename(f) not in already_parsed or f in error_files
        ]

        new_files = len([f for f in ped_html_files if extract_race_id_from_filename(f) not in already_parsed])
        error_retry = len([f for f in ped_html_files_to_process if f in error_files])
        log.info(f"  â†’ å‡¦ç†å¯¾è±¡: {len(ped_html_files_to_process):,}é ­ (æ–°è¦: {new_files:,}é ­, ã‚¨ãƒ©ãƒ¼å†å‡¦ç†: {error_retry:,}é ­)")
    elif retry_errors:
        ped_html_files_to_process = [f for f in ped_html_files if f in error_files]
        log.info(f"  â†’ å‡¦ç†å¯¾è±¡: {len(ped_html_files_to_process):,}é ­ (ã‚¨ãƒ©ãƒ¼å†å‡¦ç†ã®ã¿)")
    else:
        ped_html_files_to_process = ped_html_files

    all_pedigrees_df = []
    success_count = 0
    error_count = 0
    error_cleared_count = 0

    for html_file in tqdm(ped_html_files_to_process, desc="è¡€çµ±ãƒ‘ãƒ¼ã‚¹", unit="é ­"):
        df = pipeline_core.parse_with_error_handling(
            str(html_file), "pedigree_parser", pedigree_parser.parse_pedigree_html, conn
        )
        if df is not None and not df.empty:
            all_pedigrees_df.append(df)
            success_count += 1
        else:
            error_count += 1

    if all_pedigrees_df:
        try:
            new_pedigrees_df = pd.concat(all_pedigrees_df, ignore_index=True)

            if skip_existing and output_path.exists():
                existing_df = pd.read_parquet(output_path)
                final_pedigrees_df = pd.concat([existing_df, new_pedigrees_df], ignore_index=True)
                # è¡€çµ±ãƒ‡ãƒ¼ã‚¿ã¯1é ­ã«å¯¾ã—ã¦è¤‡æ•°ãƒ¬ã‚³ãƒ¼ãƒ‰ï¼ˆ5ä¸–ä»£åˆ†ï¼‰ãŒã‚ã‚‹ã®ã§ã€horse_id + generation + position ã§é‡è¤‡åˆ¤å®š
                if 'generation' in final_pedigrees_df.columns and 'position' in final_pedigrees_df.columns:
                    final_pedigrees_df = final_pedigrees_df.drop_duplicates(
                        subset=['horse_id', 'generation', 'position'], keep='last'
                    )
                else:
                    final_pedigrees_df = final_pedigrees_df.drop_duplicates(subset='horse_id', keep='last')
                log.info(f"  â†’ æ—¢å­˜ {len(existing_df):,}ä»¶ + æ–°è¦ {len(new_pedigrees_df):,}ä»¶ = åˆè¨ˆ {len(final_pedigrees_df):,}ä»¶")
            else:
                final_pedigrees_df = new_pedigrees_df

            final_pedigrees_df.to_parquet(output_path, index=False)
            log.info(f"  âœ“ ä¿å­˜å®Œäº†: {output_path} ({len(final_pedigrees_df):,}ãƒ¬ã‚³ãƒ¼ãƒ‰)")

            # ä¿å­˜æˆåŠŸå¾Œã€ã‚¨ãƒ©ãƒ¼ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’ã‚¯ãƒªã‚¢
            for html_file in ped_html_files_to_process:
                if html_file in error_files:
                    horse_id = extract_race_id_from_filename(html_file)
                    if horse_id in final_pedigrees_df['horse_id'].values:
                        clear_error_record(conn, html_file, 'pedigree_parser')
                        error_cleared_count += 1

            log.info(f"  ğŸ“Š å‡¦ç†çµ±è¨ˆ: æˆåŠŸ {success_count:,}é ­ / ã‚¨ãƒ©ãƒ¼ {error_count:,}é ­ / ã‚¨ãƒ©ãƒ¼è§£æ¶ˆ {error_cleared_count:,}é ­")

        except Exception as e:
            log.error(f"  âŒ Parquetä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
            log.warning(f"  âš ï¸ {len(all_pedigrees_df):,}é ­ã®ãƒ‘ãƒ¼ã‚¹çµæœãŒä¿å­˜ã§ãã¾ã›ã‚“ã§ã—ãŸ")
            log.warning(f"  ğŸ’¡ æ¬¡å›å®Ÿè¡Œæ™‚ã«å†å‡¦ç†ã•ã‚Œã¾ã™")
    else:
        log.warning("å‡¦ç†ã§ãã‚‹è¡€çµ±ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        if error_count > 0:
            log.info(f"  ğŸ“Š å‡¦ç†çµ±è¨ˆ: ã‚¨ãƒ©ãƒ¼ {error_count:,}é ­")


def parse_phase_performance(cfg, conn, skip_existing: bool = False, retry_errors: bool = False):
    """ãƒ•ã‚§ãƒ¼ã‚º5: é¦¬éå»æˆç¸¾ã®ãƒ‘ãƒ¼ã‚¹"""
    log = logging.getLogger(__name__)
    log.info("\n" + "=" * 80)
    log.info("ã€ãƒ•ã‚§ãƒ¼ã‚º5/5ã€‘é¦¬éå»æˆç¸¾HTMLã®ãƒ‘ãƒ¼ã‚¹å‡¦ç†ã‚’é–‹å§‹")
    log.info("=" * 80)

    raw_horse_html_dir = Path(cfg["default"]["raw_data_path"]) / "html" / "horse"
    parsed_perf_parquet_dir = Path(cfg["default"]["parsed_data_path"]) / "parquet" / "horses_performance"
    parsed_perf_parquet_dir.mkdir(parents=True, exist_ok=True)
    output_path = parsed_perf_parquet_dir / "horses_performance.parquet"

    horse_perf_files = []
    for root, _, files in os.walk(raw_horse_html_dir):
        for file in files:
            if file.endswith(("_perf.html", "_perf.bin")):
                horse_perf_files.append(os.path.join(root, file))

    log.info(f"  â†’ {len(horse_perf_files):,}ä»¶ã®é¦¬éå»æˆç¸¾ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸ")

    # ã‚¨ãƒ©ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—
    error_files = get_error_files(conn, 'horse_performance_parser')
    if error_files:
        log.info(f"  â†’ ã‚¨ãƒ©ãƒ¼å±¥æ­´: {len(error_files):,}ä»¶ (è‡ªå‹•çš„ã«å†å‡¦ç†å¯¾è±¡ã«å«ã‚ã¾ã™)")

    if skip_existing and not retry_errors:
        already_parsed = get_already_parsed_ids(output_path, 'horse_id')
        log.info(f"  â†’ æ—¢ã«ãƒ‘ãƒ¼ã‚¹æ¸ˆã¿: {len(already_parsed):,}é ­")

        horse_perf_files_to_process = [
            f for f in horse_perf_files
            if extract_race_id_from_filename(f) not in already_parsed or f in error_files
        ]

        new_files = len([f for f in horse_perf_files if extract_race_id_from_filename(f) not in already_parsed])
        error_retry = len([f for f in horse_perf_files_to_process if f in error_files])
        log.info(f"  â†’ å‡¦ç†å¯¾è±¡: {len(horse_perf_files_to_process):,}é ­ (æ–°è¦: {new_files:,}é ­, ã‚¨ãƒ©ãƒ¼å†å‡¦ç†: {error_retry:,}é ­)")
    elif retry_errors:
        horse_perf_files_to_process = [f for f in horse_perf_files if f in error_files]
        log.info(f"  â†’ å‡¦ç†å¯¾è±¡: {len(horse_perf_files_to_process):,}é ­ (ã‚¨ãƒ©ãƒ¼å†å‡¦ç†ã®ã¿)")
    else:
        horse_perf_files_to_process = horse_perf_files

    all_perf_df = []
    success_count = 0
    error_count = 0
    error_cleared_count = 0

    for html_file in tqdm(horse_perf_files_to_process, desc="é¦¬éå»æˆç¸¾ãƒ‘ãƒ¼ã‚¹", unit="é ­"):
        df = pipeline_core.parse_with_error_handling(
            str(html_file), "horse_performance_parser", horse_info_parser.parse_horse_performance, conn
        )
        if df is not None and not df.empty:
            all_perf_df.append(df)
            success_count += 1
        else:
            error_count += 1

    if all_perf_df:
        try:
            new_perf_df = pd.concat(all_perf_df, ignore_index=True)

            # ãƒ‡ãƒ¼ã‚¿å‹ã®æœ€é©åŒ–
            int_columns = ['race_number', 'head_count', 'bracket_number', 'horse_number',
                          'finish_position', 'popularity', 'horse_weight', 'horse_weight_change']
            for col in int_columns:
                if col in new_perf_df.columns:
                    new_perf_df[col] = new_perf_df[col].astype('Int64')

            if skip_existing and output_path.exists():
                existing_df = pd.read_parquet(output_path)
                final_perf_df = pd.concat([existing_df, new_perf_df], ignore_index=True)
                # éå»æˆç¸¾ã¯1é ­Ã—è¤‡æ•°ãƒ¬ãƒ¼ã‚¹ãªã®ã§ã€horse_id + race_date + race_name ã§é‡è¤‡åˆ¤å®š
                if 'race_date' in final_perf_df.columns and 'race_name' in final_perf_df.columns:
                    final_perf_df = final_perf_df.drop_duplicates(
                        subset=['horse_id', 'race_date', 'race_name'], keep='last'
                    )
                else:
                    final_perf_df = final_perf_df.drop_duplicates(subset='horse_id', keep='last')
                log.info(f"  â†’ æ—¢å­˜ {len(existing_df):,}ä»¶ + æ–°è¦ {len(new_perf_df):,}ä»¶ = åˆè¨ˆ {len(final_perf_df):,}ä»¶")
            else:
                final_perf_df = new_perf_df

            final_perf_df.to_parquet(output_path, index=False)
            log.info(f"  âœ“ ä¿å­˜å®Œäº†: {output_path} ({len(final_perf_df):,}ãƒ¬ã‚³ãƒ¼ãƒ‰)")

            # ä¿å­˜æˆåŠŸå¾Œã€ã‚¨ãƒ©ãƒ¼ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’ã‚¯ãƒªã‚¢
            for html_file in horse_perf_files_to_process:
                if html_file in error_files:
                    horse_id = extract_race_id_from_filename(html_file)
                    if horse_id in final_perf_df['horse_id'].values:
                        clear_error_record(conn, html_file, 'horse_performance_parser')
                        error_cleared_count += 1

            log.info(f"  ğŸ“Š å‡¦ç†çµ±è¨ˆ: æˆåŠŸ {success_count:,}é ­ / ã‚¨ãƒ©ãƒ¼ {error_count:,}é ­ / ã‚¨ãƒ©ãƒ¼è§£æ¶ˆ {error_cleared_count:,}é ­")

        except Exception as e:
            log.error(f"  âŒ Parquetä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
            log.warning(f"  âš ï¸ {len(all_perf_df):,}é ­ã®ãƒ‘ãƒ¼ã‚¹çµæœãŒä¿å­˜ã§ãã¾ã›ã‚“ã§ã—ãŸ")
            log.warning(f"  ğŸ’¡ æ¬¡å›å®Ÿè¡Œæ™‚ã«å†å‡¦ç†ã•ã‚Œã¾ã™")
    else:
        log.warning("å‡¦ç†ã§ãã‚‹é¦¬éå»æˆç¸¾ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        if error_count > 0:
            log.info(f"  ğŸ“Š å‡¦ç†çµ±è¨ˆ: ã‚¨ãƒ©ãƒ¼ {error_count:,}é ­")


def main():
    """å†é–‹å¯èƒ½ãªãƒ‘ãƒ¼ã‚¹ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³"""
    parser = argparse.ArgumentParser(description='å†é–‹å¯èƒ½ãªãƒ‘ãƒ¼ã‚¹ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³')
    parser.add_argument('--force-reparse', action='store_true',
                        help='æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å«ã‚ã¦å…¨ä»¶å†ãƒ‘ãƒ¼ã‚¹ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯æ–°è¦ãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿å‡¦ç†ï¼‰')
    parser.add_argument('--retry-errors', action='store_true',
                        help='ã‚¨ãƒ©ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿å†å‡¦ç†ï¼ˆä»–ã®ãƒ•ã‚¡ã‚¤ãƒ«ã¯ã‚¹ã‚­ãƒƒãƒ—ï¼‰')
    parser.add_argument('--phase', type=str, default='all',
                        choices=['1', '2', '3', '4', '5', 'all'],
                        help='å®Ÿè¡Œã™ã‚‹ãƒ•ã‚§ãƒ¼ã‚º (1=ãƒ¬ãƒ¼ã‚¹, 2=å‡ºé¦¬è¡¨, 3=é¦¬, 4=è¡€çµ±, 5=æˆç¸¾, all=å…¨ã¦)')
    args = parser.parse_args()

    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯ã‚¹ã‚­ãƒƒãƒ—ãƒ¢ãƒ¼ãƒ‰ï¼ˆæ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†ã—ãªã„ï¼‰
    skip_existing = not args.force_reparse

    cfg = load_config()
    setup_logging(cfg["default"]["logging"]["log_file"])

    log = logging.getLogger(__name__)
    log.info("=" * 80)
    log.info("å†é–‹å¯èƒ½ãªãƒ‡ãƒ¼ã‚¿æ•´å½¢ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’é–‹å§‹ã—ã¾ã™...")
    if args.retry_errors:
        log.info(f"ãƒ¢ãƒ¼ãƒ‰: ã‚¨ãƒ©ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿å†å‡¦ç†")
    elif args.force_reparse:
        log.info(f"ãƒ¢ãƒ¼ãƒ‰: å…¨ä»¶å†ãƒ‘ãƒ¼ã‚¹")
    else:
        log.info(f"ãƒ¢ãƒ¼ãƒ‰: æ–°è¦ãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿å‡¦ç† (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ)")
    log.info(f"ãƒ•ã‚§ãƒ¼ã‚º: {args.phase}")
    log.info("=" * 80)

    db_path = Path(cfg["default"]["database"]["path"])
    conn = sqlite3.connect(db_path)

    try:
        if args.phase in ['1', 'all']:
            parse_phase_races(cfg, conn, skip_existing=skip_existing, retry_errors=args.retry_errors)

        if args.phase in ['2', 'all']:
            parse_phase_shutuba(cfg, conn, skip_existing=skip_existing, retry_errors=args.retry_errors)

        if args.phase in ['3', 'all']:
            parse_phase_horses(cfg, conn, skip_existing=skip_existing, retry_errors=args.retry_errors)

        if args.phase in ['4', 'all']:
            parse_phase_pedigrees(cfg, conn, skip_existing=skip_existing, retry_errors=args.retry_errors)

        if args.phase in ['5', 'all']:
            parse_phase_performance(cfg, conn, skip_existing=skip_existing, retry_errors=args.retry_errors)

    except KeyboardInterrupt:
        log.warning("\n" + "=" * 80)
        log.warning("âš ï¸ ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã‚ˆã‚‹ä¸­æ–­ã‚’æ¤œå‡ºã—ã¾ã—ãŸ")
        log.warning("æ¬¡å›å®Ÿè¡Œæ™‚ã«ãã®ã¾ã¾å†å®Ÿè¡Œã™ã‚‹ã¨ã€ã“ã“ã‹ã‚‰å†é–‹ã§ãã¾ã™:")
        log.warning(f"  python run_parsing_resumable.py --phase {args.phase}")
        log.warning("=" * 80)
    except Exception as e:
        log.error(f"ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®å®Ÿè¡Œä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}", exc_info=True)
    finally:
        conn.close()
        log.info("ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šã‚’ã‚¯ãƒ­ãƒ¼ã‚ºã—ã¾ã—ãŸã€‚")

    log.info("=" * 80)
    log.info("ãƒ‡ãƒ¼ã‚¿æ•´å½¢ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ãŒçµ‚äº†ã—ã¾ã—ãŸã€‚")
    log.info("=" * 80)


if __name__ == "__main__":
    main()
