#!/usr/bin/env python3
"""
ãƒ‡ãƒãƒƒã‚°ã‚¹ã‚¯ãƒªãƒ—ãƒˆ04: åˆ†æç”¨ãƒ™ãƒ¼ã‚¹ãƒ†ãƒ¼ãƒ–ãƒ«ã®ä½œæˆ

æ—¢å­˜ã®1,000ä»¶ãƒ‡ãƒ¼ã‚¿ï¼ˆraces, shutuba, horses, pedigreesï¼‰ã‚’çµåˆã—ã€
ç‰¹å¾´é‡ç”Ÿæˆã«å¿…è¦ãªå…¨æƒ…å ±ã‚’çµ±åˆã—ãŸãƒ™ãƒ¼ã‚¹ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ä½œæˆã—ã¾ã™ã€‚

ç›®çš„:
- å°è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã§ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å…¨ä½“ã‚’æ¤œè¨¼
- ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ³ â†’ ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ â†’ è©•ä¾¡ã®ã‚µã‚¤ã‚¯ãƒ«ã‚’ç¢ºç«‹
- å•é¡Œç‚¹ã®æ—©æœŸç™ºè¦‹
"""

import pandas as pd
import numpy as np
from pathlib import Path
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

def load_parquet_files():
    """æ—¢å­˜ã®Parquetãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰"""
    print("=" * 80)
    print("ğŸ“ Parquetãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ­ãƒ¼ãƒ‰")
    print("=" * 80)

    base_dir = Path("keibaai/data/parsed/parquet")

    # 1. ãƒ¬ãƒ¼ã‚¹çµæœ
    races_path = base_dir / "races" / "races.parquet"
    df_races = pd.read_parquet(races_path)
    print(f"\nâœ… races.parquet: {len(df_races):,}è¡Œ Ã— {len(df_races.columns)}åˆ—")
    print(f"   ã‚«ãƒ©ãƒ : {', '.join(df_races.columns.tolist()[:10])}...")

    # 2. å‡ºé¦¬è¡¨
    shutuba_path = base_dir / "shutuba" / "shutuba.parquet"
    df_shutuba = pd.read_parquet(shutuba_path)
    print(f"\nâœ… shutuba.parquet: {len(df_shutuba):,}è¡Œ Ã— {len(df_shutuba.columns)}åˆ—")
    print(f"   ã‚«ãƒ©ãƒ : {', '.join(df_shutuba.columns.tolist()[:10])}...")

    # 3. é¦¬ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«
    horses_path = base_dir / "horses" / "horses.parquet"
    df_horses = pd.read_parquet(horses_path)
    print(f"\nâœ… horses.parquet: {len(df_horses):,}è¡Œ Ã— {len(df_horses.columns)}åˆ—")
    print(f"   ã‚«ãƒ©ãƒ : {', '.join(df_horses.columns.tolist())}")

    # 4. è¡€çµ±
    pedigrees_path = base_dir / "pedigrees" / "pedigrees.parquet"
    df_pedigrees = pd.read_parquet(pedigrees_path)
    print(f"\nâœ… pedigrees.parquet: {len(df_pedigrees):,}è¡Œ Ã— {len(df_pedigrees.columns)}åˆ—")
    print(f"   ã‚«ãƒ©ãƒ : {', '.join(df_pedigrees.columns.tolist())}")

    return df_races, df_shutuba, df_horses, df_pedigrees


def analyze_join_keys(df_races, df_shutuba, df_horses, df_pedigrees):
    """çµåˆã‚­ãƒ¼ã®ä¸€è‡´çŠ¶æ³ã‚’åˆ†æ"""
    print("\n" + "=" * 80)
    print("ğŸ”— çµåˆã‚­ãƒ¼ã®ä¸€è‡´çŠ¶æ³åˆ†æ")
    print("=" * 80)

    # race_id + horse_id ã®çµ„ã¿åˆã‚ã›
    races_keys = set(zip(df_races['race_id'], df_races['horse_id']))
    shutuba_keys = set(zip(df_shutuba['race_id'], df_shutuba['horse_id']))

    print(f"\nraces.parquet ã® (race_id, horse_id) ãƒšã‚¢æ•°: {len(races_keys):,}")
    print(f"shutuba.parquet ã® (race_id, horse_id) ãƒšã‚¢æ•°: {len(shutuba_keys):,}")

    common_keys = races_keys & shutuba_keys
    only_races = races_keys - shutuba_keys
    only_shutuba = shutuba_keys - races_keys

    print(f"\nå…±é€šãƒšã‚¢æ•°: {len(common_keys):,}")
    print(f"races ã®ã¿: {len(only_races):,}")
    print(f"shutuba ã®ã¿: {len(only_shutuba):,}")

    # é¦¬ID
    races_horse_ids = set(df_races['horse_id'].unique())
    horses_horse_ids = set(df_horses['horse_id'].unique())
    pedigrees_horse_ids = set(df_pedigrees['horse_id'].unique())

    print(f"\né¦¬IDæ•°:")
    print(f"  races.parquet: {len(races_horse_ids):,}")
    print(f"  horses.parquet: {len(horses_horse_ids):,}")
    print(f"  pedigrees.parquet: {len(pedigrees_horse_ids):,}")

    common_horses_all = races_horse_ids & horses_horse_ids & pedigrees_horse_ids
    print(f"\n  3ã¤å…¨ã¦ã«å­˜åœ¨: {len(common_horses_all):,}")

    missing_in_horses = races_horse_ids - horses_horse_ids
    missing_in_pedigrees = races_horse_ids - pedigrees_horse_ids

    if missing_in_horses:
        print(f"\nâš ï¸ races ã«ã¯å­˜åœ¨ã™ã‚‹ãŒ horses ã«ãªã„é¦¬: {len(missing_in_horses):,}é ­")
        print(f"   ã‚µãƒ³ãƒ—ãƒ«: {list(missing_in_horses)[:5]}")

    if missing_in_pedigrees:
        print(f"\nâš ï¸ races ã«ã¯å­˜åœ¨ã™ã‚‹ãŒ pedigrees ã«ãªã„é¦¬: {len(missing_in_pedigrees):,}é ­")
        print(f"   ã‚µãƒ³ãƒ—ãƒ«: {list(missing_in_pedigrees)[:5]}")


def create_base_table(df_races, df_shutuba, df_horses, df_pedigrees):
    """åˆ†æç”¨ãƒ™ãƒ¼ã‚¹ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ä½œæˆ"""
    print("\n" + "=" * 80)
    print("ğŸ”¨ åˆ†æç”¨ãƒ™ãƒ¼ã‚¹ãƒ†ãƒ¼ãƒ–ãƒ«ã®ä½œæˆ")
    print("=" * 80)

    # ã‚¹ãƒ†ãƒƒãƒ—1: races ã‚’åŸºæº–ã«ã™ã‚‹
    print("\nã€ã‚¹ãƒ†ãƒƒãƒ—1ã€‘races.parquet ã‚’åŸºæº–ãƒ†ãƒ¼ãƒ–ãƒ«ã¨ã—ã¦ä½¿ç”¨")
    df_base = df_races.copy()
    print(f"  åˆæœŸè¡Œæ•°: {len(df_base):,}")

    # ã‚¹ãƒ†ãƒƒãƒ—2: shutuba ã‹ã‚‰è¿½åŠ æƒ…å ±ã‚’å–å¾—
    print("\nã€ã‚¹ãƒ†ãƒƒãƒ—2ã€‘shutuba.parquet ã‹ã‚‰ morning_odds ãªã©ã‚’çµåˆ")

    # shutuba ã‹ã‚‰å¿…è¦ãªã‚«ãƒ©ãƒ ã®ã¿é¸æŠ
    shutuba_cols = ['race_id', 'horse_id', 'morning_odds', 'morning_popularity']
    # ã“ã‚Œã‚‰ã®ã‚«ãƒ©ãƒ ãŒå®Ÿéš›ã«å­˜åœ¨ã™ã‚‹ã‹ç¢ºèª
    available_shutuba_cols = ['race_id', 'horse_id']
    for col in ['morning_odds', 'morning_popularity']:
        if col in df_shutuba.columns:
            available_shutuba_cols.append(col)

    df_shutuba_subset = df_shutuba[available_shutuba_cols].copy()

    # çµåˆå‰ã®è¡Œæ•°
    before_merge = len(df_base)

    # left join
    df_base = df_base.merge(
        df_shutuba_subset,
        on=['race_id', 'horse_id'],
        how='left',
        suffixes=('', '_shutuba')
    )

    print(f"  çµåˆå¾Œè¡Œæ•°: {len(df_base):,} (å¤‰åŒ–: {len(df_base) - before_merge})")
    print(f"  è¿½åŠ ã‚«ãƒ©ãƒ : {', '.join([c for c in available_shutuba_cols if c not in ['race_id', 'horse_id']])}")

    # ã‚¹ãƒ†ãƒƒãƒ—3: horses ã‹ã‚‰è¡€çµ±æƒ…å ±ã‚’å–å¾—
    print("\nã€ã‚¹ãƒ†ãƒƒãƒ—3ã€‘horses.parquet ã‹ã‚‰è¡€çµ±æƒ…å ±ï¼ˆsire_id, dam_idç­‰ï¼‰ã‚’çµåˆ")

    # horses ã‹ã‚‰å¿…è¦ãªã‚«ãƒ©ãƒ ã‚’é¸æŠ
    horses_cols = ['horse_id']
    for col in ['sire_id', 'sire_name', 'dam_id', 'dam_name',
                'damsire_id', 'damsire_name', 'birth_date',
                'breeder_name', 'producing_area', 'coat_color']:
        if col in df_horses.columns:
            horses_cols.append(col)

    df_horses_subset = df_horses[horses_cols].copy()

    before_merge = len(df_base)
    df_base = df_base.merge(
        df_horses_subset,
        on='horse_id',
        how='left',
        suffixes=('', '_horses')
    )

    print(f"  çµåˆå¾Œè¡Œæ•°: {len(df_base):,} (å¤‰åŒ–: {len(df_base) - before_merge})")
    print(f"  è¿½åŠ ã‚«ãƒ©ãƒ : {', '.join([c for c in horses_cols if c != 'horse_id'])}")

    # ã‚¹ãƒ†ãƒƒãƒ—4: pedigrees ã‹ã‚‰ç¥–å…ˆæƒ…å ±ã‚’è¿½åŠ ï¼ˆgeneration=1ã®ã¿: çˆ¶æ¯ï¼‰
    print("\nã€ã‚¹ãƒ†ãƒƒãƒ—4ã€‘pedigrees.parquet ã‹ã‚‰çˆ¶æ¯æƒ…å ±ã‚’çµåˆ")

    # generation=1ï¼ˆçˆ¶æ¯ï¼‰ã®ã¿ã‚’å–å¾—
    df_pedigrees_gen1 = df_pedigrees[df_pedigrees['generation'] == 1].copy()

    # ãƒ”ãƒœãƒƒãƒˆã—ã¦æ¨ªæŒã¡ã«å¤‰æ›
    # ï¼ˆæœ¬æ¥ã¯çˆ¶ãƒ»æ¯ã‚’åˆ†ã‘ã‚‹ã¹ãã ãŒã€ç°¡æ˜“çš„ã«æœ€åˆã®2ç¥–å…ˆã‚’å–å¾—ï¼‰
    pedigree_info = {}
    for horse_id, group in df_pedigrees_gen1.groupby('horse_id'):
        ancestors = group[['ancestor_id', 'ancestor_name']].values
        pedigree_info[horse_id] = {
            'pedigree_ancestor_1_id': ancestors[0][0] if len(ancestors) > 0 else None,
            'pedigree_ancestor_1_name': ancestors[0][1] if len(ancestors) > 0 else None,
            'pedigree_ancestor_2_id': ancestors[1][0] if len(ancestors) > 1 else None,
            'pedigree_ancestor_2_name': ancestors[1][1] if len(ancestors) > 1 else None,
        }

    df_pedigree_pivot = pd.DataFrame.from_dict(pedigree_info, orient='index')
    df_pedigree_pivot.reset_index(inplace=True)
    df_pedigree_pivot.rename(columns={'index': 'horse_id'}, inplace=True)

    before_merge = len(df_base)
    df_base = df_base.merge(
        df_pedigree_pivot,
        on='horse_id',
        how='left'
    )

    print(f"  çµåˆå¾Œè¡Œæ•°: {len(df_base):,} (å¤‰åŒ–: {len(df_base) - before_merge})")
    print(f"  è¿½åŠ ã‚«ãƒ©ãƒ : pedigree_ancestor_1_id/name, pedigree_ancestor_2_id/name")

    return df_base


def analyze_base_table(df_base):
    """ãƒ™ãƒ¼ã‚¹ãƒ†ãƒ¼ãƒ–ãƒ«ã®å“è³ªåˆ†æ"""
    print("\n" + "=" * 80)
    print("ğŸ“Š ãƒ™ãƒ¼ã‚¹ãƒ†ãƒ¼ãƒ–ãƒ«ã®ãƒ‡ãƒ¼ã‚¿å“è³ªåˆ†æ")
    print("=" * 80)

    print(f"\nåŸºæœ¬æƒ…å ±:")
    print(f"  ç·è¡Œæ•°: {len(df_base):,}")
    print(f"  ç·åˆ—æ•°: {len(df_base.columns)}")
    print(f"  ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {df_base.memory_usage(deep=True).sum() / 1024 / 1024:.2f} MB")

    print(f"\nå…¨ã‚«ãƒ©ãƒ ä¸€è¦§ ({len(df_base.columns)}å€‹):")
    for i, col in enumerate(df_base.columns, 1):
        dtype = df_base[col].dtype
        null_count = df_base[col].isna().sum()
        null_pct = (null_count / len(df_base)) * 100
        print(f"  {i:2}. {col:30} | {str(dtype):15} | æ¬ æ: {null_count:5} ({null_pct:5.2f}%)")

    # é‡è¦ã‚«ãƒ©ãƒ ã®çµ±è¨ˆ
    print(f"\né‡è¦ã‚«ãƒ©ãƒ ã®ã‚µãƒ³ãƒ—ãƒ«å€¤:")
    important_cols = ['race_id', 'horse_id', 'finish_position', 'win_odds',
                      'sire_id', 'dam_id', 'pedigree_ancestor_1_id']

    for col in important_cols:
        if col in df_base.columns:
            non_null = df_base[col].notna().sum()
            sample_val = df_base[col].dropna().iloc[0] if non_null > 0 else 'N/A'
            print(f"  {col:30}: éæ¬ æ={non_null:5}ä»¶ | ã‚µãƒ³ãƒ—ãƒ«å€¤={sample_val}")

    # ãƒ¦ãƒ‹ãƒ¼ã‚¯æ•°
    print(f"\nãƒ¦ãƒ‹ãƒ¼ã‚¯æ•°:")
    print(f"  ãƒ¦ãƒ‹ãƒ¼ã‚¯ãƒ¬ãƒ¼ã‚¹æ•°: {df_base['race_id'].nunique():,}")
    print(f"  ãƒ¦ãƒ‹ãƒ¼ã‚¯é¦¬æ•°: {df_base['horse_id'].nunique():,}")
    if 'jockey_id' in df_base.columns:
        print(f"  ãƒ¦ãƒ‹ãƒ¼ã‚¯é¨æ‰‹æ•°: {df_base['jockey_id'].nunique():,}")
    if 'trainer_id' in df_base.columns:
        print(f"  ãƒ¦ãƒ‹ãƒ¼ã‚¯èª¿æ•™å¸«æ•°: {df_base['trainer_id'].nunique():,}")


def save_base_table(df_base):
    """ãƒ™ãƒ¼ã‚¹ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ä¿å­˜"""
    print("\n" + "=" * 80)
    print("ğŸ’¾ ãƒ™ãƒ¼ã‚¹ãƒ†ãƒ¼ãƒ–ãƒ«ã®ä¿å­˜")
    print("=" * 80)

    output_dir = Path("keibaai/data/parsed/parquet/analysis")
    output_dir.mkdir(parents=True, exist_ok=True)

    output_path = output_dir / "base_table.parquet"
    df_base.to_parquet(output_path, index=False)

    file_size = output_path.stat().st_size / 1024 / 1024

    print(f"\nâœ… ä¿å­˜å®Œäº†:")
    print(f"   ãƒ‘ã‚¹: {output_path}")
    print(f"   ã‚µã‚¤ã‚º: {file_size:.2f} MB")
    print(f"   è¡Œæ•°: {len(df_base):,}")
    print(f"   åˆ—æ•°: {len(df_base.columns)}")

    # CSVã§ã‚‚ã‚µãƒ³ãƒ—ãƒ«ã‚’ä¿å­˜ï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰
    csv_path = output_dir / "base_table_sample.csv"
    df_base.head(100).to_csv(csv_path, index=False, encoding='utf-8-sig')
    print(f"\n   ã‚µãƒ³ãƒ—ãƒ«CSV: {csv_path} (å…ˆé ­100è¡Œ)")


def check_feature_engine_compatibility(df_base):
    """FeatureEngineã¨ã®äº’æ›æ€§ãƒã‚§ãƒƒã‚¯"""
    print("\n" + "=" * 80)
    print("ğŸ”§ FeatureEngine äº’æ›æ€§ãƒã‚§ãƒƒã‚¯")
    print("=" * 80)

    # features.yaml ã§æœŸå¾…ã•ã‚Œã‚‹ã‚«ãƒ©ãƒ 
    required_cols = {
        'åŸºæœ¬': ['race_id', 'horse_id', 'race_date', 'age', 'sex', 'basis_weight'],
        'æˆç¸¾': ['finish_position', 'finish_time_seconds', 'last_3f_time'],
        'äºº': ['jockey_id', 'trainer_id'],
        'é¦¬ä½“': ['horse_weight', 'horse_weight_change'],
        'å¸‚å ´': ['win_odds', 'popularity'],
        'è¡€çµ±': ['sire_id', 'dam_id'],
    }

    print("\nå¿…è¦ã‚«ãƒ©ãƒ ã®å­˜åœ¨ç¢ºèª:")
    all_ok = True

    for category, cols in required_cols.items():
        missing = [col for col in cols if col not in df_base.columns]
        if missing:
            print(f"  âŒ {category}: ä¸è¶³ â†’ {', '.join(missing)}")
            all_ok = False
        else:
            print(f"  âœ… {category}: OK")

    if all_ok:
        print("\nğŸ‰ FeatureEngine ã§åˆ©ç”¨å¯èƒ½ãªãƒ™ãƒ¼ã‚¹ãƒ†ãƒ¼ãƒ–ãƒ«ã§ã™ï¼")
    else:
        print("\nâš ï¸ ä¸€éƒ¨ã®ã‚«ãƒ©ãƒ ãŒä¸è¶³ã—ã¦ã„ã¾ã™ãŒã€ä»£æ›¿æ‰‹æ®µã§å¯¾å¿œå¯èƒ½ã§ã™")


def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    print("=" * 80)
    print("ğŸ” KeibaAI_v2 åˆ†æç”¨ãƒ™ãƒ¼ã‚¹ãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆ")
    print(f"â° å®Ÿè¡Œæ™‚åˆ»: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 80)

    # ã‚¹ãƒ†ãƒƒãƒ—1: ãƒ•ã‚¡ã‚¤ãƒ«ãƒ­ãƒ¼ãƒ‰
    df_races, df_shutuba, df_horses, df_pedigrees = load_parquet_files()

    # ã‚¹ãƒ†ãƒƒãƒ—2: çµåˆã‚­ãƒ¼åˆ†æ
    analyze_join_keys(df_races, df_shutuba, df_horses, df_pedigrees)

    # ã‚¹ãƒ†ãƒƒãƒ—3: ãƒ™ãƒ¼ã‚¹ãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆ
    df_base = create_base_table(df_races, df_shutuba, df_horses, df_pedigrees)

    # ã‚¹ãƒ†ãƒƒãƒ—4: å“è³ªåˆ†æ
    analyze_base_table(df_base)

    # ã‚¹ãƒ†ãƒƒãƒ—5: FeatureEngineäº’æ›æ€§ãƒã‚§ãƒƒã‚¯
    check_feature_engine_compatibility(df_base)

    # ã‚¹ãƒ†ãƒƒãƒ—6: ä¿å­˜
    save_base_table(df_base)

    print("\n" + "=" * 80)
    print("ğŸ“‹ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—")
    print("=" * 80)
    print("""
ğŸ’¡ ãƒ™ãƒ¼ã‚¹ãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆå®Œäº†ï¼æ¬¡ã¯ä»¥ä¸‹ã‚’å®Ÿè¡Œã—ã¾ã™:

1. FeatureEngine ã§ã“ã®ãƒ†ãƒ¼ãƒ–ãƒ«ã‹ã‚‰ç‰¹å¾´é‡ã‚’ç”Ÿæˆ
   â†’ python keibaai/src/features/generate_features.py

2. ç”Ÿæˆã•ã‚ŒãŸç‰¹å¾´é‡ã®ç¢ºèª
   â†’ debug_05_check_features.py ã‚’å®Ÿè¡Œ

3. ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ãƒ»è©•ä¾¡
   â†’ python keibaai/src/models/train_mu_model.py

4. å•é¡Œãªã‘ã‚Œã°å…¨20,157ä»¶ã®ãƒ‘ãƒ¼ã‚¹ã‚’å®Ÿè¡Œ
    """)

    print("=" * 80)
    print("âœ… ãƒ‡ãƒãƒƒã‚°ã‚¹ã‚¯ãƒªãƒ—ãƒˆ04å®Œäº†")
    print("=" * 80)


if __name__ == "__main__":
    main()
