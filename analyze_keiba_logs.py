#!/usr/bin/env python3
"""
KeibaAI_v2 Log Analyzer
ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã¨ãƒ‘ãƒ¼ã‚¹ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®ãƒ­ã‚°ã‚’åˆ†æã—ã€çµ±è¨ˆæƒ…å ±ã‚’å‡ºåŠ›ã™ã‚‹ãƒ„ãƒ¼ãƒ«
"""

import re
from pathlib import Path
from datetime import datetime
from collections import defaultdict, Counter
from typing import Dict, List, Tuple
import json


class KeibaLogAnalyzer:
    """ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’åˆ†æã™ã‚‹ã‚¯ãƒ©ã‚¹"""

    def __init__(self, log_dir: Path):
        self.log_dir = Path(log_dir)
        self.log_entries = []
        self.stats = {
            'total_entries': 0,
            'by_level': Counter(),
            'by_module': Counter(),
            'errors': [],
            'warnings': [],
            'scraping_stats': {
                'races_scraped': 0,
                'shutuba_scraped': 0,
                'horses_scraped': 0,
                'pedigrees_scraped': 0,
                'failed_scrapes': 0,
                'http_errors': Counter(),
            },
            'parsing_stats': {
                'races_parsed': 0,
                'shutuba_parsed': 0,
                'horses_parsed': 0,
                'pedigrees_parsed': 0,
                'parse_errors': 0,
                'missing_data': Counter(),
            },
            'execution_time': {},
        }

    def find_log_files(self) -> List[Path]:
        """ãƒ­ã‚°ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®ã™ã¹ã¦ã®ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢"""
        if not self.log_dir.exists():
            print(f"âš ï¸  ãƒ­ã‚°ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã—ã¾ã›ã‚“: {self.log_dir}")
            return []

        log_files = sorted(self.log_dir.glob("*.log"))
        return log_files

    def parse_log_line(self, line: str) -> Dict:
        """ãƒ­ã‚°è¡Œã‚’ãƒ‘ãƒ¼ã‚¹ã—ã¦æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ã«å¤‰æ›"""
        # æ¨™æº–ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ: "2025-11-18 10:30:45,123 - INFO - module_name - message"
        # ã¾ãŸã¯: "2025-11-18 10:30:45,123 - INFO - message"
        pattern = r'(\d{4}-\d{2}-\d{2}\s+\d{2}:\d{2}:\d{2},\d{3})\s+-\s+(\w+)\s+-\s+(.+)'
        match = re.match(pattern, line)

        if match:
            timestamp_str, level, message = match.groups()
            try:
                timestamp = datetime.strptime(timestamp_str, '%Y-%m-%d %H:%M:%S,%f')
            except:
                timestamp = None

            return {
                'timestamp': timestamp,
                'level': level,
                'message': message.strip(),
                'raw': line
            }
        return None

    def analyze_log_file(self, log_file: Path):
        """å˜ä¸€ã®ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’åˆ†æ"""
        print(f"ğŸ“„ åˆ†æä¸­: {log_file.name}")

        try:
            with open(log_file, 'r', encoding='utf-8') as f:
                for line in f:
                    line = line.strip()
                    if not line:
                        continue

                    entry = self.parse_log_line(line)
                    if entry:
                        self.log_entries.append(entry)
                        self.stats['total_entries'] += 1
                        self.stats['by_level'][entry['level']] += 1

                        # ã‚¨ãƒ©ãƒ¼ã¨è­¦å‘Šã‚’è¨˜éŒ²
                        if entry['level'] == 'ERROR':
                            self.stats['errors'].append(entry)
                        elif entry['level'] == 'WARNING':
                            self.stats['warnings'].append(entry)

                        # ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°çµ±è¨ˆã‚’æŠ½å‡º
                        self._extract_scraping_stats(entry)

                        # ãƒ‘ãƒ¼ã‚¹çµ±è¨ˆã‚’æŠ½å‡º
                        self._extract_parsing_stats(entry)

                        # å®Ÿè¡Œæ™‚é–“ã‚’æŠ½å‡º
                        self._extract_execution_time(entry)

        except Exception as e:
            print(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {log_file.name} - {e}")

    def _extract_scraping_stats(self, entry: Dict):
        """ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–¢é€£ã®çµ±è¨ˆã‚’æŠ½å‡º"""
        msg = entry['message'].lower()

        # ãƒ¬ãƒ¼ã‚¹çµæœã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°
        if 'scraping race' in msg or 'scraped race' in msg:
            self.stats['scraping_stats']['races_scraped'] += 1

        # å‡ºé¦¬è¡¨ã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°
        if 'scraping shutuba' in msg or 'scraped shutuba' in msg:
            self.stats['scraping_stats']['shutuba_scraped'] += 1

        # é¦¬æƒ…å ±ã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°
        if 'scraping horse' in msg or 'scraped horse' in msg:
            self.stats['scraping_stats']['horses_scraped'] += 1

        # è¡€çµ±æƒ…å ±ã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°
        if 'scraping pedigree' in msg or 'scraped pedigree' in msg:
            self.stats['scraping_stats']['pedigrees_scraped'] += 1

        # HTTPã‚¨ãƒ©ãƒ¼
        if 'http' in msg and ('error' in msg or 'failed' in msg):
            self.stats['scraping_stats']['failed_scrapes'] += 1

            # HTTPã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ¼ãƒ‰ã‚’æŠ½å‡º
            status_match = re.search(r'(\d{3})', entry['message'])
            if status_match:
                status_code = status_match.group(1)
                self.stats['scraping_stats']['http_errors'][status_code] += 1

    def _extract_parsing_stats(self, entry: Dict):
        """ãƒ‘ãƒ¼ã‚¹é–¢é€£ã®çµ±è¨ˆã‚’æŠ½å‡º"""
        msg = entry['message'].lower()

        # ãƒ¬ãƒ¼ã‚¹çµæœã®ãƒ‘ãƒ¼ã‚¹
        if 'parsing race' in msg or 'parsed race' in msg:
            self.stats['parsing_stats']['races_parsed'] += 1

        # å‡ºé¦¬è¡¨ã®ãƒ‘ãƒ¼ã‚¹
        if 'parsing shutuba' in msg or 'parsed shutuba' in msg:
            self.stats['parsing_stats']['shutuba_parsed'] += 1

        # é¦¬æƒ…å ±ã®ãƒ‘ãƒ¼ã‚¹
        if 'parsing horse' in msg or 'parsed horse' in msg:
            self.stats['parsing_stats']['horses_parsed'] += 1

        # è¡€çµ±æƒ…å ±ã®ãƒ‘ãƒ¼ã‚¹
        if 'parsing pedigree' in msg or 'parsed pedigree' in msg:
            self.stats['parsing_stats']['pedigrees_parsed'] += 1

        # ãƒ‘ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼
        if entry['level'] == 'ERROR' and 'pars' in msg:
            self.stats['parsing_stats']['parse_errors'] += 1

        # æ¬ æãƒ‡ãƒ¼ã‚¿
        if 'missing' in msg or 'not found' in msg:
            # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‹ã‚‰æ¬ æãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’æŠ½å‡º
            field_match = re.search(r'(\w+)\s+(missing|not found)', entry['message'])
            if field_match:
                field = field_match.group(1)
                self.stats['parsing_stats']['missing_data'][field] += 1

    def _extract_execution_time(self, entry: Dict):
        """å®Ÿè¡Œæ™‚é–“ã‚’æŠ½å‡º"""
        msg = entry['message']

        # "Completed in X seconds" ãƒ‘ã‚¿ãƒ¼ãƒ³
        time_match = re.search(r'completed in\s+([\d.]+)\s+seconds', msg, re.IGNORECASE)
        if time_match:
            seconds = float(time_match.group(1))

            # ã©ã®å‡¦ç†ã®æ™‚é–“ã‹åˆ¤å®š
            if 'scraping' in msg.lower():
                self.stats['execution_time']['scraping'] = seconds
            elif 'parsing' in msg.lower():
                self.stats['execution_time']['parsing'] = seconds
            elif 'total' in msg.lower() or 'pipeline' in msg.lower():
                self.stats['execution_time']['total'] = seconds

    def generate_report(self) -> str:
        """åˆ†æçµæœã®ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ"""
        report = []
        report.append("=" * 80)
        report.append("ğŸ“Š KeibaAI_v2 ãƒ­ã‚°åˆ†æãƒ¬ãƒãƒ¼ãƒˆ")
        report.append("=" * 80)
        report.append(f"ğŸ“ ãƒ­ã‚°ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {self.log_dir}")
        report.append(f"ğŸ“… åˆ†ææ—¥æ™‚: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report.append("")

        # ç·åˆçµ±è¨ˆ
        report.append("â–  ç·åˆçµ±è¨ˆ")
        report.append(f"  â€¢ ç·ãƒ­ã‚°ã‚¨ãƒ³ãƒˆãƒªæ•°: {self.stats['total_entries']:,}")
        report.append("")

        # ãƒ­ã‚°ãƒ¬ãƒ™ãƒ«åˆ¥
        report.append("â–  ãƒ­ã‚°ãƒ¬ãƒ™ãƒ«åˆ¥é›†è¨ˆ")
        for level in ['DEBUG', 'INFO', 'WARNING', 'ERROR', 'CRITICAL']:
            count = self.stats['by_level'].get(level, 0)
            if count > 0:
                percentage = (count / self.stats['total_entries'] * 100) if self.stats['total_entries'] > 0 else 0
                report.append(f"  â€¢ {level:8s}: {count:6,} ({percentage:5.1f}%)")
        report.append("")

        # ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°çµ±è¨ˆ
        report.append("â–  ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°çµ±è¨ˆ")
        scraping = self.stats['scraping_stats']
        report.append(f"  â€¢ ãƒ¬ãƒ¼ã‚¹çµæœ: {scraping['races_scraped']:,} ä»¶")
        report.append(f"  â€¢ å‡ºé¦¬è¡¨: {scraping['shutuba_scraped']:,} ä»¶")
        report.append(f"  â€¢ é¦¬æƒ…å ±: {scraping['horses_scraped']:,} ä»¶")
        report.append(f"  â€¢ è¡€çµ±æƒ…å ±: {scraping['pedigrees_scraped']:,} ä»¶")
        report.append(f"  â€¢ å¤±æ•—: {scraping['failed_scrapes']:,} ä»¶")

        if scraping['http_errors']:
            report.append("  â€¢ HTTPã‚¨ãƒ©ãƒ¼:")
            for status, count in sorted(scraping['http_errors'].items()):
                report.append(f"    - {status}: {count:,} ä»¶")
        report.append("")

        # ãƒ‘ãƒ¼ã‚¹çµ±è¨ˆ
        report.append("â–  ãƒ‘ãƒ¼ã‚¹çµ±è¨ˆ")
        parsing = self.stats['parsing_stats']
        report.append(f"  â€¢ ãƒ¬ãƒ¼ã‚¹çµæœ: {parsing['races_parsed']:,} ä»¶")
        report.append(f"  â€¢ å‡ºé¦¬è¡¨: {parsing['shutuba_parsed']:,} ä»¶")
        report.append(f"  â€¢ é¦¬æƒ…å ±: {parsing['horses_parsed']:,} ä»¶")
        report.append(f"  â€¢ è¡€çµ±æƒ…å ±: {parsing['pedigrees_parsed']:,} ä»¶")
        report.append(f"  â€¢ ãƒ‘ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼: {parsing['parse_errors']:,} ä»¶")

        if parsing['missing_data']:
            report.append("  â€¢ æ¬ æãƒ‡ãƒ¼ã‚¿ (ä¸Šä½10ä»¶):")
            for field, count in parsing['missing_data'].most_common(10):
                report.append(f"    - {field}: {count:,} ä»¶")
        report.append("")

        # å®Ÿè¡Œæ™‚é–“
        if self.stats['execution_time']:
            report.append("â–  å®Ÿè¡Œæ™‚é–“")
            for process, seconds in self.stats['execution_time'].items():
                minutes = seconds / 60
                hours = minutes / 60
                if hours >= 1:
                    report.append(f"  â€¢ {process}: {hours:.2f} æ™‚é–“ ({seconds:,.1f} ç§’)")
                elif minutes >= 1:
                    report.append(f"  â€¢ {process}: {minutes:.2f} åˆ† ({seconds:,.1f} ç§’)")
                else:
                    report.append(f"  â€¢ {process}: {seconds:.1f} ç§’")
            report.append("")

        # ã‚¨ãƒ©ãƒ¼è©³ç´°ï¼ˆæœ€æ–°10ä»¶ï¼‰
        if self.stats['errors']:
            report.append("â–  ã‚¨ãƒ©ãƒ¼è©³ç´° (æœ€æ–°10ä»¶)")
            for i, error in enumerate(self.stats['errors'][-10:], 1):
                timestamp = error['timestamp'].strftime('%H:%M:%S') if error['timestamp'] else 'N/A'
                report.append(f"  {i}. [{timestamp}] {error['message'][:100]}")
                if len(error['message']) > 100:
                    report.append(f"     ... (çœç•¥)")
            report.append("")

        # è­¦å‘Šè©³ç´°ï¼ˆæœ€æ–°10ä»¶ï¼‰
        if self.stats['warnings']:
            report.append("â–  è­¦å‘Šè©³ç´° (æœ€æ–°10ä»¶)")
            for i, warning in enumerate(self.stats['warnings'][-10:], 1):
                timestamp = warning['timestamp'].strftime('%H:%M:%S') if warning['timestamp'] else 'N/A'
                report.append(f"  {i}. [{timestamp}] {warning['message'][:100]}")
                if len(warning['message']) > 100:
                    report.append(f"     ... (çœç•¥)")
            report.append("")

        report.append("=" * 80)
        return "\n".join(report)

    def save_detailed_report(self, output_path: Path):
        """è©³ç´°ãªJSONå½¢å¼ã®ãƒ¬ãƒãƒ¼ãƒˆã‚’ä¿å­˜"""
        detailed_report = {
            'metadata': {
                'log_directory': str(self.log_dir),
                'analysis_datetime': datetime.now().isoformat(),
                'total_entries': self.stats['total_entries'],
            },
            'log_levels': dict(self.stats['by_level']),
            'scraping_stats': {
                k: v if not isinstance(v, Counter) else dict(v)
                for k, v in self.stats['scraping_stats'].items()
            },
            'parsing_stats': {
                k: v if not isinstance(v, Counter) else dict(v)
                for k, v in self.stats['parsing_stats'].items()
            },
            'execution_time': self.stats['execution_time'],
            'errors': [
                {
                    'timestamp': e['timestamp'].isoformat() if e['timestamp'] else None,
                    'message': e['message']
                }
                for e in self.stats['errors']
            ],
            'warnings': [
                {
                    'timestamp': w['timestamp'].isoformat() if w['timestamp'] else None,
                    'message': w['message']
                }
                for w in self.stats['warnings']
            ],
        }

        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(detailed_report, f, ensure_ascii=False, indent=2)

        print(f"ğŸ’¾ è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆã‚’ä¿å­˜: {output_path}")

    def analyze(self):
        """ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã®åˆ†æã‚’å®Ÿè¡Œ"""
        log_files = self.find_log_files()

        if not log_files:
            print("âš ï¸  ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
            return

        print(f"ğŸ“Š {len(log_files)} å€‹ã®ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’åˆ†æã—ã¾ã™\n")

        for log_file in log_files:
            self.analyze_log_file(log_file)

        print("\nâœ… åˆ†æå®Œäº†!\n")


def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    import sys
    from datetime import date

    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ãƒ­ã‚°ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªï¼ˆä»Šæ—¥ã®æ—¥ä»˜ï¼‰
    today = date.today()
    default_log_dir = Path(f"keibaai/data/logs/{today.year}/{today.month:02d}/{today.day:02d}")

    # ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã§ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’æŒ‡å®šå¯èƒ½
    if len(sys.argv) > 1:
        log_dir = Path(sys.argv[1])
    else:
        log_dir = default_log_dir

    print(f"ğŸ” KeibaAI_v2 ãƒ­ã‚°ã‚¢ãƒŠãƒ©ã‚¤ã‚¶ãƒ¼")
    print(f"ğŸ“ ãƒ­ã‚°ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {log_dir}\n")

    # ã‚¢ãƒŠãƒ©ã‚¤ã‚¶ãƒ¼ã‚’åˆæœŸåŒ–ã—ã¦å®Ÿè¡Œ
    analyzer = KeibaLogAnalyzer(log_dir)
    analyzer.analyze()

    # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
    report = analyzer.generate_report()
    print(report)

    # è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆã‚’JSONå½¢å¼ã§ä¿å­˜
    output_dir = Path("keibaai/data/logs/analysis")
    output_dir.mkdir(parents=True, exist_ok=True)

    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    json_output = output_dir / f"log_analysis_{timestamp}.json"
    analyzer.save_detailed_report(json_output)

    # ã‚µãƒãƒªãƒ¼ã‚’ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã«ã‚‚ä¿å­˜
    txt_output = output_dir / f"log_analysis_{timestamp}.txt"
    with open(txt_output, 'w', encoding='utf-8') as f:
        f.write(report)
    print(f"ğŸ’¾ ã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆã‚’ä¿å­˜: {txt_output}")

    print("\n" + "=" * 80)
    print("ğŸ“ˆ åˆ†æå®Œäº†! è©³ç´°ã¯ä¸Šè¨˜ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚")
    print("=" * 80)


if __name__ == "__main__":
    main()
