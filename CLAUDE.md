# CLAUDE.md - KeibaAI_v2 Developer Guide for AI Assistants

**Last Updated**: 2025-11-16
**Project**: KeibaAI_v2 - Horse Racing AI Prediction & Optimal Investment System
**Purpose**: Guide for AI assistants working with this codebase

---

## üìã Table of Contents

1. [Project Overview](#project-overview)
2. [Architecture & Codebase Structure](#architecture--codebase-structure)
3. [Data Pipeline Flow](#data-pipeline-flow)
4. [Key Modules & Responsibilities](#key-modules--responsibilities)
5. [Development Workflows](#development-workflows)
6. [Configuration Management](#configuration-management)
7. [Testing Infrastructure](#testing-infrastructure)
8. [Common Tasks](#common-tasks)
9. [Code Conventions & Patterns](#code-conventions--patterns)
10. [Important Notes & Gotchas](#important-notes--gotchas)
11. [File Organization](#file-organization)
12. [Git Workflow](#git-workflow)

---

## üéØ Project Overview

### What is KeibaAI_v2?

A **sophisticated horse racing AI prediction and optimal investment system** that:
- Scrapes horse racing data from netkeiba.com and JRA sources
- Parses HTML to structured data (Parquet format)
- Generates ML features from race history, pedigrees, and performance data
- Trains probabilistic models (Œº, œÉ, ŒΩ) using LightGBM
- Runs Monte Carlo simulations to estimate win probabilities
- Optimizes betting portfolios using Fractional Kelly Criterion
- Tracks performance metrics (Brier score, ECE, ROI)

### Key Statistics

- **~5,025 lines** of core production code
- **278,098 race records** in current dataset
- **1,377,361 pedigree records** (5 generations)
- **100% test-driven** with unit, integration, and regression tests
- **Zero cloud costs** - fully local/personal use system

### Technology Stack

- **Python 3.10+** with type hints
- **Data**: pandas, pyarrow (Parquet), NumPy
- **ML**: LightGBM, scikit-learn
- **Scraping**: requests, BeautifulSoup4, Selenium
- **Storage**: Parquet files, SQLite metadata
- **Testing**: pytest
- **Config**: YAML files

---

## üèóÔ∏è Architecture & Codebase Structure

### High-Level Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    KeibaAI_v2 System                         ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  [Scraping] ‚Üí [Parsing] ‚Üí [Features] ‚Üí [Models]             ‚îÇ
‚îÇ      ‚Üì           ‚Üì            ‚Üì            ‚Üì                 ‚îÇ
‚îÇ   HTML.bin   Parquet    Feature.pq   LightGBM               ‚îÇ
‚îÇ                                          ‚Üì                   ‚îÇ
‚îÇ              [Simulation] ‚Üí [Optimization] ‚Üí [Execution]    ‚îÇ
‚îÇ                   ‚Üì              ‚Üì               ‚Üì           ‚îÇ
‚îÇ              MC Probs      Kelly Bets      Paper Trade       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Directory Structure

```
KeibaAI_v2/
‚îú‚îÄ‚îÄ keibaai/                          # Main application package
‚îÇ   ‚îú‚îÄ‚îÄ src/                          # Source code
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ modules/                  # Core business logic
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ preparing/           # Web scraping (23KB scraper)
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ parsers/             # 4 HTML parsers (results, shutuba, horse, pedigree)
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ features/            # Feature engineering (24KB engine)
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ models/              # LightGBM models (Œº, œÉ, ŒΩ)
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ sim/                 # Monte Carlo simulator
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ optimizer/           # Portfolio optimization (Kelly)
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ executor/            # Order execution (disabled)
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ monitoring/          # Performance tracking
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ constants/           # URL and path constants
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ features/                # Feature generation scripts
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ models/                  # Model training scripts
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ optimizer/               # Optimization scripts
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ sim/                     # Simulation scripts
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ utils/                   # Utility functions
‚îÇ   ‚îú‚îÄ‚îÄ configs/                      # 5 YAML configuration files
‚îÇ   ‚îú‚îÄ‚îÄ data/                         # Local data storage (GITIGNORED)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ raw/html/                # Scraped HTML (.bin files, EUC-JP)
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ race/                # Race results
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ shutuba/             # Entry lists
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ horse/               # Horse profiles (*_profile.bin, *_perf.bin)
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ped/                 # Pedigree data
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ parsed/parquet/          # Parsed Parquet files
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ races/               # races.parquet (278K records, 27+ columns)
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ shutuba/             # shutuba.parquet (278K records, 21 columns)
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ horses/              # horses.parquet (profiles)
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ pedigrees/           # pedigrees.parquet (1.3M records)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ features/parquet/        # Generated features (partitioned by year/month)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ models/                  # Trained models (LightGBM, calibration)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ simulations/             # Simulation results (JSON)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ orders/                  # Betting orders (logs, JSON)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ logs/                    # Application logs (by date)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ metadata/db.sqlite3      # Metadata database
‚îÇ   ‚îú‚îÄ‚îÄ tests/                        # Test suite
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ unit/                    # Unit tests
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ integration/             # Integration tests
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ regression/              # Regression tests
‚îÇ   ‚îú‚îÄ‚îÄ infra/                        # Infrastructure code
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ docker/                  # Docker setup
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ sched/cron_examples/     # Cron job examples
‚îÇ   ‚îú‚îÄ‚îÄ scripts/                      # Utility scripts
‚îÇ   ‚îî‚îÄ‚îÄ notebooks/                    # Jupyter notebooks
‚îú‚îÄ‚îÄ (root debug scripts)              # 20+ debugging utilities
‚îú‚îÄ‚îÄ ÊåáÁ§∫.md                           # Japanese instructions/requirements
‚îú‚îÄ‚îÄ schema.md                         # Data schema documentation
‚îú‚îÄ‚îÄ PROGRESS.md                       # Data quality progress tracking
‚îú‚îÄ‚îÄ DEBUG_REPORT.md                   # HTML parser improvements
‚îî‚îÄ‚îÄ CLAUDE.md                         # This file
```

---

## üîÑ Data Pipeline Flow

### Complete End-to-End Pipeline

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ PHASE 0: Data Acquisition (Deep Night Batch - 03:00)          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    ‚îú‚îÄ‚ñ∫ [Scrape race dates] ‚Üí kaisai_dates.csv
    ‚îú‚îÄ‚ñ∫ [Scrape race IDs] ‚Üí race_id_list.csv
    ‚îú‚îÄ‚ñ∫ [Scrape race results] ‚Üí data/raw/html/race/*.bin
    ‚îú‚îÄ‚ñ∫ [Scrape entry lists] ‚Üí data/raw/html/shutuba/*.bin
    ‚îú‚îÄ‚ñ∫ [Extract horse IDs] ‚Üí horse_id_list.txt
    ‚îú‚îÄ‚ñ∫ [Scrape horse profiles] ‚Üí data/raw/html/horse/*_profile.bin
    ‚îú‚îÄ‚ñ∫ [Scrape horse performance] ‚Üí data/raw/html/horse/*_perf.bin
    ‚îî‚îÄ‚ñ∫ [Scrape pedigrees] ‚Üí data/raw/html/ped/*.bin

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ PHASE 1: Parsing (Immediately After Scraping)                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    ‚îú‚îÄ‚ñ∫ [Parse race results] ‚Üí races.parquet (27+ columns)
    ‚îú‚îÄ‚ñ∫ [Parse entry lists] ‚Üí shutuba.parquet (21 columns)
    ‚îú‚îÄ‚ñ∫ [Parse horse profiles] ‚Üí horses.parquet
    ‚îî‚îÄ‚ñ∫ [Parse pedigrees] ‚Üí pedigrees.parquet (5 generations)

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ PHASE 2: Feature Engineering (After Parsing)                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    ‚îú‚îÄ‚ñ∫ Load: races, shutuba, horses, pedigrees
    ‚îú‚îÄ‚ñ∫ Generate: race features (distance, surface, weather)
    ‚îú‚îÄ‚ñ∫ Generate: horse features (rolling stats, trends)
    ‚îú‚îÄ‚ñ∫ Generate: jockey/trainer features (win rates, ROI)
    ‚îú‚îÄ‚ñ∫ Generate: pedigree features (sire/dam encoding)
    ‚îî‚îÄ‚ñ∫ Save: features/parquet/year=YYYY/month=MM/ (partitioned)

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ PHASE 3: Model Training (Weekly/On-Demand)                    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    ‚îú‚îÄ‚ñ∫ Train Œº model (LightGBM Ranker + Regressor)
    ‚îú‚îÄ‚ñ∫ Train œÉ model (variance estimation)
    ‚îú‚îÄ‚ñ∫ Train ŒΩ model (chaos parameter, t-distribution)
    ‚îú‚îÄ‚ñ∫ Calibrate probabilities (temperature scaling)
    ‚îî‚îÄ‚ñ∫ Save models ‚Üí data/models/{model_id}/

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ PHASE 4: Daily Prediction (Race Day Morning - 10:00)          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    ‚îú‚îÄ‚ñ∫ Load today's features
    ‚îú‚îÄ‚ñ∫ Run Œº, œÉ, ŒΩ inference
    ‚îî‚îÄ‚ñ∫ Save predictions ‚Üí predictions/parquet/

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ PHASE 5: Simulation (10-30 min before race)                   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    ‚îú‚îÄ‚ñ∫ Load predictions (Œº, œÉ, ŒΩ)
    ‚îú‚îÄ‚ñ∫ Run Monte Carlo (K=1000 iterations)
    ‚îú‚îÄ‚ñ∫ Compute win/place/exacta probabilities
    ‚îî‚îÄ‚ñ∫ Save simulations ‚Üí simulations/{race_id}.json

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ PHASE 6: Optimization (Just before betting deadline)          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    ‚îú‚îÄ‚ñ∫ Scrape latest JRA odds (Selenium)
    ‚îú‚îÄ‚ñ∫ Load simulation results
    ‚îú‚îÄ‚ñ∫ Run Fractional Kelly optimization
    ‚îú‚îÄ‚ñ∫ Filter by EV threshold
    ‚îî‚îÄ‚ñ∫ Save orders ‚Üí orders/{race_id}_order.json

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ PHASE 7: Execution (MANUAL - Disabled by Default)             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    ‚îî‚îÄ‚ñ∫ Paper trading only (logs for backtesting)

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ PHASE 8: Monitoring (Post-Race)                               ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    ‚îú‚îÄ‚ñ∫ Scrape final results
    ‚îú‚îÄ‚ñ∫ Calculate actual returns
    ‚îú‚îÄ‚ñ∫ Update Brier score, ECE, ROI metrics
    ‚îî‚îÄ‚ñ∫ Save metrics ‚Üí metrics/weekly_report.json
```

---

## üîß Key Modules & Responsibilities

### 1. Preparing Module (`modules/preparing/`)

**Purpose**: Web scraping from netkeiba.com

**Key Files**:
- `_scrape_html.py` (23KB): Main scraping engine
  - Anti-ban measures: random user agents, sleep intervals, retry backoff
  - Handles HTTP 400 ‚Üí 60-second pause
  - Saves as `.bin` files (EUC-JP encoded HTML)
- `_scrape_jra_odds.py`: JRA official odds (currently stub)
- `_requests_utils.py`: HTTP utilities with robust error handling

**Technologies**: requests, BeautifulSoup4, Selenium

### 2. Parsers Module (`modules/parsers/`)

**Purpose**: Convert raw HTML to structured DataFrames

**4 Parsers**:

1. **`results_parser.py`** (14KB)
   - Parses race results HTML
   - Extracts: finish data, race metadata, jockey/trainer/owner info
   - **Recent improvements**: Enhanced metadata extraction (distance_m, track_surface, weather, venue, race_class, etc.)
   - **Robustness**: 4-level fallback for distance/surface extraction
   - Handles obstacle races, regional races, multiple HTML formats

2. **`shutuba_parser.py`** (14KB)
   - Parses entry lists (Âá∫È¶¨Ë°®)
   - Extracts: morning odds, blinkers, prediction marks

3. **`horse_info_parser.py`** (16KB)
   - Parses horse profile pages
   - Extracts: birth date, breeder, physical stats
   - Parses past performance tables

4. **`pedigree_parser.py`** (9KB)
   - Parses 5-generation pedigree trees
   - Extracts: ancestor IDs, names, coat colors, birth years

**Common Utilities** (`common_utils.py`):
- `parse_time_to_seconds()`: HH:MM.S ‚Üí seconds
- `parse_margin_to_seconds()`: body lengths ‚Üí seconds
- `parse_sex_age()`: "Áâ°3" ‚Üí sex="Áâ°", age=3
- `parse_horse_weight()`: "476(+2)" ‚Üí weight=476, change=+2

**Output Format**: pandas DataFrame ‚Üí Parquet files

### 3. Features Module (`features/`)

**Purpose**: Generate ML features from parsed data

**Key Components**:
- `feature_engine.py` (25KB): Central feature generation engine
- `generate_features.py` (10KB): CLI script for feature generation
- `advanced_features.py` (9KB): Advanced features (interaction terms, domain-specific)
- `time_series_features.py` (3KB): Rolling window calculations, trend detection

**Feature Categories**:
- **Race features**: distance, surface, weather, track_condition, class
- **Horse features**: age, sex, weight, weight_change, past performance aggregates
- **Jockey features**: win_rate, place_rate, ROI
- **Trainer features**: stable statistics
- **Pedigree features**: sire/dam performance encoding
- **Derived features**: pace_index, position_changes, popularity_finish_diff

### 4. Models Module (`models/`)

**Purpose**: Train probabilistic models

**Mathematical Framework**: Three-parameter model (Œº, œÉ, ŒΩ)
- **Œº (mu)**: Expected finish time for each horse
- **œÉ (sigma)**: Horse-specific variance (uncertainty)
- **ŒΩ (nu)**: Race-level "chaos" parameter (t-distribution degrees of freedom)

**Key Files**:
- `model_train.py`: MuEstimator class (LightGBM)
- `train_mu_model.py`: CLI script for Œº model training
- `sigma_estimator.py`: œÉ model (variance)
- `nu_estimator.py`: ŒΩ model (heavy-tailed distribution)
- `train_sigma_nu_models.py`: Combined œÉ and ŒΩ training
- `calibration.py`: Probability calibration (temperature scaling)
- `predict.py`: Inference script
- `evaluate_model.py`: Metrics (Brier score, log loss, accuracy)

### 5. Simulation Module (`sim/`)

**Purpose**: Monte Carlo simulation for win probability

**Algorithm**:
```python
For k = 1 to K (default K=1000):
    For each horse i:
        time_i,k ~ t_ŒΩ(Œº_i, œÉ_i¬≤)
    Sort horses by time
    Record finish order
Estimate P(win) = count(1st place) / K
```

**Key Files**:
- `simulator.py`: RaceSimulator class
- `simulate_daily_races.py`: CLI script for daily simulations

### 6. Optimization Module (`optimizer/`)

**Purpose**: Portfolio optimization using Fractional Kelly Criterion

**Formula**:
```
Maximize: Œ£ f_i * log(1 + b_i * o_i)
Subject to:
  - Œ£ f_i ‚â§ W_0 (capital constraint)
  - f_i ‚â• 0 (no short selling)
  - EV_i > threshold (expected value filter)
```

**Key Files**:
- `optimizer.py`: PortfolioOptimizer class
- `optimize_daily_races.py`: CLI script for daily optimization

### 7. Monitoring Module (`monitoring/`)

**Purpose**: Performance tracking

**Metrics**:
- Brier score (probability calibration)
- ECE (Expected Calibration Error)
- ROI (Return on Investment)
- Hit rate (accuracy)

---

## üõ†Ô∏è Development Workflows

### Workflow 1: Data Quality Improvement

**Evidence from codebase**:
- 20+ `debug_*.py` scripts in root
- `DEBUG_REPORT.md`: Documents HTML parser improvements
- `PROGRESS.md`: Tracks data quality issues

**Recent Fixes**:
1. Changed HTML parser from `lxml` to `html.parser` (compatibility)
2. Reduced race result missing data from 0.8% to near-zero
3. Added 11 new metadata columns to race results
4. Implemented 4-level fallback for distance/surface extraction

### Workflow 2: Parser Development

**Process**:
1. Write parser in `modules/parsers/`
2. Create debug script (e.g., `debug_scraping_and_parsing.py`)
3. Validate with `validate_parquet.py`, `validate_parsed_data.py`
4. Update `schema.md` with new columns
5. Run regression tests

**Key Pattern**:
```python
# Always use html.parser (NOT lxml)
soup = BeautifulSoup(html_text, 'html.parser')

# Always use nullable integer types
df['finish_position'] = df['finish_position'].astype('Int64')

# Always handle multiple HTML formats (fallback patterns)
race_data = soup.find('div', class_='data_intro')
if not race_data:
    race_data = soup.find('div', class_='diary_snap_cut')
if not race_data:
    race_data = soup.find('dl', class_='racedata')
```

### Workflow 3: Feature Engineering Experimentation

**Process**:
1. Explore data in Jupyter notebooks (`notebooks/`)
2. Implement features in `features/advanced_features.py`
3. Validate with `check_features_data.py`
4. Update `configs/features.yaml` to enable new features
5. Regenerate features with `generate_features.py`

### Workflow 4: Model Development Cycle

**Process**:
1. Train models weekly: `python src/models/train_mu_model.py`
2. Evaluate on validation set: `python src/models/evaluate_model.py`
3. Calibrate probabilities: Use calibration module
4. Backtest on historical data
5. Monitor Brier score and ECE

---

## ‚öôÔ∏è Configuration Management

### 5 YAML Configuration Files

Located in: `keibaai/configs/`

1. **`default.yaml`**: Base paths and logging
   ```yaml
   data_path: "data"
   database: {path: "${metadata_path}/db.sqlite3"}
   logging: {level: "INFO", format: "%(asctime)s - %(levelname)s - %(message)s"}
   ```

2. **`scraping.yaml`**: Scraping behavior
   - Date ranges, retry settings, sleep intervals

3. **`features.yaml`**: Feature generation settings
   ```yaml
   feature_engine: {version: "v1.0"}
   race_features: {enabled: true}
   horse_features: {enabled: true}
   jockey_features: {enabled: true}
   trainer_features: {enabled: true}
   pedigree_features: {enabled: true}
   output: {partition_by: [year, month]}
   ```

4. **`models.yaml`**: Model hyperparameters
   ```yaml
   models:
     lgbm_ranker:
       objective: "lambdarank"
       hyperparameters:
         n_estimators: 2000
         learning_rate: 0.01
         num_leaves: 31
   ```

5. **`optimization.yaml`**: Portfolio constraints
   - Kelly fraction, max bet size, EV thresholds

**Variable Substitution**: Supports `${data_path}` placeholders

---

## üß™ Testing Infrastructure

### Test Organization

Located in: `keibaai/tests/`

1. **Unit Tests** (`tests/unit/`):
   - `test_parsers.py`: Parser validation with fixture HTML
   - `test_features.py`: Feature engineering logic
   - `test_models.py`: Model components

2. **Integration Tests** (`tests/integration/`):
   - `test_pipeline_e2e.py`: End-to-end pipeline validation

3. **Regression Tests** (`tests/regression/`):
   - `test_parser_regression.py`: Ensures parser output consistency

### Test Fixtures

- Sample HTML files in `tests/fixtures/`
- Organized by type: `race_samples/`, `shutuba_samples/`, `horse_samples/`, `ped_samples/`

### Running Tests

```bash
# Run all tests
pytest

# Run specific test file
pytest tests/unit/test_parsers.py

# Run with coverage
pytest --cov=keibaai
```

---

## üìù Common Tasks

### Task 1: Scrape New Data

```bash
# Edit date range in script first
python keibaai/src/run_scraping_pipeline_local.py
```

**What it does**:
1. Scrapes race dates ‚Üí `kaisai_dates.csv`
2. Scrapes race IDs ‚Üí `race_id_list.csv`
3. Scrapes race results ‚Üí `data/raw/html/race/*.bin`
4. Scrapes entry lists ‚Üí `data/raw/html/shutuba/*.bin`
5. Scrapes horse data ‚Üí `data/raw/html/horse/*.bin`

### Task 2: Parse Scraped Data

```bash
python keibaai/src/run_parsing_pipeline_local.py
```

**What it does**:
1. Parses race results ‚Üí `data/parsed/parquet/races/races.parquet`
2. Parses shutuba ‚Üí `data/parsed/parquet/shutuba/shutuba.parquet`
3. Parses horses ‚Üí `data/parsed/parquet/horses/horses.parquet`
4. Parses pedigrees ‚Üí `data/parsed/parquet/pedigrees/pedigrees.parquet`

### Task 3: Generate Features

```bash
python keibaai/src/features/generate_features.py \
  --start_date 2023-01-01 \
  --end_date 2023-12-31
```

**Output**: `data/features/parquet/year=YYYY/month=MM/`

### Task 4: Train Models

```bash
# Train Œº model
python keibaai/src/models/train_mu_model.py

# Train œÉ and ŒΩ models
python keibaai/src/models/train_sigma_nu_models.py
```

### Task 5: Run Predictions

```bash
python keibaai/src/models/predict.py \
  --date 2023-10-01 \
  --model_dir data/models/latest
```

### Task 6: Run Simulation

```bash
python keibaai/src/sim/simulate_daily_races.py \
  --date 2023-10-01 \
  --K 1000
```

### Task 7: Optimize Portfolio

```bash
python keibaai/src/optimizer/optimize_daily_races.py \
  --date 2023-10-01 \
  --W_0 100000
```

### Task 8: Validate Data Quality

```bash
# Check parsed data
python check_parsed_data.py

# Check features
python check_features_data.py

# Validate parquet files
python validate_parquet.py
```

---

## üìê Code Conventions & Patterns

### 1. File Naming

- **Modules**: `snake_case.py`
- **Scripts**: `verb_object.py` (e.g., `generate_features.py`, `train_mu_model.py`)
- **Debug scripts**: `debug_*.py` (root directory)
- **Validation scripts**: `validate_*.py`, `check_*.py`

### 2. Data Files

- **Raw HTML**: `.bin` extension, EUC-JP encoding
- **Parsed data**: `.parquet` (columnar format)
- **Features**: `.parquet`, partitioned by year/month
- **Models**: Directory per model (e.g., `data/models/20231001_lgbm/`)
- **Logs**: JSON or text, organized by date

### 3. Pandas DataFrame Patterns

**Always use nullable integer types**:
```python
# ‚ùå BAD: float64 for integers
df['finish_position'] = df['finish_position'].astype('float64')

# ‚úÖ GOOD: nullable integer
df['finish_position'] = df['finish_position'].astype('Int64')
```

**Always use explicit dtype mapping**:
```python
int_columns = [
    'finish_position', 'bracket_number', 'horse_number', 'age',
    'passing_order_1', 'passing_order_2', 'passing_order_3', 'passing_order_4',
    'popularity', 'horse_weight', 'horse_weight_change',
    'distance_m', 'head_count', 'round_of_year', 'day_of_meeting'
]

for col in int_columns:
    if col in df.columns:
        df[col] = df[col].astype('Int64')
```

### 4. HTML Parsing Patterns

**Always use `html.parser` (NOT `lxml`)**:
```python
# ‚úÖ CORRECT
soup = BeautifulSoup(html_text, 'html.parser')

# ‚ùå WRONG
soup = BeautifulSoup(html_text, 'lxml')
```

**Always implement fallback patterns**:
```python
# 4-level fallback for robustness
race_data = soup.find('div', class_='data_intro')
if not race_data:
    race_data = soup.find('div', class_='diary_snap_cut')
if not race_data:
    race_data_dl = soup.find('dl', class_='racedata')
    if race_data_dl:
        race_data = race_data_dl.find('dd')
if not race_data:
    # Log warning and return empty
    logging.warning(f"Race data not found in {file_path}")
    return {}
```

### 5. Error Handling

**Always use try-except with logging**:
```python
try:
    df = parse_race_results(file_path)
except Exception as e:
    logging.error(f"Parse error in {file_path}: {e}")
    # Save to error database
    save_parse_failure(file_path, str(e))
    return pd.DataFrame()
```

### 6. Configuration Loading

**Always use centralized config loading**:
```python
from keibaai.src.utils.config_utils import load_config

config = load_config('default')  # Loads configs/default.yaml
```

### 7. Path Handling

**Always use Path objects**:
```python
from pathlib import Path

data_path = Path(config['data_path'])
parquet_path = data_path / 'parsed' / 'parquet' / 'races'
parquet_path.mkdir(parents=True, exist_ok=True)
```

### 8. Logging

**Always use module-level logger**:
```python
import logging

logger = logging.getLogger(__name__)

logger.info("Processing started")
logger.warning("Missing data detected")
logger.error("Parse failed", exc_info=True)
```

---

## ‚ö†Ô∏è Important Notes & Gotchas

### 1. HTML Encoding

**Issue**: Raw HTML files are **EUC-JP encoded**, not UTF-8

**Solution**:
```python
with open(file_path, 'rb') as f:
    html_bytes = f.read()

try:
    html_text = html_bytes.decode('euc_jp', errors='replace')
except:
    html_text = html_bytes.decode('utf-8', errors='replace')
```

### 2. Data Leakage Prevention

**CRITICAL**: Never use final odds in training

```python
# ‚ùå DATA LEAKAGE
X_train = features[['distance_m', 'win_odds', 'popularity']]

# ‚úÖ CORRECT (use morning odds only)
X_train = features[['distance_m', 'morning_odds', 'jockey_win_rate']]
```

**Reason**: Final odds are determined just before race start and already incorporate all public information. Using them in training would give artificially high accuracy that doesn't translate to real predictions.

### 3. Time Series Validation

**Always use TimeSeriesSplit, NOT KFold**:
```python
from sklearn.model_selection import TimeSeriesSplit

# ‚úÖ CORRECT
tscv = TimeSeriesSplit(n_splits=5)
for train_idx, valid_idx in tscv.split(X):
    # Train on past, validate on future
    X_train, X_valid = X.iloc[train_idx], X.iloc[valid_idx]
```

**Reason**: Horse racing data is time-dependent. Random splits cause data leakage from future to past.

### 4. Parquet Partitioning

**Features are partitioned by year/month**:
```python
# Reading partitioned data
import pyarrow.parquet as pq

table = pq.read_table('data/features/parquet/',
                      filters=[('year', '=', 2023), ('month', '=', 10)])
df = table.to_pandas()
```

### 5. Nullable vs. NaN

**Use pd.NA for missing values, NOT np.nan**:
```python
# ‚úÖ CORRECT
df['finish_position'] = pd.NA

# ‚ùå WRONG (converts to float)
df['finish_position'] = np.nan
```

### 6. Race ID Format

**Format**: `YYYYPPNNDDRR`
- `YYYY`: Year (4 digits)
- `PP`: Venue code (2 digits)
  - 05 = Tokyo (Êù±‰∫¨)
  - 06 = Nakayama (‰∏≠Â±±)
  - 08 = Kyoto (‰∫¨ÈÉΩ)
  - 09 = Hanshin (Èò™Á•û)
  - Others: See venue master data
- `NN`: Round number (2 digits) - e.g., 04 = 4th round
- `DD`: Day of meeting (2 digits) - e.g., 03 = 3rd day
- `RR`: Race number (2 digits) - e.g., 01 = Race 1, 12 = Race 12

**Example**: `202305040301` =
- 2023Âπ¥ (Year 2023)
- Êù±‰∫¨Á´∂È¶¨Â†¥ (Tokyo, code 05)
- Á¨¨4ÂõûÈñãÂÇ¨ (4th round)
- 3Êó•ÁõÆ (3rd day)
- 1R (Race 1)

**URL Example**: `https://db.netkeiba.com/race/202305040301`

### 7. Horse ID Format

**Format**: `YYYYNNNNNN`
- `YYYY`: Birth year (4 digits)
- `NNNNNN`: Sequential number (6 digits)

**Example**: `2009100502` = Born in 2009, ID 100502

### 8. Disabled Executor

**The order executor is DISABLED by default**:
- Legal/ethical concerns about automated gambling
- Only generates order logs for paper trading
- Enable only for research/backtesting

### 9. Memory Management

**Large datasets require memory optimization**:
```python
# Use categorical dtypes for repeated strings
df['jockey_name'] = df['jockey_name'].astype('category')
df['track_surface'] = df['track_surface'].astype('category')

# Use nullable integers
df['finish_position'] = df['finish_position'].astype('Int64')
```

### 10. Anti-Ban Strategy

**Scraping must respect server load**:
- Random sleep: 2.5-5 seconds between requests
- HTTP 400 detection ‚Üí 60-second pause
- Random user agents
- Retry with exponential backoff

**Do NOT**:
- Run multiple scrapers in parallel
- Reduce sleep intervals below 2 seconds
- Ignore HTTP 400/403 errors

---

## üìÇ File Organization

### Data Files (.gitignored)

```
data/
‚îú‚îÄ‚îÄ raw/html/              # Never commit (large binary files)
‚îú‚îÄ‚îÄ parsed/parquet/        # Never commit (generated from raw)
‚îú‚îÄ‚îÄ features/parquet/      # Never commit (generated from parsed)
‚îú‚îÄ‚îÄ models/                # Never commit (large model files)
‚îú‚îÄ‚îÄ simulations/           # Never commit (ephemeral)
‚îú‚îÄ‚îÄ orders/                # Commit sample only
‚îú‚îÄ‚îÄ logs/                  # Never commit
‚îî‚îÄ‚îÄ metadata/db.sqlite3    # Consider committing schema only
```

### Documentation Files (commit these)

```
root/
‚îú‚îÄ‚îÄ schema.md              # Data schema documentation
‚îú‚îÄ‚îÄ PROGRESS.md            # Data quality tracking
‚îú‚îÄ‚îÄ DEBUG_REPORT.md        # Parser improvement reports
‚îú‚îÄ‚îÄ CLAUDE.md              # This file
‚îú‚îÄ‚îÄ ÊåáÁ§∫.md                # Japanese requirements/instructions
‚îî‚îÄ‚îÄ README.md              # User-facing documentation (TODO)
```

### Debug Scripts (commit selectively)

```
root/
‚îú‚îÄ‚îÄ debug_*.py             # Keep useful ones, remove obsolete
‚îú‚îÄ‚îÄ check_*.py             # Keep validation scripts
‚îú‚îÄ‚îÄ validate_*.py          # Keep validation scripts
‚îî‚îÄ‚îÄ inspect_*.py           # Keep inspection scripts
```

---

## üîÄ Git Workflow

### Branch Strategy

**Main branch**: `main` or `master` (production-ready code)

**Feature branches**: `claude/claude-md-mi12qj9z2fr7948a-01EkmapzBs2F3dK9kganLBYD` (session-specific)

### Commit Message Conventions

**Format**: `<type>: <description>`

**Types**:
- `feat`: New feature
- `fix`: Bug fix
- `refactor`: Code refactoring (no functional change)
- `perf`: Performance improvement
- `docs`: Documentation update
- `test`: Test addition/modification
- `chore`: Build/tooling changes

**Examples**:
```
feat: Add 11 new metadata columns to race results parser
fix: Use html.parser instead of lxml for better compatibility
refactor: Extract common HTML parsing utilities to shared module
perf: Optimize feature generation with vectorized operations
docs: Update schema.md with new race metadata columns
```

### Commit Workflow

```bash
# Make changes
git add .

# Commit with clear message
git commit -m "fix: Improve distance/surface extraction with 4-level fallback"

# Push to feature branch
git push -u origin claude/claude-md-mi12qj9z2fr7948a-01EkmapzBs2F3dK9kganLBYD
```

### Pull Request Guidelines

**When creating PR**:
1. Include summary of changes
2. Reference related issues/documents (e.g., "Fixes issue #123", "See DEBUG_REPORT.md")
3. List testing performed
4. Note any breaking changes

**Example PR template**:
```markdown
## Summary
Improved race metadata extraction by implementing 4-level fallback for distance/surface parsing.

## Changes
- Modified `results_parser.py` to try 4 different HTML selectors
- Reduced missing data from 10.2% to 0%
- Added regression tests for obstacle races

## Testing
- Validated with 440 race samples from 2023-10-09
- All regression tests pass
- No breaking changes to output schema

## References
- See DEBUG_REPORT.md for detailed analysis
```

---

## üéì Learning Resources

### Understanding the System

1. **Start with**: `schema.md` - Understand data structure
2. **Then read**: `DEBUG_REPORT.md` - Learn about recent improvements
3. **Review**: `PROGRESS.md` - Understand data quality journey
4. **Explore**: `ÊåáÁ§∫.md` - Original requirements (Japanese)

### Key Papers/Concepts

1. **Fractional Kelly Criterion**: Portfolio optimization under uncertainty
2. **LightGBM**: Gradient boosting for ranking
3. **Probability Calibration**: Temperature scaling, isotonic regression
4. **Monte Carlo Simulation**: Estimating probability distributions
5. **Expected Calibration Error (ECE)**: Measuring probability accuracy

### Similar Systems

- **betfair**: Betting exchange with trading API
- **pymc**: Probabilistic programming for Bayesian models
- **mlflow**: ML experiment tracking
- **optuna**: Hyperparameter optimization

---

## üöÄ Future Enhancements

### Short-term (1-3 months)

1. Complete JRA odds scraping (Selenium implementation)
2. Build monitoring dashboard (Streamlit/Dash)
3. Automate model retraining pipeline
4. Expand test coverage to 80%+

### Medium-term (3-6 months)

1. Add support for different bet types (trifecta, quinella)
2. Implement ensemble models (stacking)
3. Add feature importance visualization
4. Create comprehensive API documentation

### Long-term (6-12 months)

1. Deploy as web service (optional)
2. Add real-time odds tracking
3. Implement reinforcement learning for dynamic betting
4. Create mobile app for notifications

---

## üìû Contact & Support

### For AI Assistants

When working with this codebase:
1. **Always read**: `schema.md`, `PROGRESS.md`, `DEBUG_REPORT.md` first
2. **Never commit**: Large data files (`.bin`, `.parquet`, models)
3. **Always validate**: Changes with regression tests
4. **Always document**: New features in this file
5. **Always follow**: Code conventions in this guide

### For Human Developers

- **GitHub Issues**: Report bugs, request features
- **Pull Requests**: Contribute improvements
- **Documentation**: Update CLAUDE.md when making significant changes

---

## üìú Version History

| Version | Date       | Changes                                      |
|---------|------------|----------------------------------------------|
| 1.0     | 2025-11-16 | Initial comprehensive CLAUDE.md creation     |

---

**End of CLAUDE.md**

This document should be updated whenever:
- New modules are added
- Data schemas change significantly
- New workflows are established
- Important patterns emerge
- Critical bugs are discovered and fixed
