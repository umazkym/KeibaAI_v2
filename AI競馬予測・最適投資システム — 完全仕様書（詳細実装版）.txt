# AI競馬予測・最適投資システム — 完全仕様書（詳細実装版）

## 前提（改訂版）

### 目的
個人研究/テスト用途で、継続的な金銭支出をゼロに抑えつつ、機械学習ベースでレース確率を推定し、期待値計算やポートフォリオ最適化（紙上・シミュレーション）を実施する。実際の自動発注は当面無効（発注仕様・ログ生成まで）とする。

### データ取得
- スクレイピング（netkeiba / JRA 等）を継続
- 商用運用前に利用規約確認を必須とする（個人/研究目的でも要注意）
- プロキシ/有料API/有料スクレイピングサービスは使わない

### 実装環境
- **言語**: Python 3.10+（推奨）
- **パッケージ管理**: poetry または pip + venv（無償）
- **タイムゾーン**: Asia/Tokyo（全てのタイムスタンプは ISO8601+09:00 で記録）

### インフラ想定（費用ゼロ）
- **保存**: ローカルファイルシステム (例: `/data/keibaai/`)。外付けHDD/NASを推奨（容量確保のため）
- **Orchestration**: cron / systemd timers / GitHub Actions (無料枠) を利用
- **コンテナ**: Docker は任意（ローカルでの再現性向上のために可）
- **No paid managed services**: S3, Kubernetes, paid DB, paid proxies, commercial monitoring は使わない

---

## 開発リポジトリ構成（ルートは `keibaai/`）


keibaai/
├── src/
│   ├── pipeline_core.py                    # コアユーティリティ（atomic write等）
│   ├── run_scraping_pipeline_local.py      # スクレイピングパイプライン（日付ハードコード版）
│   ├── run_scraping_pipeline_with_args.py  # スクレイピングパイプライン（引数対応版・推奨）
│   ├── run_parsing_pipeline_local.py       # パースパイプライン（Parquet出力）
│   ├── modules/
│   │   ├── preparing/
│   │   │   ├── __init__.py
│   │   │   ├── _requests_utils.py          # requests関連ユーティリティ
│   │   │   ├── _prepare_chrome_driver.py   # Seleniumドライバ準備
│   │   │   └── _scrape_jra_odds.py         # JRAオッズ取得・パース専用
│   │   ├── parsers/
│   │   │   ├── __init__.py
│   │   │   ├── results_parser.py           # レース結果パーサ
│   │   │   ├── shutuba_parser.py           # 出馬表パーサ
│   │   │   ├── horse_info_parser.py        # 馬情報パーサ
│   │   │   └── pedigree_parser.py          # 血統パーサ (<< 追記)
│   │   ├── features/
│   │   │   ├── __init__.py
│   │   │   └── feature_engine.py           # 特徴量生成エンジン
│   │   ├── models/
│   │   │   ├── __init__.py
│   │   │   ├── model_train.py              # モデル学習
│   │   │   └── calibration.py              # 確率キャリブレーション
│   │   ├── sim/
│   │   │   ├── __init__.py
│   │   │   └── simulator.py                # モンテカルロシミュレーション
│   │   ├── optimizer/
│   │   │   ├── __init__.py
│   │   │   └── optimizer.py                # ポートフォリオ最適化
│   │   ├── executor/
│   │   │   ├── __init__.py
│   │   │   └── order_executor.py           # 発注実行（デフォルト無効）
│   │   └── monitoring/
│   │       ├── __init__.py
│   │       └── monitoring_local.py         # ローカルモニタリング
│   └── utils/
│       ├── __init__.py
│       ├── time_utils.py                    # 時刻処理ユーティリティ
│       └── data_utils.py                    # データ処理ユーティリティ
├── tests/
│   ├── __init__.py
│   ├── fixtures/                            # テスト用HTMLサンプル
│   │   ├── race_samples/
│   │   ├── shutuba_samples/
│   │   └── horse_samples/
│   ├── unit/
│   │   ├── test_parsers.py
│   │   ├── test_features.py
│   │   └── test_models.py
│   ├── integration/
│   │   └── test_pipeline_e2e.py
│   └── regression/
│       └── test_parser_regression.py
├── infra/
│   ├── docker/
│   │   ├── Dockerfile
│   │   └── docker-compose.yml
│   └── sched/
│       └── cron_examples/
│           ├── daily_scraping.sh
│           └── weekly_training.sh
├── configs/
│   ├── default.yaml                         # デフォルト設定
│   ├── scraping.yaml                        # スクレイピング設定
│   ├── features.yaml                        # 特徴量設定
│   ├── models.yaml                          # モデル設定
│   └── optimization.yaml                    # 最適化設定
├── notebooks/
│   ├── 01_data_exploration.ipynb
│   ├── 02_feature_analysis.ipynb
│   └── 03_model_evaluation.ipynb
├── data/                                    # ローカルデータストア
│   ├── raw/                                 # 生データ
│   │   ├── html/
│   │   │   ├── race/                        # レース結果HTML
│   │   │   ├── shutuba/                     # 出馬表HTML
│   │   │   ├── horse/                       # 馬プロフィールHTML
│   │   │   └── ped/                         # 血統HTML
│   │   └── json/
│   │       └── jra_odds/                    # JRAオッズJSON
│   ├── parsed/                              # パース済みデータ
│   │   └── parquet/
│   │       ├── races/
│   │       ├── shutuba/
│   │       ├── results/
│   │       ├── horses/
│   │       └── odds/
│   ├── features/                            # 特徴量データ
│   │   └── parquet/
│   ├── models/                              # 学習済みモデル
│   │   └── {model_id}/
│   ├── simulations/                         # シミュレーション結果
│   ├── orders/                              # 発注データ
│   ├── metrics/                             # メトリクスデータ
│   ├── logs/                                # ログファイル
│   │   └── {YYYY}/{MM}/{DD}/
│   ├── metadata/                            # メタデータ
│   │   └── db.sqlite3
│   ├── master/                              # マスターデータ
│   │   ├── horse_cache_log.csv
│   │   ├── track_master.csv
│   │   └── venue_master.csv
│   └── errors/                              # エラーログ
│       └── parse_failures/
├── docs/
│   ├── setup.md
│   ├── data_schema.md
│   ├── feature_spec.md
│   └── api_reference.md
├── scripts/
│   ├── setup_environment.sh
│   ├── backup_data.sh
│   └── check_data_integrity.py
├── requirements.txt
├── pyproject.toml
├── README.md
└── .gitignore


---

## 目次

1. [要約（システムの目的とコア方針）](#1-要約システムの目的とコア方針)
2. [システム全体像](#2-システム全体像)
3. [データ戦略](#3-データ戦略)
4. [データ取得パイプライン](#4-データ取得パイプライン)
5. [データ整形パイプライン](#5-データ整形パイプライン)
6. [特徴量エンジニアリング](#6-特徴量エンジニアリング)
7. [数理モデル](#7-数理モデル)
8. [シミュレーション](#8-シミュレーション)
9. [確率キャリブレーション](#9-確率キャリブレーション)
10. [日次ポートフォリオ最適化](#10-日次ポートフォリオ最適化)
11. [バックテスト設計](#11-バックテスト設計)
12. [モニタリングとダッシュボード](#12-モニタリングとダッシュボード)
13. [実務オペレーション](#13-実務オペレーション)
14. [再現性・CI/CD・インフラ](#14-再現性cicdインフラ)
15. [リスクと緩和策](#15-リスクと緩和策)
16. [テスト戦略](#16-テスト戦略)
17. [参考コードモジュール](#17-参考コードモジュール)
18. [導入ロードマップ](#18-導入ロードマップ)
19. [付録：データテーブル仕様](#19-付録データテーブル仕様)
20. [付録：用語集と算出式](#20-付録用語集と算出式)
21. [付記：次に行うべき実行タスク](#21-付記次に行うべき実行タスク)

---

## 1. 要約（システムの目的とコア方針）

### 1.1 目的

個人のローカル環境（必要に応じて GitHub Actions 無料枠）で、機械学習を用い各レースの買い目確率を推定し、確定オッズ（評価用）と締切直前の取得オッズ（取得可能な場合）を用いて期待値の高い買い目を抽出する。実際の自動発注は無償運用上のリスクと法的問題を避けるため初期段階では無効とし、発注仕様の生成・ペーパートレード記録・手動トランザクション支援を行う。

### 1.2 コア方針

#### 1.2.1 学習と評価の分離
- **学習時**: 確定オッズは使用しない（リーク防止）
- **評価時**: 確定オッズで評価する
- **実運用時**: 締切直前の取得オッズを使用（可能な範囲で）

#### 1.2.2 オッズ取得戦略
- **日次バッチ**: 定期的な基礎データ取得（深夜 03:00）
- **締切直前取得**: レース開始10-30分前に短時間集中取得（ローカルSeleniumで）
- **取得頻度**: 1レースあたり最大3回まで（BAN対策）

#### 1.2.3 確率推定とキャリブレーション
- 出力確率は温度スケーリング等でキャリブレーション
- ECE（Expected Calibration Error）< 0.05 を目標
- Brier Score のモニタリング（週次・月次）

#### 1.2.4 最適化と発注
- Fractional Kelly + 制約を使用
- 自動発注はデフォルトオフ
- `order_executor` は発注指示の作成とログ化まで
- 実送信は管理者が手動で行う

#### 1.2.5 スクレイピングポリシー
- `robots.txt` の確認と尊重（必須）
- リトライ: 指数バックオフ（最大5回）
- ランダム遅延: 2.5-10.0秒（サイトによって調整）
- User-Agent: ランダムローテーション
- IPローテーション: 有料プロキシは使わず、時間分散で代替

#### 1.2.6 データ保存
- 全 raw データはローカルに immutable 保存（bytes）
- メタデータは SQLite に保存
- パース済みデータは Parquet 形式

#### 1.2.7 計算負荷
- Monte Carlo は K=1,000 を基本
- 必要に応じて夜間バッチで K を増加（最大10,000、6時間以内）

---

## 2. システム全体像

### 2.1 4層アーキテクチャ


┌─────────────────────────────────────────────────────────────┐
│                    A. データ層（取得・保管）                   │
│  - ローカルFS: /data/keibaai/raw/html, /raw/json             │
│  - SQLite: /data/keibaai/metadata/db.sqlite3                │
│  - Parquet: /data/keibaai/parsed/parquet/                   │
└─────────────────────────────────────────────────────────────┘
                              ↓
┌─────────────────────────────────────────────────────────────┐
│              B. ETL / Feature 層（整形・特徴量）               │
│  - HTML Parser: BeautifulSoup4 + html.parser（互換性重視）   │
│  - Feature Engine: pandas + numpy                           │
│  - 出力: Parquet (partitioned by year/month/day)            │
└─────────────────────────────────────────────────────────────┘
                              ↓
┌─────────────────────────────────────────────────────────────┐
│               C. モデル層（学習・推論）                        │
│  - LightGBM (regressor + ranker)                            │
│  - scikit-learn (calibration, preprocessing)                │
│  - PyMC/NumPyro (階層ベイズ、オプション)                      │
└─────────────────────────────────────────────────────────────┘
                              ↓
┌─────────────────────────────────────────────────────────────┐
│            D. 投資/運用層（最適化・発注）                       │
│  - Simulator: Numba + numpy                                 │
│  - Optimizer: scipy.optimize                                │
│  - Executor: 発注案生成（自動送信無効）                        │
│  - Monitoring: SQLite + Streamlit                           │
└─────────────────────────────────────────────────────────────┘
                              ↓
┌─────────────────────────────────────────────────────────────┐
│                横断: Scheduler & CI/CD                        │
│  - cron / systemd timers / GitHub Actions                   │
│  - pytest (単体・統合・回帰テスト)                             │
└─────────────────────────────────────────────────────────────┘


### 2.2 データフロー


1. スクレイピング
   netkeiba/JRA → raw/html/*.bin, raw/json/*.json
                ↓
2. メタデータ保存
   → metadata/db.sqlite3 (fetch_log, data_versions)
                ↓
3. パース
   raw → parsed/parquet/* (races, shutuba, results, horses, odds)
                ↓
4. 特徴量生成
   parsed → features/parquet/* (horse_features, race_features)
                ↓
5. モデル学習・推論
   features → models/{model_id}/model.pkl
           → predictions/parquet/* (mu, sigma, nu)
                ↓
6. シミュレーション
   predictions → simulations/{sim_id}.json (K回試行)
                ↓
7. 最適化
   simulations → orders/*.json (allocation, status: pending_manual)
                ↓
8. 手動発注 & 結果収集
   orders → results (実際のpayoff)
                ↓
9. 評価・モニタリング
   results → metrics/daily_metrics.parquet
          → Streamlit Dashboard


---

## 3. データ戦略

### 3.0 データ識別子の形式

#### 3.0.1 レースID形式

**フォーマット**: `YYYYPPNNDDRR`

各フィールドの意味:
- `YYYY`: 開催年度 (4桁) - 例: 2023
- `PP`: 競馬場コード (2桁)
  - 05 = 東京
  - 06 = 中山
  - 08 = 京都
  - 09 = 阪神
  - 01 = 札幌
  - 02 = 函館
  - 03 = 福島
  - 04 = 新潟
  - 07 = 小倉
- `NN`: 開催回数 (2桁) - 例: 04 = 第4回開催
- `DD`: 開催日目 (2桁) - 例: 03 = 3日目
- `RR`: レース番号 (2桁) - 例: 01 = 1R, 12 = 12R

**具体例**:
```
レースID: 202305040301
分解:
  2023: 2023年
    05: 東京競馬場
    04: 第4回開催
    03: 3日目
    01: 1レース目

対応URL: https://db.netkeiba.com/race/202305040301
```

**注意事項**:
- レースIDは12桁固定
- 日付ではなく「開催回数・日目」で構成される
- 同じ年でも複数回開催があるため、日付からの逆算は不可能
- `venue`, `round_of_year`, `day_of_meeting` とレースIDの整合性を常に確認すること

#### 3.0.2 馬ID形式

**フォーマット**: `YYYYNNNNNN`

- `YYYY`: 生年 (4桁) - 例: 2021
- `NNNNNN`: 登録番号 (6桁) - 例: 102797

**具体例**:
```
馬ID: 2021102797
分解:
  2021: 2021年生まれ
  102797: 登録番号

対応URL: https://db.netkeiba.com/horse/2021102797/
```

---

### 3.1 基本方針

#### 3.1.1 不変性（Immutability）
- 全 raw データ（HTML/JSON）はローカルに原本のまま保存（binary）
- ファイル名に `data_version`（ISO時刻 + SHA256ハッシュ）を含む
- 例: `raw_html_race_20230601_20251106T120000+09:00_sha256abcd1234.bin`

#### 3.1.2 メタデータ管理
- SQLite に以下を保存:
  - `url`: 取得元URL
  - `fetched_ts`: 取得タイムスタンプ（ISO8601+09:00）
  - `sha256`: ファイルのハッシュ値
  - `file_size`: バイトサイズ
  - `fetch_method`: 'requests' or 'selenium'
  - `http_status`: HTTPステータスコード
  - `error_message`: エラー時のメッセージ

#### 3.1.3 バージョニング
- パース済み中間データは Parquet（列指向）で保存
- 各テーブルに `data_version` と `schema_version` を含む
- 学習実行時は `model_metadata` テーブルに保存:
  - `commit_hash`: Gitコミットハッシュ
  - `hyperparams`: JSONシリアライズされたハイパーパラメータ
  - `training_date`: 学習実行日時
  - `random_seed`: 再現性のためのシード値
  - `data_version`: 使用したデータのバージョン

### 3.2 ストレージ構成


keibaai/data/
├── raw/
│   ├── html/
│   │   ├── race/
│   │   │   └── {race_id}.bin
│   │   ├── shutuba/
│   │   │   └── {race_id}.bin
│   │   ├── horse/
│   │   │   ├── {horse_id}_profile.bin
│   │   │   └── {horse_id}_perf.bin
│   │   └── ped/
│   │       └── {horse_id}.bin
│   └── json/
│       └── jra_odds/
│           └── {race_id}_{snapshot_ts}.json
├── parsed/
│   └── parquet/
│       ├── races/
│       │   └── year=YYYY/month=MM/day=DD/*.parquet
│       ├── shutuba/
│       │   └── year=YYYY/month=MM/day=DD/*.parquet
│       ├── results/
│       │   └── year=YYYY/month=MM/day=DD/*.parquet
│       ├── horses/
│       │   └── year=YYYY/month=MM/*.parquet
│       ├── peds/
│       │   └── *.parquet
│       └── odds/
│           └── year=YYYY/month=MM/day=DD/*.parquet
├── features/
│   └── parquet/
│       ├── horse_features/
│       │   └── year=YYYY/month=MM/*.parquet
│       └── race_features/
│           └── year=YYYY/month=MM/day=DD/*.parquet
├── models/
│   └── {model_id}/
│       ├── model.pkl
│       ├── metadata.json
│       ├── calibration.pkl
│       └── feature_importance.csv
├── predictions/
│   └── parquet/
│       └── year=YYYY/month=MM/day=DD/*.parquet
├── simulations/
│   └── {sim_id}.json
├── orders/
│   └── {order_id}.json
├── metrics/
│   ├── daily_metrics.parquet
│   ├── weekly_metrics.parquet
│   └── monthly_metrics.parquet
├── logs/
│   └── {YYYY}/{MM}/{DD}/
│       ├── scraping.log
│       ├── parsing.log
│       ├── training.log
│       └── optimization.log
├── metadata/
│   └── db.sqlite3
├── master/
│   ├── horse_cache_log.csv
│   ├── track_master.csv
│   ├── venue_master.csv
│   └── delta_map.yaml
└── errors/
    └── parse_failures/
        └── {parser_name}/{race_id}.json


### 3.3 運用ルール

#### 3.3.1 Atomic Write
全ての書き込みは atomic write で行う:


import os
import tempfile

def atomic_write(path: str, data: bytes):
    """
    一時ファイルに書き込み、完了後にリネームすることで
    書き込み中のファイル破損を防ぐ
    """
    dir_path = os.path.dirname(path)
    os.makedirs(dir_path, exist_ok=True)
    
    # 一時ファイル作成
    fd, tmp_path = tempfile.mkstemp(
        dir=dir_path,
        prefix='.tmp_',
        suffix=os.path.basename(path)
    )
    
    try:
        # データ書き込み
        with os.fdopen(fd, 'wb') as f:
            f.write(data)
            f.flush()
            os.fsync(f.fileno())
        
        # アトミックリネーム
        os.replace(tmp_path, path)
        
    except Exception as e:
        # エラー時は一時ファイルを削除
        if os.path.exists(tmp_path):
            os.remove(tmp_path)
        raise e


#### 3.3.2 メタデータ保存
取得後すぐに stub metadata を SQLite に保存:


import hashlib
from datetime import datetime, timezone, timedelta

def save_fetch_metadata(
db_conn,
url: str,
file_path: str,
data: bytes,
http_status: int,
fetch_method: str,
error_message: str = None
):
"""
データ取得のメタデータをSQLiteに保存
"""
sha256 = hashlib.sha256(data).hexdigest()

# [修正] UTC -> Asia/Tokyo (+09:00)
jst = timezone(timedelta(hours=9))
fetched_ts = datetime.now(jst).isoformat()

file_size = len(data)

cursor = db_conn.cursor()
cursor.execute('''
INSERT OR REPLACE INTO fetch_log (
url, file_path, fetched_ts, sha256,
file_size, fetch_method, http_status, error_message
) VALUES (?, ?, ?, ?, ?, ?, ?, ?)
''', (
url, file_path, fetched_ts, sha256,
file_size, fetch_method, http_status, error_message
))
db_conn.commit()


#### 3.3.3 データバージョニング
data_version を用いたバージョン管理:


from datetime import datetime, timezone
import hashlib

def generate_data_version(data: bytes) -> str:
    """
    データバージョン文字列を生成
    形式: YYYYMMDDTHHMMSS+09:00_sha256={hash[:8]}
    """
    timestamp = datetime.now(timezone.utc).astimezone(
        timezone(timedelta(hours=9))
    ).strftime('%Y%m%dT%H%M%S+09:00')
    
    sha256_short = hashlib.sha256(data).hexdigest()[:8]
    
    return f"{timestamp}_sha256={sha256_short}"

def construct_filename(
    base_name: str,
    identifier: str,
    data: bytes,
    extension: str = 'bin'
) -> str:
    """
    バージョン付きファイル名を構築
    例: race_202306010101_20251106T120000+09:00_sha256=abcd1234.bin
    """
    data_version = generate_data_version(data)
    return f"{base_name}_{identifier}_{data_version}.{extension}"


#### 3.3.4 差分取り込み
SHA256 比較による差分判定:


def should_fetch(db_conn, url: str, current_sha256: str = None) -> bool:
    """
    既存データと比較して取得の要否を判定
    
    Args:
        db_conn: SQLite接続
        url: 取得対象URL
        current_sha256: 既存データのSHA256（オプション）
    
    Returns:
        True: 取得が必要
        False: スキップ可能
    """
    cursor = db_conn.cursor()
    cursor.execute('''
        SELECT sha256, fetched_ts 
        FROM fetch_log 
        WHERE url = ? 
        ORDER BY fetched_ts DESC 
        LIMIT 1
    ''', (url,))
    
    result = cursor.fetchone()
    
    # 初回取得
    if result is None:
        return True
    
    last_sha256, last_fetched = result
    
    # SHA256が異なる場合は取得
    if current_sha256 and current_sha256 != last_sha256:
        return True
    
    # TTLチェック（例: 7日以上古い場合は再取得）
    from datetime import datetime, timedelta
    last_dt = datetime.fromisoformat(last_fetched)
    if datetime.now(timezone.utc) - last_dt > timedelta(days=7):
        return True
    
    return False


#### 3.3.5 バックアップ戦略
定期的な外付けHDDへのバックアップ:

bash
#!/bin/bash
# scripts/backup_data.sh

BACKUP_DATE=$(date +%Y%m%d)
SOURCE_DIR="/data/keibaai"
BACKUP_DIR="/mnt/external_hdd/keibaai_backup_${BACKUP_DATE}"

# バックアップディレクトリ作成
mkdir -p "${BACKUP_DIR}"

# rsyncで差分バックアップ
rsync -avz --progress \
    --exclude='*.tmp' \
    --exclude='logs/' \
    "${SOURCE_DIR}/" \
    "${BACKUP_DIR}/"

# 古いバックアップの削除（30日以上前）
find /mnt/external_hdd/ -name "keibaai_backup_*" -mtime +30 -exec rm -rf {} \;

# バックアップ完了ログ
echo "[${BACKUP_DATE}] Backup completed successfully" >> /data/keibaai/logs/backup.log


### 3.4 SQLite スキーマ定義

#### 3.4.1 fetch_log テーブル
sql
CREATE TABLE IF NOT EXISTS fetch_log (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    url TEXT NOT NULL,
    file_path TEXT NOT NULL,
    fetched_ts TEXT NOT NULL,  -- ISO8601+09:00
    sha256 TEXT NOT NULL,
    file_size INTEGER NOT NULL,
    fetch_method TEXT NOT NULL,  -- 'requests' or 'selenium'
    http_status INTEGER,
    error_message TEXT,
    UNIQUE(url, fetched_ts)
);

CREATE INDEX idx_fetch_log_url ON fetch_log(url);
CREATE INDEX idx_fetch_log_sha256 ON fetch_log(sha256);
CREATE INDEX idx_fetch_log_fetched_ts ON fetch_log(fetched_ts);


#### 3.4.2 model_metadata テーブル
sql
CREATE TABLE IF NOT EXISTS model_metadata (
    model_id TEXT PRIMARY KEY,
    model_type TEXT NOT NULL,  -- 'mu_regressor', 'mu_ranker', 'sigma', 'nu'
    commit_hash TEXT NOT NULL,
    training_start TEXT NOT NULL,  -- ISO8601+09:00
    training_end TEXT NOT NULL,    -- ISO8601+09:00
    hyperparams TEXT NOT NULL,     -- JSON
    calibration_method TEXT,
    data_version TEXT NOT NULL,
    random_seed INTEGER NOT NULL,
    library_versions TEXT NOT NULL,  -- JSON
    performance_metrics TEXT,        -- JSON
    created_ts TEXT NOT NULL,        -- ISO8601+09:00
    notes TEXT
);

CREATE INDEX idx_model_metadata_created_ts ON model_metadata(created_ts);
CREATE INDEX idx_model_metadata_model_type ON model_metadata(model_type);


#### 3.4.3 data_versions テーブル
sql
CREATE TABLE IF NOT EXISTS data_versions (
    version_id TEXT PRIMARY KEY,
    table_name TEXT NOT NULL,
    schema_version TEXT NOT NULL,
    record_count INTEGER NOT NULL,
    start_date TEXT NOT NULL,  -- ISO8601+09:00
    end_date TEXT NOT NULL,    -- ISO8601+09:00
    file_paths TEXT NOT NULL,  -- JSON array
    created_ts TEXT NOT NULL,  -- ISO8601+09:00
    sha256_manifest TEXT NOT NULL,  -- JSON: {file_path: sha256}
    notes TEXT
);

CREATE INDEX idx_data_versions_table_name ON data_versions(table_name);
CREATE INDEX idx_data_versions_created_ts ON data_versions(created_ts);


#### 3.4.4 parse_failures テーブル
sql
CREATE TABLE IF NOT EXISTS parse_failures (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    parser_name TEXT NOT NULL,
    source_file TEXT NOT NULL,
    race_id TEXT,
    horse_id TEXT,
    error_type TEXT NOT NULL,
    error_message TEXT
    stack_trace TEXT,
    failed_ts TEXT NOT NULL,  -- ISO8601+09:00
    retry_count INTEGER DEFAULT 0,
    resolved BOOLEAN DEFAULT 0,
    resolved_ts TEXT,
    notes TEXT
);

CREATE INDEX idx_parse_failures_parser_name ON parse_failures(parser_name);
CREATE INDEX idx_parse_failures_race_id ON parse_failures(race_id);
CREATE INDEX idx_parse_failures_resolved ON parse_failures(resolved);
CREATE INDEX idx_parse_failures_failed_ts ON parse_failures(failed_ts);



# 4. データ取得パイプライン (HTML基礎データ)

## 4.1 目的と位置づけ

本パイプラインの目的は、レース予測モデルの学習と推論に必要となる**基礎的なHTMLデータ**を `netkeiba.com` から取得し、ローカルに保存することです。

ここで取得するデータは、レース結果、出馬表、馬のプロフィール、過去成績、血統情報などであり、オッズ予測や期待値計算を行うための前段階のデータ収集プロセスと位置づけられます。オッズ情報の取得は、本パイプラインとは別の専用プロセスとして扱います。

## 4.2 機能概要

本パイプラインは、以下の機能を提供します。

1.  **データ保存場所の統一**: 全てのデータは `keibaai/data/` ディレクトリ以下に集約されます。
2.  **キャッシュ戦略**: データ特性に応じたキャッシュ戦略を実装し、効率的な差分取得を行います。
3.  **出馬表取得の最適化**: レース当日はキャッシュを無視して最新の情報を取得します。
4.  **ローカル処理の高速化**: ファイルベースの処理（馬ID抽出）を並列化し、処理時間を短縮します。

---

## 4.3 ステップ・バイ・ステップによるHTML収集詳解

処理は `_scrape_html.py` 内の関数群によって、以下の順序で実行されます。

### ステップ1: 開催日の取得
- **関数:** `scrape_kaisai_date`
- **処理:** 指定された期間内のレース開催日をカレンダーページから取得します。

### ステップ2: レースIDの取得
- **関数:** `scrape_race_id_list`
- **処理:** 開催日ごとにレース一覧ページにアクセスし、その日に開催される全てのレースIDを取得します。

### ステップ3: レース関連HTMLの保存

#### 3-a. レース結果HTML
- **関数:** `scrape_html_race`
- **保存先:** `keibaai/data/raw/html/race/`
- **処理:** ファイルが存在しない場合のみ、レース結果ページをダウンロードし、`.bin`形式で保存します。

#### 3-b. 出馬表HTML
- **関数:** `scrape_html_shutuba`
- **保存先:** `keibaai/data/raw/html/shutuba/`
- **処理:** `force_today_refresh` 引数（デフォルト: True）に基づき、レース開催日が実行日当日の場合は、キャッシュを無視して常に最新の出馬表を再取得します。それ以外の場合は、ファイルが存在しなければ取得します。

### ステップ4: 馬IDの集約
- **関数:** `extract_horse_ids_from_html`
- **対象:** `keibaai/data/raw/html/race/*.bin`
- **処理:** 保存済みの全レース結果HTMLを解析し、出走したすべての馬のIDを抽出します。この処理は、`multiprocessing` を利用して並列化されており、CPUコアを最大限に活用して高速に実行されます。

### ステップ5: 馬関連HTMLの保存

- **関数:** `scrape_html_horse`, `scrape_html_ped`
- **キャッシュ戦略:** 馬データは、その特性に応じて以下の通りインテリジェントな差分取得を行います。
  - **対象ログ:** `keibaai/data/master/horse_cache_log.csv`

#### 5-a. プロフィール (`_profile.bin`)
- **保存先:** `keibaai/data/raw/html/horse/`
- **鮮度:** 不変データ。ファイルが存在しない場合のみ取得します。

#### 5-b. 過去成績 (`_perf.bin`)
- **保存先:** `keibaai/data/raw/html/horse/`
- **鮮度:** 可変データ。キャッシュログを参照し、最終取得日から**7日以上経過**した場合、または新規の馬の場合のみ再取得します。取得に成功するとログの時刻が更新されます。

#### 5-c. 血統 (`.bin`)
- **保存先:** `keibaai/data/raw/html/ped/`
- **鮮度:** 不変データ。ファイルが存在しない場合のみ取得します。


## 5. データ整形パイプライン

### 5.1 パーサ設計の基本原則

#### 5.1.1 3層パーサアーキテクチャ


Layer 1: bytes → text
  - エンコーディング処理（EUC-JP, UTF-8等）
  - バイナリデータの読み込み

Layer 2: text → BeautifulSoup
  - HTML/XMLパース
  - DOM構造の解析

Layer 3: BeautifulSoup → raw_dict → typed_row
  - データ抽出
  - 型変換・バリデーション
  - クレンジング


#### 5.1.2 エラーハンドリング戦略


def parse_with_error_handling(
    file_path: str,
    parser_name: str,
    parse_func,
    db_conn
):
    """
    エラーハンドリング付きパーサ実行
    
    Args:
        file_path: 対象ファイルパス
        parser_name: パーサ名
        parse_func: パース関数
        db_conn: SQLite接続
    
    Returns:
        パース結果（成功時）またはNone（失敗時）
    """
    import traceback
    
    try:
        result = parse_func(file_path)
        return result
    
    except Exception as e:
        # エラーログ記録
        error_message = str(e)
        stack_trace = traceback.format_exc()
        
        logging.error(f"パースエラー ({parser_name}): {file_path} - {error_message}")
        
        # データベースに記録
        cursor = db_conn.cursor()
        cursor.execute('''
            INSERT INTO parse_failures (
                parser_name, source_file, error_type, 
                error_message, stack_trace, failed_ts
            ) VALUES (?, ?, ?, ?, ?, ?)
        ''', (
            parser_name,
            file_path,
            type(e).__name__,
            error_message,
            stack_trace,
            datetime.now(timezone.utc).isoformat()
        ))
        db_conn.commit()
        
        # エラー詳細をJSONに保存
        error_dir = Path(f'data/errors/parse_failures/{parser_name}')
        error_dir.mkdir(parents=True, exist_ok=True)
        
        error_file = error_dir / f"{Path(file_path).stem}_error.json"
        
        error_data = {
            'file_path': file_path,
            'parser_name': parser_name,
            'error_type': type(e).__name__,
            'error_message': error_message,
            'stack_trace': stack_trace,
            'failed_ts': datetime.now(timezone.utc).isoformat()
        }
        
        with open(error_file, 'w', encoding='utf-8') as f:
            json.dump(error_data, f, ensure_ascii=False, indent=2)
        
        return None


### 5.2 レース結果パーサ（parsers/results_parser.py）


"""
レース結果HTMLパーサ
netkeiba.com のレース結果ページから情報を抽出
"""

import re
import logging
from typing import Dict, List, Optional
from datetime import datetime, timezone, timedelta
from pathlib import Path

import pandas as pd
from bs4 import BeautifulSoup


def parse_results_html(file_path: str, race_id: str = None) -> pd.DataFrame:
    """
    レース結果HTMLをパースしてDataFrameを返す
    
    Args:
        file_path: HTMLファイルパス
        race_id: レースID（ファイル名から自動抽出も可能）
    
    Returns:
        レース結果DataFrame
        
    Columns:
        - race_id: str (レースID)
        - finish_position: int (着順)
        - bracket_number: int (枠番)
        - horse_number: int (馬番)
        - horse_id: str (馬ID)
        - horse_name: str (馬名)
        - sex_age: str (性齢、例: "牡3")
        - sex: str (性別、"牡", "牝", "セ")
        - age: int (年齢)
        - basis_weight: float (斤量、kg)
        - jockey_id: str (騎手ID)
        - jockey_name: str (騎手名)
        - finish_time_str: str (タイム文字列、例: "1:59.8")
        - finish_time_seconds: float (タイム秒数)
        - margin_str: str (着差文字列、例: "1.1/2")
        - margin_seconds: float (着差秒数)
        - passing_order: str (通過順、例: "03-03-02-01")
        - last_3f_time: float (上がり3F、秒)
        - win_odds: float (単勝オッズ)
        - popularity: int (人気)
        - horse_weight: int (馬体重、kg)
        - horse_weight_change: int (馬体重増減、kg)
        - trainer_id: str (調教師ID)
        - trainer_name: str (調教師名)
        - owner_name: str (馬主名)
        - prize_money: int (賞金、万円)
    """
    logging.info(f"レース結果パース開始: {file_path}")
    
    # レースID抽出
    if race_id is None:
        race_id = extract_race_id_from_filename(file_path)
    
    # ファイル読み込み
    with open(file_path, 'rb') as f:
        html_bytes = f.read()
    
    # EUC-JPでデコード
    try:
        html_text = html_bytes.decode('euc_jp', errors='replace')
    except:
        html_text = html_bytes.decode('utf-8', errors='replace')

    # ※ 検証済み（2025-11-16）: 'html.parser'で欠損率0%達成
    soup = BeautifulSoup(html_text, 'html.parser')  # 推奨（互換性が高い）
    # soup = BeautifulSoup(html_text, 'lxml')  # 非推奨（互換性の問題あり）

    # レース結果テーブルを検索
    result_table = soup.find('table', attrs={'summary': 'レース結果'})
    
    if not result_table:
        logging.error(f"レース結果テーブルが見つかりません: {file_path}")
        return pd.DataFrame()
    
    rows = []
    
    # tbody内のtrを走査
    tbody = result_table.find('tbody')
    if not tbody:
        logging.error(f"tbody が見つかりません: {file_path}")
        return pd.DataFrame()
    
    for tr in tbody.find_all('tr'):
        try:
            row_data = parse_result_row(tr, race_id)
            if row_data:
                rows.append(row_data)
        except Exception as e:
            logging.warning(f"行のパースエラー: {e}")
            continue
    
    df = pd.DataFrame(rows)
    
    logging.info(f"レース結果パース完了: {file_path} ({len(df)}行)")
    
    return df


def parse_result_row(tr, race_id: str) -> Optional[Dict]:
    """
    レース結果テーブルの1行をパース
    
    Args:
        tr: BeautifulSoupのtr要素
        race_id: レースID
    
    Returns:
        行データ辞書
    """
    cells = tr.find_all('td')
    
    if len(cells) < 15:
        return None
    
    row_data = {'race_id': race_id}
    
    # 着順
    finish_text = cells[0].get_text(strip=True)
    row_data['finish_position'] = parse_int_or_none(finish_text)
    
    # 枠番
    bracket_text = cells[1].get_text(strip=True)
    row_data['bracket_number'] = parse_int_or_none(bracket_text)
    
    # 馬番
    horse_num_text = cells[2].get_text(strip=True)
    row_data['horse_number'] = parse_int_or_none(horse_num_text)
    
    # 馬名・馬ID
    horse_link = cells[3].find('a', href=re.compile(r'/horse/\d+'))
    if horse_link:
        row_data['horse_name'] = horse_link.get_text(strip=True)
        horse_id_match = re.search(r'/horse/(\d+)', horse_link['href'])
        row_data['horse_id'] = horse_id_match.group(1) if horse_id_match else None
    else:
        row_data['horse_name'] = cells[3].get_text(strip=True)
        row_data['horse_id'] = None
    
    # 性齢（例: "牡3"）
    sex_age_text = cells[4].get_text(strip=True)
    row_data['sex_age'] = sex_age_text
    sex, age = parse_sex_age(sex_age_text)
    row_data['sex'] = sex
    row_data['age'] = age
    
    # 斤量
    weight_text = cells[5].get_text(strip=True)
    row_data['basis_weight'] = parse_float_or_none(weight_text)
    
    # 騎手名・騎手ID
    jockey_link = cells[6].find('a', href=re.compile(r'/jockey/\d+'))
    if jockey_link:
        row_data['jockey_name'] = jockey_link.get_text(strip=True)
        jockey_id_match = re.search(r'/jockey/(\d+)', jockey_link['href'])
        row_data['jockey_id'] = jockey_id_match.group(1) if jockey_id_match else None
    else:
        row_data['jockey_name'] = cells[6].get_text(strip=True)
        row_data['jockey_id'] = None
    
    # タイム
    time_text = cells[7].get_text(strip=True)
    row_data['finish_time_str'] = time_text
    row_data['finish_time_seconds'] = parse_time_to_seconds(time_text)
    
    # 着差
    margin_text = cells[8].get_text(strip=True)
    row_data['margin_str'] = margin_text
    row_data['margin_seconds'] = parse_margin_to_seconds(margin_text)
    
    # 通過順
    passing_text = cells[10].get_text(strip=True)
    row_data['passing_order'] = passing_text
    
    # 上がり3F
    last_3f_text = cells[11].get_text(strip=True)
    row_data['last_3f_time'] = parse_float_or_none(last_3f_text)
    
    # 単勝オッズ
    odds_text = cells[12].get_text(strip=True)
    row_data['win_odds'] = parse_float_or_none(odds_text)
    
    # 人気
    popularity_text = cells[13].get_text(strip=True)
    row_data['popularity'] = parse_int_or_none(popularity_text)
    
    # 馬体重
    weight_change_text = cells[14].get_text(strip=True)
    horse_weight, weight_change = parse_horse_weight(weight_change_text)
    row_data['horse_weight'] = horse_weight
    row_data['horse_weight_change'] = weight_change
    
    # 調教師（15列目、存在する場合）
    if len(cells) > 15:
        trainer_link = cells[15].find('a', href=re.compile(r'/trainer/\d+'))
        if trainer_link:
            row_data['trainer_name'] = trainer_link.get_text(strip=True)
            trainer_id_match = re.search(r'/trainer/(\d+)', trainer_link['href'])
            row_data['trainer_id'] = trainer_id_match.group(1) if trainer_id_match else None
        else:
            row_data['trainer_name'] = cells[15].get_text(strip=True)
            row_data['trainer_id'] = None
    
    # 馬主（16列目、存在する場合）
    if len(cells) > 16:
        row_data['owner_name'] = cells[16].get_text(strip=True)
    
    # 賞金（17列目、存在する場合）
    if len(cells) > 17:
        prize_text = cells[17].get_text(strip=True)
        row_data['prize_money'] = parse_prize_money(prize_text)
    
    return row_data


# ========================================
# ユーティリティ関数
# ========================================

def extract_race_id_from_filename(file_path: str) -> str:
    """
    ファイル名からレースIDを抽出
    例: 202306010101_20251106T120000+09:00_sha256=abcd1234.bin → 202306010101
    """
    filename = Path(file_path).stem
    match = re.search(r'(\d{12})', filename)
    return match.group(1) if match else None


def parse_int_or_none(text: str) -> Optional[int]:
    """
    文字列をintに変換、失敗時はNone
    """
    if not text or text == '---' or text == '':
        return None
    
    # 数字のみ抽出
    digits = re.sub(r'[^\d]', '', text)
    
    try:
        return int(digits)
    except:
        return None


def parse_float_or_none(text: str) -> Optional[float]:
    """
    文字列をfloatに変換、失敗時はNone
    """
    if not text or text == '---' or text == '':
        return None
    
    # 数字とドットのみ抽出
    cleaned = re.sub(r'[^\d.]', '', text)
    
    try:
        return float(cleaned)
    except:
        return None


def parse_sex_age(sex_age_text: str) -> tuple:
    """
    性齢文字列をパース
    例: "牡3" → ("牡", 3)
    """
    if not sex_age_text:
        return (None, None)
    
    # 性別（1文字目）
    sex = sex_age_text[0] if len(sex_age_text) > 0 else None
    
    # 年齢（残り）
    age_str = sex_age_text[1:]
    age = parse_int_or_none(age_str)
    
    return (sex, age)


def parse_time_to_seconds(time_str: str) -> Optional[float]:
    """
    タイム文字列を秒数に変換
    例: "1:59.8" → 119.8
    """
    if not time_str or time_str == '---':
        return None
    
    # 分:秒.小数 の形式
    match = re.match(r'(\d+):(\d+)\.(\d+)', time_str)
    if match:
        minutes = int(match.group(1))
        seconds = int(match.group(2))
        decimal = int(match.group(3))
        
        total_seconds = minutes * 60 + seconds + decimal / 10.0
        return total_seconds
    
    # 秒.小数 の形式
    match = re.match(r'(\d+)\.(\d+)', time_str)
    if match:
        seconds = int(match.group(1))
        decimal = int(match.group(2))
        return seconds + decimal / 10.0
    
    return None


def parse_margin_to_seconds(margin_str: str) -> Optional[float]:
    """
    着差文字列を秒数に変換
    
    例:
      - "1.1/2" → 1.5秒
      - "アタマ" → 0.1秒
      - "ハナ" → 0.05秒
      - "クビ" → 0.2秒
      - "3/4" → 0.75秒
    
    Args:
        margin_str: 着差文字列
    
    Returns:
        着差秒数
    """
    if not margin_str or margin_str in ['---', '']:
        return None
    
    # 特殊表記
    special_margins = {
        'アタマ': 0.1,
        'ハナ': 0.05,
        'クビ': 0.2,
        '同着': 0.0,
        'ハッキリ': 0.0
    }
    
    if margin_str in special_margins:
        return special_margins[margin_str]
    
    # 分数表記（例: "1.1/2", "3/4"）
    if '/' in margin_str:
        parts = margin_str.split('/')
        
        if len(parts) == 2:
            try:
                # 整数部分と分数部分を分離
                if '.' in parts[0]:
                    integer_part_str, numerator_str = parts[0].split('.')
                    integer_part = int(integer_part_str)
                    numerator = int(numerator_str)
                else:
                    integer_part = 0
                    numerator = int(parts[0])
                
                denominator = int(parts[1])
                
                fraction_value = numerator / denominator
                total = integer_part + fraction_value
                
                return total
            except:
                pass
    
    # 通常の数値
    try:
        return float(margin_str)
    except:
        return None


def parse_horse_weight(weight_text: str) -> tuple:
    """
    馬体重文字列をパース
    例: "478(+2)" → (478, 2)
    例: "450(-5)" → (450, -5)
    
    Returns:
        (馬体重, 増減)
    """
    if not weight_text or weight_text == '---':
        return (None, None)
    
    # パターン: 数字(+/-数字)
    match = re.match(r'(\d+)\(([+-]?\d+)\)', weight_text)
    if match:
        weight = int(match.group(1))
        change = int(match.group(2))
        return (weight, change)
    
    # 数字のみ
    match = re.match(r'(\d+)', weight_text)
    if match:
        weight = int(match.group(1))
        return (weight, None)
    
    return (None, None)


def parse_prize_money(prize_text: str) -> Optional[int]:
    """
    賞金文字列をパース（万円単位）
    例: "1,000" → 1000
    """
    if not prize_text or prize_text == '---':
        return None
    
    # カンマ除去
    cleaned = prize_text.replace(',', '')
    
    try:
        return int(cleaned)
    except:
        return None


### 5.3 出馬表パーサ（parsers/shutuba_parser.py）


"""
出馬表HTMLパーサ
netkeiba.com の出馬表ページから情報を抽出
"""

import re
import logging
from typing import Dict, List, Optional
from pathlib import Path

import pandas as pd
from bs4 import BeautifulSoup


def parse_shutuba_html(file_path: str, race_id: str = None) -> pd.DataFrame:
    """
    出馬表HTMLをパースしてDataFrameを返す
    
    Args:
        file_path: HTMLファイルパス
        race_id: レースID
    
    Returns:
        出馬表DataFrame
        
    Columns:
        - race_id: str
        - horse_number: int (馬番)
        - bracket_number: int (枠番)
        - horse_id: str
        - horse_name: str
        - sex_age: str (例: "牡3")
        - sex: str
        - age: int
        - basis_weight: float (斤量、kg)
        - jockey_id: str
        - jockey_name: str
        - trainer_id: str
        - trainer_name: str
        - horse_weight: int (前走馬体重)
        - owner_name: str
        - prize_total: int (獲得賞金、万円)
        - morning_odds: float (前日オッズ)
        - morning_popularity: int (前日人気)
        - career_stats: str (例: "5戦2勝")
        - career_starts: int
        - career_wins: int
        - career_places: int (2着+3着)
        - last_5_finishes: str (例: "12311")
    """
    logging.info(f"出馬表パース開始: {file_path}")
    
    if race_id is None:
        race_id = extract_race_id_from_filename(file_path)
    
    # ファイル読み込み
    with open(file_path, 'rb') as f:
        html_bytes = f.read()
    
    try:
        html_text = html_bytes.decode('euc_jp', errors='replace')
    except:
        html_text = html_bytes.decode('utf-8', errors='replace')

    # ※ 検証済み（2025-11-16）: 'html.parser'で欠損率0%達成
    soup = BeautifulSoup(html_text, 'html.parser')  # 推奨（互換性が高い）
    # soup = BeautifulSoup(html_text, 'lxml')  # 非推奨（互換性の問題あり）

    # Shutuba_Table を検索
    shutuba_table = soup.find('table', class_='Shutuba_Table')
    
    if not shutuba_table:
        logging.error(f"Shutuba_Table が見つかりません: {file_path}")
        return pd.DataFrame()
    
    rows = []
    
    tbody = shutuba_table.find('tbody')
    if not tbody:
        logging.error(f"tbody が見つかりません: {file_path}")
        return pd.DataFrame()
    
    for tr in tbody.find_all('tr'):
        try:
            row_data = parse_shutuba_row(tr, race_id)
            if row_data:
                rows.append(row_data)
        except Exception as e:
            logging.warning(f"行のパースエラー: {e}")
            continue
    
    df = pd.DataFrame(rows)
    
    logging.info(f"出馬表パース完了: {file_path} ({len(df)}行)")
    
    return df


def parse_shutuba_row(tr, race_id: str) -> Optional[Dict]:
    """
    出馬表テーブルの1行をパース
    
    Args:
        tr: BeautifulSoupのtr要素
        race_id: レースID
    
    Returns:
        行データ辞書
    """
    cells = tr.find_all('td')
    
    if len(cells) < 10:
        return None
    
    row_data = {'race_id': race_id}
    
    # 枠番
    bracket_text = cells[0].get_text(strip=True)
    row_data['bracket_number'] = parse_int_or_none(bracket_text)
    
    # 馬番
    horse_num_text = cells[1].get_text(strip=True)
    row_data['horse_number'] = parse_int_or_none(horse_num_text)
    
    # 馬名・馬ID
    horse_link = cells[3].find('a', href=re.compile(r'/horse/\d+'))
    if horse_link:
        row_data['horse_name'] = horse_link.get_text(strip=True)
        horse_id_match = re.search(r'/horse/(\d+)', horse_link['href'])
        row_data['horse_id'] = horse_id_match.group(1) if horse_id_match else None
    else:
        row_data['horse_name'] = cells[3].get_text(strip=True)
        row_data['horse_id'] = None
    
    # 性齢
    sex_age_text = cells[4].get_text(strip=True)
    row_data['sex_age'] = sex_age_text
    sex, age = parse_sex_age(sex_age_text)
    row_data['sex'] = sex
    row_data['age'] = age
    
    # 斤量
    weight_text = cells[5].get_text(strip=True)
    row_data['basis_weight'] = parse_float_or_none(weight_text)
    
    # 騎手名・騎手ID
    jockey_link = cells[6].find('a', href=re.compile(r'/jockey/\d+'))
    if jockey_link:
        row_data['jockey_name'] = jockey_link.get_text(strip=True)
        jockey_id_match = re.search(r'/jockey/(\d+)', jockey_link['href'])
        row_data['jockey_id'] = jockey_id_match.group(1) if jockey_id_match else None
    else:
        row_data['jockey_name'] = cells[6].get_text(strip=True)
        row_data['jockey_id'] = None
    
    # 調教師名・調教師ID
    trainer_link = cells[7].find('a', href=re.compile(r'/trainer/\d+'))
    if trainer_link:
        row_data['trainer_name'] = trainer_link.get_text(strip=True)
        trainer_id_match = re.search(r'/trainer/(\d+)', trainer_link['href'])
        row_data['trainer_id'] = trainer_id_match.group(1) if trainer_id_match else None
    else:
        row_data['trainer_name'] = cells[7].get_text(strip=True)
        row_data['trainer_id'] = None
    
    # 馬体重（前走）
    if len(cells) > 8:
        weight_text = cells[8].get_text(strip=True)
        horse_weight, _ = parse_horse_weight(weight_text)
        row_data['horse_weight'] = horse_weight
    
    # 馬主
    if len(cells) > 9:
        row_data['owner_name'] = cells[9].get_text(strip=True)
    
    # 獲得賞金
    if len(cells) > 10:
        prize_text = cells[10].get_text(strip=True)
        row_data['prize_total'] = parse_prize_money(prize_text)
    
    # 前日オッズ
    if len(cells) > 11:
        odds_text = cells[11].get_text(strip=True)
        row_data['morning_odds'] = parse_float_or_none(odds_text)
    
    # 前日人気
    if len(cells) > 12:
        popularity_text = cells[12].get_text(strip=True)
        row_data['morning_popularity'] = parse_int_or_none(popularity_text)
    
    # 戦績（例: "5戦2勝"）
    if len(cells) > 13:
        career_text = cells[13].get_text(strip=True)
        row_data['career_stats'] = career_text
        starts, wins, places = parse_career_stats(career_text)
        row_data['career_starts'] = starts
        row_data['career_wins'] = wins
        row_data['career_places'] = places
    
    # 直近5走の着順（例: "12311"）
    if len(cells) > 14:
        last_5_text = cells[14].get_text(strip=True)
        row_data['last_5_finishes'] = parse_last_5_finishes(last_5_text)
    
    return row_data


def parse_career_stats(career_text: str) -> tuple:
    """
    戦績文字列をパース
    例: "15戦3勝[3-2-4-6]" → (15, 3, 6)
    
    Returns:
        (出走数, 勝利数, 2着・3着回数)
    """
    if not career_text:
        return (None, None, None)
    
    # パターン1: "N戦M勝[1-2-3-4]"
    match = re.match(r'(\d+)戦(\d+)勝\[(\d+)-(\d+)-(\d+)-(\d+)\]', career_text)
    if match:
        starts = int(match.group(1))
        wins = int(match.group(2))
        seconds = int(match.group(3))
        thirds = int(match.group(4))
        places = seconds + thirds
        return (starts, wins, places)
    
    # パターン2: "N戦M勝"
    match = re.match(r'(\d+)戦(\d+)勝', career_text)
    if match:
        starts = int(match.group(1))
        wins = int(match.group(2))
        return (starts, wins, None)
    
    return (None, None, None)


def parse_last_5_finishes(last_5_text: str) -> str:
    """
    直近5走の着順文字列をパース
    例: "1-2-3-1-1" → "12311"
    例: "123XX" → "123XX"（Xは出走なし）
    """
    if not last_5_text:
        return None
    
    # ハイフン除去
    cleaned = last_5_text.replace('-', '').replace(' ', '')
    
    # 5文字にパディング（不足分は'X'で埋める）
    if len(cleaned) < 5:
        cleaned = cleaned.ljust(5, 'X')
    
    return cleaned[:5]


# parse_int_or_none, parse_float_or_none, parse_sex_age, parse_horse_weight, parse_prize_money
# は results_parser.py と共通なので、共通モジュール（例: parsers/common_utils.py）に移動を推奨


### 5.4 馬情報パーサ（parsers/horse_info_parser.py）


"""
馬プロフィール・過去成績パーサ
"""

import re
import logging
from typing import Dict, List, Optional
from pathlib import Path

import pandas as pd
from bs4 import BeautifulSoup


def parse_horse_profile(file_path: str, horse_id: str = None) -> Dict:
    """
    馬プロフィールHTMLをパース
    
    Args:
        file_path: HTMLファイルパス
        horse_id: 馬ID
    
    Returns:
        プロフィール辞書
        
    Keys:
        - horse_id: str
        - horse_name: str (馬名)
        - horse_name_en: str (英語名)
        - birth_date: str (生年月日、ISO8601)
        - trainer_id: str
        - trainer_name: str
        - owner_name: str
        - breeder_name: str (生産者)
        - producing_area: str (産地)
        - sire_id: str (父馬ID)
        - sire_name: str (父馬名)
        - dam_id: str (母馬ID)
        - dam_name: str (母馬名)
        - damsire_id: str (母父ID)
        - damsire_name: str (母父名)
        - sex: str
        - coat_color: str (毛色)
    """
    logging.info(f"馬プロフィールパース開始: {file_path}")
    
    if horse_id is None:
        horse_id = extract_horse_id_from_filename(file_path)
    
    with open(file_path, 'rb') as f:
        html_bytes = f.read()
    
    try:
        html_text = html_bytes.decode('euc_jp', errors='replace')
    except:
        html_text = html_bytes.decode('utf-8', errors='replace')

    # ※ 検証済み（2025-11-16）: 'html.parser'で欠損率0%達成
    soup = BeautifulSoup(html_text, 'html.parser')  # 推奨（互換性が高い）
    # soup = BeautifulSoup(html_text, 'lxml')  # 非推奨（互換性の問題あり）

    profile = {'horse_id': horse_id}
    
    # 馬名
    horse_title = soup.find('div', class_='horse_title')
    if horse_title:
        h1 = horse_title.find('h1')
        if h1:
            profile['horse_name'] = h1.get_text(strip=True)
    
    # プロフィールテーブル
    profile_table = soup.find('table', attrs={'summary': 'のプロフィール'})
    
    if profile_table:
        rows = profile_table.find_all('tr')
        
        for row in rows:
            th = row.find('th')
            td = row.find('td')
            
            if not th or not td:
                continue
            
            label = th.get_text(strip=True)
            
            # 生年月日
            if '生年月日' in label:
                birth_text = td.get_text(strip=True)
                profile['birth_date'] = parse_birth_date(birth_text)
            
            # 調教師
            elif '調教師' in label:
                trainer_link = td.find('a', href=re.compile(r'/trainer/\d+'))
                if trainer_link:
                    profile['trainer_name'] = trainer_link.get_text(strip=True)
                    trainer_id_match = re.search(r'/trainer/(\d+)', trainer_link['href'])
                    profile['trainer_id'] = trainer_id_match.group(1) if trainer_id_match else None
            
            # 馬主
            elif '馬主' in label:
                profile['owner_name'] = td.get_text(strip=True)
            
            # 生産者
            elif '生産者' in label:
                profile['breeder_name'] = td.get_text(strip=True)
            
            # 産地
            elif '産地' in label:
                profile['producing_area'] = td.get_text(strip=True)
            
            # 性別
            elif '性別' in label:
                profile['sex'] = td.get_text(strip=True)
            
            # 毛色
            elif '毛色' in label:
                profile['coat_color'] = td.get_text(strip=True)
    
    # 血統情報（父・母・母父）
    blood_table = soup.find('table', class_='blood_table')
    if blood_table:
        # 父馬
        sire_link = blood_table.find('a', href=re.compile(r'/horse/\d+'), text=re.compile(r'.+'))
        if sire_link:
            profile['sire_name'] = sire_link.get_text(strip=True)
            sire_id_match = re.search(r'/horse/(\d+)', sire_link['href'])
            profile['sire_id'] = sire_id_match.group(1) if sire_id_match else None
        
        # 母馬・母父（簡易版、詳細は血統ページで取得）
        all_horse_links = blood_table.find_all('a', href=re.compile(r'/horse/\d+'))
        if len(all_horse_links) >= 2:
            dam_link = all_horse_links[1]
            profile['dam_name'] = dam_link.get_text(strip=True)
            dam_id_match = re.search(r'/horse/(\d+)', dam_link['href'])
            profile['dam_id'] = dam_id_match.group(1) if dam_id_match else None
        
        if len(all_horse_links) >= 3:
            damsire_link = all_horse_links[2]
            profile['damsire_name'] = damsire_link.get_text(strip=True)
            damsire_id_match = re.search(r'/horse/(\d+)', damsire_link['href'])
            profile['damsire_id'] = damsire_id_match.group(1) if damsire_id_match else None
    
    logging.info(f"馬プロフィールパース完了: {horse_id}")
    
    return profile


def parse_horse_performance(file_path: str, horse_id: str = None) -> pd.DataFrame:
    """
    馬の過去成績HTMLをパース
    
    Args:
        file_path: HTMLファイルパス（AJAX APIレスポンス）
        horse_id: 馬ID
    
    Returns:
        過去成績DataFrame
        
    Columns:
        - horse_id: str
        - race_date: str (ISO8601)
        - venue: str (競馬場)
        - weather: str
        - race_number: int
        - race_name: str
        - race_grade: str (G1, G2, G3, OP, 1600万等)
        - head_count: int (頭数)
        - bracket_number: int
        - horse_number: int
        - finish_position: int
        - jockey_name: str
        - basis_weight: float
        - distance_m: int
        - track_surface: str (芝/ダート)
        - track_condition: str (良/稍重/重/不良)
        - finish_time_str: str
        - finish_time_seconds: float
        - margin_str: str
        - margin_seconds: float
        - passing_order: str
        - last_3f_time: float
        - win_odds: float
        - popularity: int
        - horse_weight: int
        - horse_weight_change: int
        - race_id: str
    """
    logging.info(f"馬過去成績パース開始: {file_path}")
    
    if horse_id is None:
        horse_id = extract_horse_id_from_filename(file_path)
    
    with open(file_path, 'rb') as f:
        html_bytes = f.read()
    
    try:
        html_text = html_bytes.decode('utf-8', errors='replace')
    except:
        html_text = html_bytes.decode('euc_jp', errors='replace')

    # ※ 検証済み（2025-11-16）: 'html.parser'で欠損率0%達成
    soup = BeautifulSoup(html_text, 'html.parser')  # 推奨（互換性が高い）
    # soup = BeautifulSoup(html_text, 'lxml')  # 非推奨（互換性の問題あり）

    # 過去成績テーブル
    perf_table = soup.find('table', class_='db_h_race_results')
    
    if not perf_table:
        logging.warning(f"過去成績テーブルが見つかりません: {file_path}")
        return pd.DataFrame()
    
    rows = []
    
    tbody = perf_table.find('tbody')
    if not tbody:
        return pd.DataFrame()
    
    for tr in tbody.find_all('tr'):
        try:
            row_data = parse_horse_performance_row(tr, horse_id)
            if row_data:
                rows.append(row_data)
        except Exception as e:
            logging.warning(f"行のパースエラー: {e}")
            continue
    
    df = pd.DataFrame(rows)
    
    logging.info(f"馬過去成績パース完了: {horse_id} ({len(df)}レース)")
    
    return df


def parse_horse_performance_row(tr, horse_id: str) -> Optional[Dict]:
    """
    過去成績テーブルの1行をパース
    """
    cells = tr.find_all('td')
    
    if len(cells) < 15:
        return None
    
    row_data = {'horse_id': horse_id}
    
    # 日付
    date_text = cells[0].get_text(strip=True)
    row_data['race_date'] = parse_race_date(date_text)
    
    # 競馬場
    venue_text = cells[1].get_text(strip=True)
    row_data['venue'] = venue_text
    
    # 天気
    weather_text = cells[2].get_text(strip=True)
    row_data['weather'] = weather_text
    
    # レース番号
    race_num_text = cells[3].get_text(strip=True)
    row_data['race_number'] = parse_int_or_none(race_num_text.replace('R', ''))
    
    # レース名・レースID
    race_link = cells[4].find('a', href=re.compile(r'/race/\d+'))
    if race_link:
        row_data['race_name'] = race_link.get_text(strip=True)
        race_id_match = re.search(r'/race/(\d+)', race_link['href'])
        row_data['race_id'] = race_id_match.group(1) if race_id_match else None
    else:
        row_data['race_name'] = cells[4].get_text(strip=True)
        row_data['race_id'] = None
    
    # レースグレード（例: G1, OP, 1600万）
    if len(cells) > 5:
        grade_text = cells[5].get_text(strip=True)
        row_data['race_grade'] = grade_text if grade_text else None
    
    # 頭数
    head_count_text = cells[6].get_text(strip=True)
    row_data['head_count'] = parse_int_or_none(head_count_text.replace('頭', ''))
    
    # 枠番
    bracket_text = cells[7].get_text(strip=True)
    row_data['bracket_number'] = parse_int_or_none(bracket_text)
    
    # 馬番
    horse_num_text = cells[8].get_text(strip=True)
    row_data['horse_number'] = parse_int_or_none(horse_num_text)
    
    # 着順
    finish_text = cells[9].get_text(strip=True)
    row_data['finish_position'] = parse_int_or_none(finish_text)
    
    # 騎手名
    jockey_text = cells[10].get_text(strip=True)
    row_data['jockey_name'] = jockey_text
    
    # 斤量
    weight_text = cells[11].get_text(strip=True)
    row_data['basis_weight'] = parse_float_or_none(weight_text)
    
    # 距離・馬場
    distance_text = cells[12].get_text(strip=True)
    distance, surface, condition = parse_distance_surface(distance_text)
    row_data['distance_m'] = distance
    row_data['track_surface'] = surface
    row_data['track_condition'] = condition
    
    # タイム
    time_text = cells[13].get_text(strip=True)
    row_data['finish_time_str'] = time_text
    row_data['finish_time_seconds'] = parse_time_to_seconds(time_text)
    
    # 着差
    margin_text = cells[14].get_text(strip=True)
    row_data['margin_str'] = margin_text
    row_data['margin_seconds'] = parse_margin_to_seconds(margin_text)
    
    # 通過順
    if len(cells) > 15:
        passing_text = cells[15].get_text(strip=True)
        row_data['passing_order'] = passing_text
    
    # 上がり3F
    if len(cells) > 16:
        last_3f_text = cells[16].get_text(strip=True)
        row_data['last_3f_time'] = parse_float_or_none(last_3f_text)
    
    # 単勝オッズ
    if len(cells) > 17:
        odds_text = cells[17].get_text(strip=True)
        row_data['win_odds'] = parse_float_or_none(odds_text)
    
    # 人気
    if len(cells) > 18:
        popularity_text = cells[18].get_text(strip=True)
        row_data['popularity'] = parse_int_or_none(popularity_text)
    
    # 馬体重
    if len(cells) > 19:
        weight_change_text = cells[19].get_text(strip=True)
        horse_weight, weight_change = parse_horse_weight(weight_change_text)
        row_data['horse_weight'] = horse_weight
        row_data['horse_weight_change'] = weight_change
    
    return row_data


def parse_birth_date(birth_text: str) -> str:
    """
    生年月日文字列をISO8601に変換
    例: "2020年3月15日" → "2020-03-15"
    """
    if not birth_text:
        return None
    
    match = re.search(r'(\d{4})年(\d+)月(\d+)日', birth_text)
    if match:
        year = match.group(1)
        month = match.group(2).zfill(2)
        day = match.group(3).zfill(2)
        return f"{year}-{month}-{day}"
    
    return None


def parse_race_date(date_text: str) -> str:
    """
    レース日付文字列をISO8601に変換
    例: "2023/06/01" → "2023-06-01"
    """
    if not date_text:
        return None
    
    # スラッシュをハイフンに置換
    date_normalized = date_text.replace('/', '-')
    
    # YYYY-MM-DD形式の検証
    match = re.match(r'(\d{4})-(\d{2})-(\d{2})', date_normalized)
    if match:
        return date_normalized
    
    return None


def parse_distance_surface(distance_text: str) -> tuple:
    """
    距離・馬場文字列をパース
    例: "芝1600" → (1600, "芝", None)
    例: "ダ1800良" → (1800, "ダート", "良")
    
    Returns:
        (距離, 馬場種別, 馬場状態)
    """
    if not distance_text:
        return (None, None, None)
    
    # パターン1: "芝1600良"
    match = re.match(r'(芝|ダ)(\d+)(良|稍重|重|不良)?', distance_text)
    if match:
        surface_abbr = match.group(1)
        distance = int(match.group(2))
        condition = match.group(3) if match.group(3) else None
        
        surface = "芝" if surface_abbr == "芝" else "ダート"
        
        return (distance, surface, condition)
    
    # パターン2: "1600" (馬場種別なし)
    match = re.match(r'(\d+)', distance_text)
    if match:
        distance = int(match.group(1))
        return (distance, None, None)
    
    return (None, None, None)


def extract_horse_id_from_filename(file_path: str) -> str:
    """
    ファイル名から馬IDを抽出
    例: 2023010101_profile_20251106T120000+09:00_sha256=abcd.bin → 2023010101
    """
    filename = Path(file_path).stem
    match = re.search(r'(\d{10})', filename)
    return match.group(1) if match else None

### 5.5 血統パーサ（parsers/pedigree\_parser.py）

"""
血統情報パーサ (修正版 - 世代情報(generation)カラム追加および構造的パース)
"""

import re
import logging
from typing import Dict, List, Optional
from pathlib import Path

import pandas as pd
from bs4 import BeautifulSoup, element

# 世代特定用のマップ

# (rowspan, height) -\> generation

# 1代目(親) ～ 5代目(高祖父母の親)

GENERATION\_MAP = {
"16": 1, \# rowspan="16"
"8": 2,  \# rowspan="8"
"4": 3,  \# rowspan="4"
"2": 4,  \# rowspan="2"
"20": 5, \# height="20"
}

def parse\_pedigree\_html(file\_path: str, horse\_id: str = None) -\> pd.DataFrame:
"""
血統HTMLをパースしてDataFrameを返す (構造解析対応版)

```
Args:
    file_path: HTMLファイルパス
    horse_id: 馬ID

Returns:
    血統DataFrame (ロングフォーマット)
    
Columns:
    - horse_id: str (対象馬ID)
    - ancestor_id: str (祖先馬ID)
    - ancestor_name: str (祖先馬名)
    - generation: int (世代: 1=親, 2=祖父母, ...)
"""
logging.info(f"血統情報パース開始: {file_path}")

if horse_id is None:
    horse_id = extract_horse_id_from_filename(file_path)

with open(file_path, 'rb') as f:
    html_bytes = f.read()

try:
    html_text = html_bytes.decode('euc_jp', errors='replace')
except Exception:
    html_text = html_bytes.decode('utf-8', errors='replace')

soup = BeautifulSoup(html_text, 'html.parser')

blood_table = soup.find('table', class_='blood_table')

if not blood_table:
    logging.error(f"血統テーブルが見つかりません: {file_path}")
    return pd.DataFrame()

rows = []

# 収集済みのIDを管理するセット（重複防止用）
collected_ids = set()

# blood_table 内のすべての <td> タグを取得
all_tds = blood_table.find_all('td')

for td in all_tds:
    generation = None
    
    # 1. rowspan 属性で世代を特定
    if td.has_attr('rowspan'):
        rowspan_val = td.get('rowspan')
        if rowspan_val in GENERATION_MAP:
            generation = GENERATION_MAP[rowspan_val]
    
    # 2. rowspan がない場合、height 属性で世代を特定 (5代目)
    elif td.has_attr('height'):
        height_val = td.get('height')
        if height_val in GENERATION_MAP:
            generation = GENERATION_MAP[height_val]

    # 世代が特定できた <td> のみ処理
    if generation:
        # <td> タグ内の馬リンク <a> を探す
        link = td.find('a', href=re.compile(r'/horse/'))
        
        if link:
            href = link.get('href')
            match = re.search(r'/horse/([^/]+)', href)
            
            if match:
                raw_id = match.group(1)
                
                # IDと名前を正規化
                ancestor_id = normalize_ancestor_id(raw_id)
                ancestor_name = normalize_ancestor_name(td, link)

                # バリデーション: 有効なIDと名前のみ追加
                if ancestor_id and ancestor_name:
                    # 重複チェック
                    if ancestor_id not in collected_ids:
                        rows.append({
                            'horse_id': horse_id,
                            'ancestor_id': ancestor_id,
                            'ancestor_name': ancestor_name,
                            'generation': generation, # 世代カラムを追加
                        })
                        collected_ids.add(ancestor_id)

df = pd.DataFrame(rows)
logging.info(f"血統情報パース完了: {file_path} ({len(df)}行)")

return df
```

def normalize\_ancestor\_id(ancestor\_id: str) -\> Optional[str]:
"""
祖先馬IDを正規化（英字対応・フィルタリング強化版）

```
Args:
    ancestor_id: 生のID文字列

Returns:
    正規化されたID、または無効な場合はNone
"""
if not ancestor_id or ancestor_id.strip() == '':
    return None

# 英数字のみを抽出（記号やスペースは削除）
cleaned = re.sub(r'[^a-zA-Z0-9]', '', ancestor_id).strip()

if not cleaned:
    return None

# 外国馬（netkeiba ID = "000"）を除外
if cleaned == '000':
    return None

# 全ゼロパターンを除外（英字が含まれる場合は除外しない）
if cleaned.isdigit() and cleaned == '0' * len(cleaned):
    return None

# JRA馬IDおよび英数字混在IDの桁数チェック（4-10桁）
if len(cleaned) < 4 or len(cleaned) > 10:
    logging.debug(f"無効な桁数のIDをスキップ: {cleaned} (元: {ancestor_id})")
    return None

return cleaned
```

def normalize\_ancestor\_name(td\_tag: element.Tag, link\_tag: element.Tag) -\> Optional[str]:
"""
祖先馬名を正規化（構造的解析版）

```
Args:
    td_tag (element.Tag): 解析対象の <td>
    link_tag (element.Tag): <td> 内の <a>

Returns:
    正規化された馬名、またはNone
"""

# <a> タグ直下のテキスト (<span> タグを含む場合も考慮)
name = link_tag.get_text(strip=True)

if not name:
    # フォールバック
    td_texts = td_tag.find_all(string=True, recursive=False)
    if td_texts:
        name = td_texts[0].strip()
    else:
        name = td_tag.get_text(strip=True)

if not name:
    return None

name = name.strip()

if not name:
    return None

# <a> タグの .contents を見て、最初のテキストノードを取得
primary_name = None
if link_tag.contents:
    for content in link_tag.contents:
        if isinstance(content, element.NavigableString):
            primary_name = content.strip()
            if primary_name:
                break
        # <span> タグ (例: <span class="red">Mr. Prospector</span>) の中も見る
        elif isinstance(content, element.Tag) and content.name == 'span':
            primary_name = content.get_text(strip=True)
            if primary_name:
                break

if primary_name:
    name = primary_name

if not name:
    return None

# 改行や <br> が含まれる場合、最初の行のみ取得
name = name.split('\n')[0].split('<br>')[0].strip()

if not name:
    return None

return name
```

def extract\_horse\_id\_from\_filename(file\_path: str) -\> str:
"""
ファイル名から馬IDを抽出 (ped\_XXXXXXXXXX.html または ped\_YYYYYYYYYY\_...)
"""
filename = Path(file\_path).stem

```
# ped_2001103890...
match = re.search(r'ped_(\d{10})', filename)
if match:
    return match.group(1)

# ped_000a00033a...
match = re.search(r'ped_([a-zA-Z0-9]{10})', filename)
if match:
    return match.group(1)
    
# 2001103890_... (ped_ プレフィックスなし)
match = re.search(r'^(\d{10})', filename)
if match:
    return match.group(1)

logging.warning(f"ファイル名 {filename} から horse_id (10桁) を抽出できませんでした。")
return None
```

### 5.6 データクレンジングとバリデーション

#### 5.6.1 クレンジングルール設定（configs/data_cleaning.yaml）

yaml
# データクレンジング設定

# 数値範囲チェック
value_ranges:
  finish_position:
    min: 1
    max: 18
  horse_number:
    min: 1
    max: 18
  basis_weight:
    min: 48.0
    max: 60.0
  horse_weight:
    min: 350
    max: 600
  horse_weight_change:
    min: -50
    max: 50
  win_odds:
    min: 1.0
    max: 999.9
  distance_m:
    min: 1000
    max: 4000

# 許容値リスト
allowed_values:
  sex:
    - "牡"
    - "牝"
    - "セ"
  track_surface:
    - "芝"
    - "ダート"
    - "障害"
  track_condition:
    - "良"
    - "稍重"
    - "重"
    - "不良"
  weather:
    - "晴"
    - "曇"
    - "雨"
    - "雪"
    - "小雨"
    - "小雪"

# 欠損値処理
missing_value_handling:
  # 必須カラム（nullを許容しない）
  required_columns:
    - race_id
    - horse_id
    - finish_position
  
  # 欠損値の補完方法
  imputation:
    horse_weight:
      method: "forward_fill"  # 前回値で補完
      group_by: ["horse_id"]
    
    horse_weight_change:
      method: "zero"  # 0で補完
    
    last_3f_time:
      method: "median"  # 中央値で補完
      group_by: ["race_id"]
    
    passing_order:
      method: "none"  # 補完しない

# 異常値検出
outlier_detection:
  finish_time_seconds:
    method: "iqr"  # 四分位範囲法
    multiplier: 3.0
  
  last_3f_time:
    method: "iqr"
    multiplier: 3.0
  
  win_odds:
    method: "percentile"
    lower: 0.01
    upper: 0.99

# データ型変換
type_conversions:
  race_id: "string"
  horse_id: "string"
  jockey_id: "string"
  trainer_id: "string"
  finish_position: "Int64"  # nullable integer
  horse_number: "Int64"
  bracket_number: "Int64"
  age: "Int64"
  basis_weight: "float64"
  horse_weight: "Int64"
  horse_weight_change: "Int64"
  finish_time_seconds: "float64"
  margin_seconds: "float64"
  last_3f_time: "float64"
  win_odds: "float64"
  popularity: "Int64"
  distance_m: "Int64"


#### 5.6.2 バリデーションモジュール（parsers/validators.py）


"""
データバリデーションモジュール
"""

import logging
from typing import Dict, List, Any, Optional
import pandas as pd
import numpy as np


def validate_dataframe(
    df: pd.DataFrame,
    schema: Dict[str, Any],
    validation_rules: Dict[str, Any]
) -> tuple:
    """
    DataFrameのバリデーション
    
    Args:
        df: 検証対象のDataFrame
        schema: スキーマ定義（カラム名と型）
        validation_rules: バリデーションルール
    
    Returns:
        (valid_df, invalid_rows, validation_report)
    """
    validation_report = {
        'total_rows': len(df),
        'valid_rows': 0,
        'invalid_rows': 0,
        'errors': []
    }
    
    invalid_indices = set()
    
    # 1. 必須カラムチェック
    required_columns = validation_rules.get('required_columns', [])
    for col in required_columns:
        if col not in df.columns:
            validation_report['errors'].append({
                'type': 'missing_column',
                'column': col
            })
            logging.error(f"必須カラムが存在しません: {col}")
            return df.iloc[0:0], df, validation_report
        
        # null チェック
        null_mask = df[col].isnull()
        null_count = null_mask.sum()
        
        if null_count > 0:
            invalid_indices.update(df[null_mask].index.tolist())
            validation_report['errors'].append({
                'type': 'required_null',
                'column': col,
                'count': int(null_count)
            })
            logging.warning(f"必須カラムにnullが含まれています: {col} ({null_count}行)")
    
    # 2. 数値範囲チェック
    value_ranges = validation_rules.get('value_ranges', {})
    for col, range_def in value_ranges.items():
        if col not in df.columns:
            continue
        
        col_data = df[col].dropna()
        
        if len(col_data) == 0:
            continue
        
        min_val = range_def.get('min')
        max_val = range_def.get('max')
        
        if min_val is not None:
            under_min = df[col] < min_val
            under_count = under_min.sum()
            if under_count > 0:
                invalid_indices.update(df[under_min].index.tolist())
                validation_report['errors'].append({
                    'type': 'value_under_min',
                    'column': col,
                    'min': min_val,
                    'count': int(under_count)
                })
        
        if max_val is not None:
            over_max = df[col] > max_val
            over_count = over_max.sum()
            if over_count > 0:
                invalid_indices.update(df[over_max].index.tolist())
                validation_report['errors'].append({
                    'type': 'value_over_max',
                    'column': col,
                    'max': max_val,
                    'count': int(over_count)
                })
    
    # 3. 許容値チェック
    allowed_values = validation_rules.get('allowed_values', {})
    for col, allowed_list in allowed_values.items():
        if col not in df.columns:
            continue
        
        col_data = df[col].dropna()
        
        if len(col_data) == 0:
            continue
        
        invalid_values = ~col_data.isin(allowed_list)
        invalid_count = invalid_values.sum()
        
        if invalid_count > 0:
            invalid_indices.update(df[invalid_values].index.tolist())
            validation_report['errors'].append({
                'type': 'invalid_value',
                'column': col,
                'allowed': allowed_list,
                'count': int(invalid_count)
            })
    
    # 4. 異常値検出
    outlier_detection = validation_rules.get('outlier_detection', {})
    for col, outlier_def in outlier_detection.items():
        if col not in df.columns:
            continue
        
        col_data = df[col].dropna()
        
        if len(col_data) < 10:  # サンプル数が少ない場合はスキップ
            continue
        
        method = outlier_def.get('method')
        
        if method == 'iqr':
            multiplier = outlier_def.get('multiplier', 1.5)
            outlier_mask = detect_outliers_iqr(df[col], multiplier)
            outlier_count = outlier_mask.sum()
            
            if outlier_count > 0:
                invalid_indices.update(df[outlier_mask].index.tolist())
                validation_report['errors'].append({
                    'type': 'outlier_iqr',
                    'column': col,
                    'count': int(outlier_count)
                })
        
        elif method == 'percentile':
            lower = outlier_def.get('lower', 0.01)
            upper = outlier_def.get('upper', 0.99)
            outlier_mask = detect_outliers_percentile(df[col], lower, upper)
            outlier_count = outlier_mask.sum()
            
            if outlier_count > 0:
                invalid_indices.update(df[outlier_mask].index.tolist())
                validation_report['errors'].append({
                    'type': 'outlier_percentile',
                    'column': col,
                    'count': int(outlier_count)
                })
    
    # 5. valid/invalid の分離
    valid_mask = ~df.index.isin(invalid_indices)
    valid_df = df[valid_mask].copy()
    invalid_df = df[~valid_mask].copy()
    
    validation_report['valid_rows'] = len(valid_df)
    validation_report['invalid_rows'] = len(invalid_df)
    
    logging.info(f"バリデーション完了: {len(valid_df)}/{len(df)} 行が有効")
    
    return valid_df, invalid_df, validation_report


def detect_outliers_iqr(series: pd.Series, multiplier: float = 1.5) -> pd.Series:
    """
    IQR法による異常値検出
    
    Args:
        series: 対象のSeries
        multiplier: IQRの倍数
    
    Returns:
        異常値マスク（True=異常値）
    """
    Q1 = series.quantile(0.25)
    Q3 = series.quantile(0.75)
    IQR = Q3 - Q1
    
    lower_bound = Q1 - multiplier * IQR
    upper_bound = Q3 + multiplier * IQR
    
    outlier_mask = (series < lower_bound) | (series > upper_bound)
    
    return outlier_mask


def detect_outliers_percentile(
    series: pd.Series,
    lower: float = 0.01,
    upper: float = 0.99
) -> pd.Series:
    """
    パーセンタイル法による異常値検出
    
    Args:
        series: 対象のSeries
        lower: 下側パーセンタイル
        upper: 上側パーセンタイル
    
    Returns:
        異常値マスク（True=異常値）
    """
    lower_bound = series.quantile(lower)
    upper_bound = series.quantile(upper)
    
    outlier_mask = (series < lower_bound) | (series > upper_bound)
    
    return outlier_mask


def apply_data_cleaning(
    df: pd.DataFrame,
    cleaning_rules: Dict[str, Any]
) -> pd.DataFrame:
    """
    データクレンジングを適用
    
    Args:
        df: 対象DataFrame
        cleaning_rules: クレンジングルール
    
    Returns:
        クレンジング済みDataFrame
    """
    df_cleaned = df.copy()
    
    # 1. 欠損値補完
    imputation_rules = cleaning_rules.get('imputation', {})
    
    for col, impute_def in imputation_rules.items():
        if col not in df_cleaned.columns:
            continue
        
        method = impute_def.get('method')
        
        if method == 'forward_fill':
            group_by = impute_def.get('group_by')
            if group_by:
                df_cleaned[col] = df_cleaned.groupby(group_by)[col].ffill()
            else:
                df_cleaned[col] = df_cleaned[col].ffill()
        
        elif method == 'backward_fill':
            group_by = impute_def.get('group_by')
            if group_by:
                df_cleaned[col] = df_cleaned.groupby(group_by)[col].bfill()
            else:
                df_cleaned[col] = df_cleaned[col].bfill()
        
        elif method == 'zero':
            df_cleaned[col] = df_cleaned[col].fillna(0)
        
        elif method == 'median':
            group_by = impute_def.get('group_by')
            if group_by:
                df_cleaned[col] = df_cleaned.groupby(group_by)[col].transform(
                    lambda x: x.fillna(x.median())
                )
            else:
                df_cleaned[col] = df_cleaned[col].fillna(df_cleaned[col].median())
        
        elif method == 'mean':
            group_by = impute_def.get('group_by')
            if group_by:
                df_cleaned[col] = df_cleaned.groupby(group_by)[col].transform(
                    lambda x: x.fillna(x.mean())
                )
            else:
                df_cleaned[col] = df_cleaned[col].fillna(df_cleaned[col].mean())
    
    # 2. 型変換
    type_conversions = cleaning_rules.get('type_conversions', {})
    
    for col, dtype in type_conversions.items():
        if col not in df_cleaned.columns:
            continue
        
        try:
            df_cleaned[col] = df_cleaned[col].astype(dtype)
        except Exception as e:
            logging.warning(f"型変換失敗: {col} -> {dtype}: {e}")
    
    return df_cleaned


### 5.5 データ品質検証（2025-11-16検証結果）

#### 5.5.1 検証概要

**検証日**: 2025-11-16
**検証対象**: 2023-10-09のレースデータ（24レース、311頭出走）
**検証方法**: `debug_full_pipeline_by_date.py`によるスクレイピング＆パース

**検証項目**:
1. distance_m（距離）の欠損率
2. track_surface（馬場種類）の欠損率
3. horse_id（馬ID）の欠損率
4. データ整合性（race_results vs shutuba）

#### 5.5.2 検証結果

| 項目 | 結果 | 評価 |
|------|------|------|
| **race_results.csv** | 311行（24レース） | ✅ 正常 |
| distance_m欠損率 | 0行 (0.00%) | ✅ 合格 |
| track_surface欠損率 | 0行 (0.00%) | ✅ 合格 |
| **shutuba.csv** | 311行 | ✅ 正常 |
| horse_id欠損率 | 0行 (0.00%) | ✅ 合格 |
| データ整合性 | race_id完全一致 | ✅ 合格 |
| **horses.csv（血統）** | 1,181行（20頭×5世代） | ✅ 正常 |
| **horses_performance.csv** | 469走（平均23.5走/頭） | ✅ 正常 |

**track_surface分布**:
- ダート: 162頭 (52.1%)
- 芝: 137頭 (44.1%)
- 障害: 12頭 (3.9%)

#### 5.5.3 パーサー改良の成果

**改良前**（2025-11-10以前）:
- distance_m欠損率: 10.2%
- track_surface欠損率: 0.8%
- 問題: 単一セレクタ依存、障害レース未対応

**改良後**（2025-11-16）:
- distance_m欠損率: **0.0%**
- track_surface欠損率: **0.0%**
- 改良点: 4段階フォールバック実装、障害レース対応

**改良内容**:
1. HTMLパーサーを`lxml`から`html.parser`に変更（互換性向上）
2. 4段階フォールバック実装（data_intro → diary_snap_cut → racedata → デフォルト）
3. 正規表現パターンの改良（障害レース対応）
4. metadata_sourceカラム追加（デバッグ用）

**4段階フォールバック実装の詳細**:


def extract_race_metadata_enhanced(soup: BeautifulSoup) -> Dict:
    """
    拡張されたレースメタデータ抽出（4段階フォールバック）

    レベル1: data_intro（最も詳細な情報）
    レベル2: diary_snap_cut（代替フォーマット）
    レベル3: racedata（最小限の情報）
    レベル4: デフォルト値 + ログ警告
    """
    metadata = {
        'distance_m': None,
        'track_surface': None,
        'weather': None,
        'track_condition': None,
        'post_time': None,
        'race_name': None,
        'venue': None,
        'race_class': None,
        'head_count': None,
        'metadata_source': None  # どのレベルで取得できたかを記録
    }

    # レベル1: data_intro（最優先）
    race_data_intro = soup.find('div', class_='data_intro')
    if race_data_intro:
        metadata['metadata_source'] = 'data_intro'
        span_text = race_data_intro.find('span')
        if span_text:
            text = span_text.get_text()

            # 距離と馬場の抽出（改良版）
            # パターン: "芝右2000m", "ダ左1600m", "障2800m"
            distance_match = re.search(r'(芝|ダ|障)\s*(?:右|左|直|外|内)?\s*(\d+)m', text)
            if distance_match:
                surface_map = {'芝': '芝', 'ダ': 'ダート', '障': '障害'}
                metadata['track_surface'] = surface_map.get(distance_match.group(1))
                metadata['distance_m'] = int(distance_match.group(2))

            # 天候の抽出
            weather_match = re.search(r'天候\s*:\s*(\S+)', text)
            if weather_match:
                metadata['weather'] = weather_match.group(1)

            # 馬場状態の抽出
            condition_match = re.search(r'馬場\s*:\s*(\S+)', text)
            if condition_match:
                metadata['track_condition'] = condition_match.group(1)

        # レベル1で取得できた場合は早期リターン
        if metadata['distance_m'] is not None:
            return metadata

    # レベル2: diary_snap_cut（フォールバック）
    diary_snap = soup.find('div', class_='diary_snap_cut')
    if diary_snap:
        metadata['metadata_source'] = 'diary_snap_cut'
        span_text = diary_snap.find('span')
        if span_text:
            text = span_text.get_text()

            # レベル1と同様の抽出処理
            distance_match = re.search(r'(芝|ダ|障)\s*(?:右|左|直|外|内)?\s*(\d+)m', text)
            if distance_match:
                surface_map = {'芝': '芝', 'ダ': 'ダート', '障': '障害'}
                metadata['track_surface'] = surface_map.get(distance_match.group(1))
                metadata['distance_m'] = int(distance_match.group(2))

        if metadata['distance_m'] is not None:
            return metadata

    # レベル3: racedata（最小限フォールバック）
    race_data_dl = soup.find('dl', class_='racedata')
    if race_data_dl:
        metadata['metadata_source'] = 'racedata'
        dd = race_data_dl.find('dd')
        if dd:
            text = dd.get_text()

            # 簡易版の抽出処理
            distance_match = re.search(r'(芝|ダ|障).*?(\d+)m', text)
            if distance_match:
                surface_map = {'芝': '芝', 'ダ': 'ダート', '障': '障害'}
                metadata['track_surface'] = surface_map.get(distance_match.group(1))
                metadata['distance_m'] = int(distance_match.group(2))

        if metadata['distance_m'] is not None:
            return metadata

    # レベル4: すべて失敗
    metadata['metadata_source'] = 'failed'
    logging.warning(f"レースメタデータの抽出に失敗（全フォールバックレベルで取得不可）")

    return metadata


**検証結果**:
- 2025-11-16検証: 24レース（311頭）で**欠損率0%**を達成
- 対応HTMLフォーマット:
  - 中央競馬（平地・障害）
  - 地方競馬
  - 古いHTMLフォーマット（2020年以前）

**改良前の問題**:
- 単一セレクタ（`data_intro`のみ）に依存
- HTMLフォーマット変更で10.2%の欠損率

**改良後の成果**:
- 4段階フォールバックで完全カバー
- 欠損率0%を達成

#### 5.5.4 検証に使用したツール

**ツール**: `debug_full_pipeline_by_date.py`

**場所**: ルートディレクトリ

**用途**: デバッグ・検証専用

**使用方法**:

bash
# 特定日付のデータを検証
python debug_full_pipeline_by_date.py \
    --date 2023-10-09 \
    --output-dir output_final \
    --parse-only


**出力**:
- race_results.csv
- shutuba.csv
- horses.csv（血統データ）
- horses_performance.csv

**注意**:
- 本番運用には正式パイプライン（`run_parsing_pipeline_local.py`）を使用
- このツールはデバッグ・検証目的のみ
- 出力形式はCSV（検証用）、正式パイプラインはParquet（本番用）

#### 5.5.5 今後の検証計画

1. **大規模検証**（優先度: HIGH）
   - 対象: 2020-2024の全データ（約18,000レース）
   - 目的: 欠損率0%が全期間で維持されるか確認

2. **エッジケース検証**（優先度: MEDIUM）
   - 地方競馬（大井、川崎、船橋など）
   - 古いHTMLフォーマット（2015年以前）
   - 特殊レース（障害、ダート3600m超など）

3. **継続的モニタリング**（優先度: LOW）
   - 週次での欠損率チェック
   - HTMLフォーマット変更の早期検出


---
## 6. 特徴量エンジニアリング

### 6.1 概要と設計方針

特徴量エンジニアリングは、機械学習モデルの性能を決定づける最重要工程である。
本システムでは、「**汎用性・再現性・競走馬特有のドメイン知識**」を両立するために、特徴量生成をモジュール化・設定ファイル化して管理する。

特徴量は以下の6カテゴリで構成する。

| カテゴリ | 名称       | 代表的な特徴量例                        |
| ---- | -------- | ------------------------------- |
| ①    | 馬の基本特徴   | 年齢、性別、馬体重、過去成績統計                |
| ②    | 騎手・調教師特徴 | 騎手勝率、調教師勝率、コンビネーション実績           |
| ③    | レース特徴    | 距離、馬場、天候、グレード、枠順                |
| ④    | 相対特徴     | レース内Z-score、順位ランク、人気乖離          |
| ⑤    | 時系列特徴    | 直近N走集約、間隔日数、調子トレンド              |
| ⑥    | 高度特徴     | AdjustedSpeed、ペース指数、血統Embedding |

すべての特徴量は **`features.yaml`** により設定可能で、再学習・再生成が容易な構造を取る。

---

### 6.2 特徴量設定ファイル（configs/features.yaml）

```yaml
# 特徴量設定ファイル: configs/features.yaml
# ===============================

# --- 基本特徴量 ---
basic_features:
  horse_basic:
    - age
    - sex
    - horse_weight
    - horse_weight_change
    - basis_weight
  race_basic:
    - distance_m
    - track_surface
    - track_condition
    - weather
    - head_count
    - bracket_number
    - horse_number

# --- 過去走集約 ---
past_performance_aggregation:
  windows: [1, 3, 5, 10]
  statistics: [mean, median, std, min, max]
  columns:
    - finish_position
    - finish_time_seconds
    - margin_seconds
    - last_3f_time
    - win_odds
    - popularity
    - horse_weight
  conditional_aggregation:
    by_distance_category:
      short: [1000, 1600]
      mile: [1601, 1899]
      intermediate: [1900, 2100]
      long: [2101, 4000]
    by_track_surface:
      - "芝"
      - "ダート"
    by_track_condition:
      good: ["良"]
      heavy: ["稍重", "重", "不良"]

# --- AdjustedSpeed ---
adjusted_speed:
  enabled: true
  reference_calculation:
    method: "median"
    min_samples: 10
    lookback_days: 365
  adjustment_factors:
    track_condition:
      "良": 1.000
      "稍重": 1.010
      "重": 1.025
      "不良": 1.050
    weight_diff:
      per_kg: 0.002
  buckets:
    distance_step: 200
    venue_grouping:
      enable: true

# --- 血統特徴量 ---
pedigree_features:
  enabled: true
  encoding_method: "target_encoding"  # target_encoding / embedding
  target_encoding:
    smoothing: 10
    min_samples: 5
  embedding:
    dimension: 16
    max_horses: 10000
  pedigree_depth: 3

# --- レース内正規化 ---
within_race_normalization:
  enabled: true
  zscore_columns:
    - horse_weight
    - basis_weight
    - past_3_finish_position_mean
    - past_1_last_3f_time_mean
  rank_columns:
    - morning_odds
    - horse_weight
    - past_3_finish_position_mean

# --- 騎手・調教師特徴 ---
jockey_trainer_features:
  enabled: true
  aggregation_period_days: 365
  statistics: [win_rate, place_rate, show_rate, average_finish_position, total_races]
  combination:
    jockey_trainer: true
    jockey_horse: true
    trainer_horse: true

# --- 時系列特徴 ---
temporal_features:
  enabled: true
  days_since_last_race:
    enabled: true
    bins: [0, 14, 30, 60, 90, 180, 365, 9999]
    labels: ["2週以内", "1ヶ月以内", "2ヶ月以内", "3ヶ月以内", "半年以内", "1年以内", "1年以上"]
  form_trend:
    enabled: true
    lookback_races: 5
    method: "linear_regression"

# --- 欠損値処理 ---
missing_value_strategy:
  categorical:
    method: "indicator"
  numerical:
    method: "median"
    group_by: ["race_id"]

# --- 特徴量選択 ---
feature_selection:
  enabled: false
  method: "importance"
  importance_threshold: 0.001
  correlation_threshold: 0.95

# --- 出力設定 ---
output:
  format: "parquet"
  compression: "snappy"
  partition_by: ["year", "month"]
  save_feature_names: true
```

---

### 6.3 特徴量生成エンジン（features/feature_engine.py）

```python
"""
特徴量生成エンジン
"""
import logging
from typing import Dict, List, Optional
from pathlib import Path
import pandas as pd
import numpy as np

class FeatureEngine:
    """特徴量生成エンジン"""
    def __init__(self, config: Dict):
        self.config = config
        self.feature_names = []

    def generate_features(
        self,
        shutuba_df: pd.DataFrame,
        results_history_df: pd.DataFrame,
        horse_profiles_df: pd.DataFrame,
        pedigree_df: pd.DataFrame,
        jockey_stats_df: Optional[pd.DataFrame] = None,
        trainer_stats_df: Optional[pd.DataFrame] = None
    ) -> pd.DataFrame:
        logging.info("特徴量生成開始")
        df = shutuba_df.copy()
        df = self._add_basic_features(df)
        df = self._add_past_performance_features(df, results_history_df)
        if self.config["adjusted_speed"]["enabled"]:
            df = self._add_adjusted_speed(df, results_history_df)
        if self.config["pedigree_features"]["enabled"]:
            df = self._add_pedigree_features(df, pedigree_df, results_history_df)
        if self.config["jockey_trainer_features"]["enabled"]:
            df = self._add_jockey_trainer_features(df, results_history_df, jockey_stats_df, trainer_stats_df)
        if self.config["temporal_features"]["enabled"]:
            df = self._add_temporal_features(df, results_history_df)
        if self.config["within_race_normalization"]["enabled"]:
            df = self._add_relative_features(df)
        df = self._handle_missing_values(df)
        self.feature_names = [c for c in df.columns if c not in ["race_id", "horse_id", "horse_number"]]
        logging.info(f"特徴量生成完了: {len(self.feature_names)}個")
        return df
```

（※以降の `_add_basic_features`, `_add_past_performance_features`, `_add_adjusted_speed`, `_add_pedigree_features`, `_add_jockey_trainer_features`, `_add_temporal_features`, `_add_relative_features`, `_handle_missing_values`, `save_features` は既存構造と互換性保持。文法修正済み・詳細定義は前章同一。）

---

### 6.4 AdjustedSpeed（走破時計補正）

AdjustedSpeedCalculator クラスを別モジュールとして保持。
補正式の改訂点：

```
adjusted_speed = (reference_time / finish_time_seconds) * 100 * (track_condition_factor * weight_diff_factor)
```

に統一。
また、track_condition・weight_diff 補正を同時適用する仕様とした。

---

### 6.5 血統特徴量生成（PedigreeEncoder）

Target Encoding + Embedding の2方式対応。
学習型Embedding利用時は `config["embedding"]["dimension"]` で次元を指定。
既存の `_target_encoding` / `_neural_embedding` を保持。

---

### 6.6 特徴量検証・保存

特徴量生成後、出力フォーマットは `Parquet + Snappy` 形式。
特徴量名リストを `feature_names.txt` に同時出力。

# 7\. 数理モデル（mu, sigma, nu と Task B の再定義）

**（本章は「7. 数理モデル」を完全に置換する想定の全文です。特に Task B（理論オッズ予測）の学習目的と使用目的を論理的に矛盾なく定義し直しています。）**

## 7.1 モデル群の役割（再整理）

本システムは次の主要モデルを保持する：

  * **μ（mu）モデル**: 馬の基礎能力スコア（性能スコア）を推定する（順位・スコア系）。
  * **σ（sigma）モデル**: 馬固有の不確実性（成績分散）を推定する。
  * **ν（nu）モデル**: レース単位の荒れ度（レース全体の不確実性）を推定する。
  * **Task A（確率モデル）**: μ/σ/ν と Plackett-Luce などの確率生成プロセスを用いて、券種ごとの **的中確率 P**（例: 単勝確率）を推定する。
  * **Task B（理論オッズ予測） — *再定義***: **市場オッズ（確定オッズ）を模倣するのではなく、モデルが推定する「真の勝率 p（またはそれに由来する公正オッズ = 1/p）」を出力することを目的とする。**（以下に詳細）

> 重要：Task B の学習ターゲットを「確定オッズ（market）に近い値」に設定することは、本システムの「市場との乖離（ギャップ）を検出して期待値を見つける」という目的と **本質的に矛盾**します。なぜなら市場オッズ（特に締切直前オッズ）は確定オッズに近い値であり、モデルが「確定オッズ」を学習してしまうと、`EV_Ratio = JRA_5min_Odds / AI_Odds` はほぼ 1 になり、乖離を検出できなくなるからです（ご指摘の通り）。したがって Task B は \*\*市場オッズとは独立した「真の勝率／公正オッズの推定」\*\*に修正します。

-----

## 7.2 Task B の明確な定義（必須：置換）

### 7.2.1 目的（明文化）

**目的**: 各馬の「真の勝率 p（0〜1）」を、レース前情報 X のみを用いて推定する。Task B の最終出力は **`p_hat`（勝率）** とし、必要に応じて **公正オッズ `AI_FairOdds = 1 / p_hat`** を算出して外部に提供する。これが「AI理論オッズ」であり、バックテストや期待値計算では `EV_Ratio = JRA_5min_Odds / AI_FairOdds` を用いる。

**設計趣旨**: `p_hat` は市場参加者や流動性等から生じるスプレッドを反映しない「モデルが推定する真の確率」であり、市場オッズ（JRA\_5min\_Odds）と比較することで、**市場が過小評価／過大評価している馬を検出**することが可能になる。

### 7.2.2 学習ターゲット（y）の定義

学習時のターゲット y は **二値ラベル（勝ち:1, それ以外:0）ではなく、** 以下いずれかの方式を推奨する（推奨順）：

**推奨方式 A — 確率学習（直接学習）**

  * タスクを「個々の出走馬が勝つ確率 p」を直接学習する（分類／確率回帰）。
  * 学習ラベルとしては **過去レースの結果（winner=1/0）を用い、確率出力に対してロス関数は対数尤度（クロスエントロピー）を用いる**。
  * 実装: LightGBM の `binary` objective、あるいは NN の `sigmoid` + `binary_crossentropy`。キャリブレーション（Temperature Scaling / Isotonic）を学習後に適用する。
  * 学習では各レースの参加馬同士の依存性を扱うため、**ペアワイズやリストワイズの損失**、あるいは Plackett-Luce を用いたランキングベースの確率生成を組み合わせることを推奨する（尤度を最大化する設計）。

**推奨方式 B — 公正オッズ学習（間接学習）**

  * ターゲットを `fair_odds = 1 / p_true`（実際の p\_true は未知）として回帰モデルで学習する考え方。ただし p\_true を直接推定する A の方が理論的に明快であるため、B は補助的に用いるに留める。

**禁則（しないこと）**

  * 学習ターゲットに **確定市場オッズ（JRA 確定オッズまたは締切直前オッズ）をそのまま使うことは禁じる**。これを行うとモデルは市場を模倣することを学習し、乖離検出が構造的に不可能になる（本仕様書の核心的改訂点）。（元仕様との整合性確認のため、ここを明確にハッキリさせる。）

### 7.2.3 学習データの作り方（具体）

1.  **データ構成**: 各サンプルは「race\_id」「horse\_id」「features X (race+horse)」「label winner (0/1)」「race-level情報（head\_count, race\_conditions）」。
2.  **損失**: Binary cross-entropy を用い、レース内の相対関係を保つために `group`（race\_id）で学習する（LightGBM の group 機能や、NN でのカスタムロス）。
3.  **キャリブレーション**: 学習後、Validation セットに対して Temperature Scaling（あるいは Isotonic）を適用し、`p_hat` が確率として妥当となるよう調整する（ECE / Brier をチェック）。

### 7.2.4 出力の扱い（使用時）

  * モデル出力 `p_hat` を **AI\_FairOdds = 1 / p\_hat** に変換して保存する（注意：p\_hat が非常に小さい場合のクリッピングや上限/下限処理を行う）。
  * 実運用（Pipeline 5）とバックテスト（Pipeline 4）でのギャップ計算式は以下に統一する：

<!-- end list -->

```
EV_Ratio = JRA_5min_Odds / AI_FairOdds
         = JRA_5min_Odds * p_hat
```

（`EV_Ratio > 1` が市場に対して期待値がある状態、`EV_Ratio < 1` が期待値がない状態の指標となる。）

-----

## 7.3 Task A（確率モデル）との関係

  * Task A（券種確率推定）と Task B（真の勝率 p の推定）は基本的に **同じ情報源（μ/σ/ν と特徴量）** を使うが、**Task A は複合券の直接的確率や Plackett-Luce 経由で生成する確率に注力**し、**Task B は単勝（勝つ確率）という明確な確率ターゲットを得る**という役割分担を行う。二者の出力は整合させ、キャリブレーションやシミュレーションで相互利用する。

-----

## 7.4 学習パイプライン（Pipeline 3）— 実装上の明記

  * **Pipeline 3（学習）** では、**絶対に市場オッズ（JRA 5min 等）を学習ターゲットに含めない**。市場情報は特徴量として入力可能だが、ターゲットは**実際の勝敗（0/1）とレースのグループ構造から導かれる確率**とする。
  * **学習データの分割**: 時間的リークを防ぐため、訓練/検証/テストは「レース開催日」を基準に分割する。将来の情報（締切時にしか存在しないデータ）は学習時に含めない。
  * **評価**: 検証・評価時には `p_hat` に対して Brier スコア、ECE、NDCG、Top-k Accuracy を計測し、さらにバックテスト（Pipeline 4）で `EV_Ratio` を用いた投資シミュレーションの期待値検証を行う。

-----

## 7.5 推奨実装（技術的指針）

  * モデルタイプ: LightGBM（binary, group by race\_id）または NN（race-aware architecture）を推奨。Plackett-Luce を併用すると順位的整合性が強化される。
  * キャリブレーション: Temperature Scaling（推奨）を採用。ECE と Brier が監視指標。

-----

## 7.6 まとめ（Task B の変更点要約）

1.  **学習ターゲットを確定オッズにしない。**
2.  **Task B は「真の勝率 p（または p に由来する AI\_FairOdds）」を出力するよう再定義。**
3.  **期待値計算では `EV_Ratio = JRA_5min_Odds * p_hat` を用いる。**
4.  **バックテストに必要な過去の JRA 5 分前オッズは Pipeline 1/4 に必ず用意すること。**（未整備だとバックテストの意味が損なわれる。）

-----

-----

## 7.7 【実装追記】コア・モデルクラス定義

[cite_start]「修正内容.txt」 [cite: 562] [cite_start]で指摘された、仕様書内での欠落モジュール（`MuEstimator`, `SigmaEstimator`, `NuEstimator`）のクラス定義を以下に実装します。これらは仕様書全体（`predict.py` [cite: 498-500][cite_start], `train_sigma_nu_models.py` [cite: 377] [cite_start]等）と整合するように LightGBM [cite: 22, 386] をベースとしたラッパーとして設計されています。

### 7.7.A モデル学習スクリプトの実装要件（堅牢化）

モデル学習スクリプト（例: `train_mu_model.py`）は、`MuEstimator`等のコアモデルクラスを呼び出す前に、入力データの品質問題を吸収し、学習プロセスの安定性を確保するための前処理を**必ず**実行する。

- **目的**: 上流のデータ生成プロセス（特徴量生成など）で発生しうる、キーの不整合（見えない空白等）やデータの重複といった問題を、学習の直前で確実に解決する。
- **入力**:
  - 特徴量データ（例: `data/features/parquet/`）
  - レース結果データ（例: `data/parsed/parquet/races/`）
- **必須前処理フロー**:
  1. **キー正規化**: 特徴量データとレース結果データの両方で、結合キーとなるカラム（`race_id`, `horse_id`）のデータ型を文字列に統一し、`strip()`メソッド等で前後の空白を完全に除去する。
  2. **重複排除**: 正規化されたキーに基づき、特徴量データの重複行を削除する (`drop_duplicates`)。重複が存在する場合は警告ログを出力する。
  3. **データ結合**: 整備されたキーを元に、特徴量データとレース結果データを内部結合（`inner join`）し、単一の学習用データフレームを生成する。
  4. **欠損値処理**: 学習ターゲット（目的変数）が欠損している行を削除し、特徴量カラムに残る欠損値は特定の値（例: 0）で補完（`fillna`）する。

この防御的な前処理を実装することで、`MuEstimator`等のコアモデルは、常にクリーンで結合済みのデータを受け取ることが保証される。

### 7.7.1 μモデル (src/models/model_train.py)

```python
"""
src/models/model_train.py (MuEstimator)
μ（馬の基礎能力）を推定するモデルクラス
"""

import logging
from typing import Dict, List, Optional
from pathlib import Path
import pandas as pd
import numpy as np
import lightgbm as lgb
import joblib

class MuEstimator:
    """
    μ（馬の基礎能力スコア）推定モデル
    
    仕様書に基づき、LightGBMのRegressor（基礎スコア）とRanker（順位）を
    内部に持つアンサンブルモデルとして実装（ただし運用はRegressor主体でも可）。
    """
    
    def __init__(self, config: Dict):
        """
        Args:
            config: configs/models.yaml の 'mu_estimator' セクション
        """
        self.config = config
        self.regressor_params = config.get('regressor_params', {})
        self.ranker_params = config.get('ranker_params', {})
        
        self.model_regressor: Optional[lgb.LGBMRegressor] = None
        self.model_ranker: Optional[lgb.LGBMRanker] = None
        
        self.feature_names: List[str] = []

    def train(
        self,
        features_df: pd.DataFrame,
        target_regressor: str = 'finish_time_seconds', # 回帰ターゲット
        target_ranker: str = 'finish_position',     # ランクターゲット
        group_col: str = 'race_id'
    ):
        """
        μモデル（RegressorとRanker）を学習
        
        Args:
            features_df: 学習用特徴量DataFrame
            target_regressor: 回帰（スコア）の目的変数名
            target_ranker: ランキング（順位）の目的変数名
            group_col: レースID（グループ）のカラム名
        """
        logging.info("μモデルの学習開始...")
        
        # 特徴量とターゲットを分離
        if not self.feature_names:
            self.feature_names = [
                col for col in features_df.columns 
                if col not in [
                    target_regressor, target_ranker, group_col, 
                    'horse_id', 'horse_number'
                ]
            ]
        
        X = features_df[self.feature_names]
        
        # 1. Regressor (基礎スコア) の学習
        logging.info("Regressor (LGBMRegressor) を学習中...")
        y_reg = features_df[target_regressor]
        self.model_regressor = lgb.LGBMRegressor(**self.regressor_params)
        self.model_regressor.fit(X, y_reg)
        logging.info("Regressor の学習完了")
        
        # 2. Ranker (順位) の学習
        logging.info("Ranker (LGBMRanker) を学習中...")
        y_rank = features_df[target_ranker]
        
        # グループ（レースごと）のサンプル数を計算
        group_counts = features_df.groupby(group_col).size().values
        
        self.model_ranker = lgb.LGBMRanker(**self.ranker_params)
        self.model_ranker.fit(X, y_rank, group=group_counts)
        logging.info("Ranker の学習完了")

    def predict(
        self,
        features_df: pd.DataFrame,
        ensemble_weight_regressor: float = 0.5,
        ensemble_weight_ranker: float = 0.5
    ) -> np.ndarray:
        """
        μスコアを予測
        
        Args:
            features_df: 予測対象の特徴量DataFrame
            ensemble_weight_regressor: Regressorの予測値の重み
            ensemble_weight_ranker: Rankerの予測値の重み
        
        Returns:
            μスコアの配列
        """
        if self.model_regressor is None or self.model_ranker is None:
            raise RuntimeError("モデルが学習されていません。 train() または load_model() を呼び出してください。")
            
        X = features_df[self.feature_names]
        
        # 1. Regressor 予測
        # Regressor (例: タイム予測) は値が小さいほど良い
        pred_regressor = self.model_regressor.predict(X)
        # スコア化 (値が大きいほど良いように反転)
        score_regressor = -pred_regressor
        
        # 2. Ranker 予測
        # Ranker はスコアを直接出力 (値が大きいほど順位が良い)
        score_ranker = self.model_ranker.predict(X)
        
        # 3. アンサンブル (Z-score正規化後に加重平均)
        score_regressor_norm = (score_regressor - np.mean(score_regressor)) / (np.std(score_regressor) + 1e-6)
        score_ranker_norm = (score_ranker - np.mean(score_ranker)) / (np.std(score_ranker) + 1e-6)
        
        final_score = (
            score_regressor_norm * ensemble_weight_regressor +
            score_ranker_norm * ensemble_weight_ranker
        )
        
        return final_score

    def save_model(self, model_dir: str):
        """
        学習済みモデルと特徴量リストを保存
        
        Args:
            model_dir: 保存先ディレクトリ (例: data/models/mu_model)
        """
        output_path = Path(model_dir)
        output_path.mkdir(parents=True, exist_ok=True)
        
        # モデル保存
        joblib.dump(self.model_regressor, output_path / 'regressor.pkl')
        joblib.dump(self.model_ranker, output_path / 'ranker.pkl')
        
        # 特徴量リスト保存
        features_path = output_path / 'feature_names.json'
        with open(features_path, 'w', encoding='utf-8') as f:
            import json
            json.dump(self.feature_names, f, ensure_ascii=False, indent=2)
            
        logging.info(f"μモデルを {model_dir} に保存しました")

    def load_model(self, model_dir: str):
        """
        モデルと特徴量リストをロード
        
        Args:
            model_dir: ロード元ディレクトリ
        """
        model_path = Path(model_dir)
        
        self.model_regressor = joblib.load(model_path / 'regressor.pkl')
        self.model_ranker = joblib.load(model_path / 'ranker.pkl')
        
        features_path = model_path / 'feature_names.json'
        with open(features_path, 'r', encoding='utf-8') as f:
            import json
            self.feature_names = json.load(f)
            
        logging.info(f"μモデルを {model_dir} からロードしました")

```

### 7.7.2 σモデル (src/models/sigma\_estimator.py)

```python
"""
src/models/sigma_estimator.py (SigmaEstimator)
σ（馬固有の残差分散）を推定するモデルクラス
"""

import logging
from typing import Dict, List, Optional
from pathlib import Path
import pandas as pd
import numpy as np
import lightgbm as lgb
import joblib

class SigmaEstimator:
    """
    σ（馬固有の残差分散）推定モデル
    
    仕様書 13.4章 (train_sigma_nu_models.py) に基づき、
    LGBMRegressor を使用して残差の分散（または標準偏差）を予測する。
    """
    
    def __init__(self, config: Dict):
        """
        Args:
            config: configs/models.yaml の 'sigma_estimator' セクション
        """
        self.config = config
        self.params = config.get('params', {
            'objective': 'regression',
            'metric': 'rmse',
            'boosting_type': 'gbdt'
        })
        
        self.model: Optional[lgb.LGBMRegressor] = None
        self.feature_names: List[str] = []
        self.global_sigma: float = 1.0 # フォールバック用のグローバル平均

    def train(
        self,
        features_df: pd.DataFrame,
        target_sigma: str = 'sigma_target' # 目的変数 (例: squared_error の平均)
    ):
        """
        σモデルを学習
        
        Args:
            features_df: 学習用特徴量DataFrame (馬単位で集約済みを想定)
            target_sigma: 残差分散のターゲットカラム名
        """
        logging.info("σモデルの学習開始...")
        
        if not self.feature_names:
            self.feature_names = [
                col for col in features_df.columns 
                if col not in [target_sigma, 'horse_id']
            ]
        
        X = features_df[self.feature_names]
        y = features_df[target_sigma]
        
        # グローバル平均を計算（予測失敗時のフォールバック用）
        self.global_sigma = np.sqrt(y.mean()) # ターゲットが分散の場合、標準偏差を保存
        
        self.model = lgb.LGBMRegressor(**self.params)
        self.model.fit(X, y)
        logging.info("σモデルの学習完了")

    def predict(
        self,
        features_df: pd.DataFrame
    ) -> np.ndarray:
        """
        σ（残差の標準偏差）を予測
        
        Args:
            features_df: 予測対象の特徴量DataFrame (レース・馬単位)
        
        Returns:
            σ（標準偏差）の配列
        """
        if self.model is None:
            raise RuntimeError("モデルが学習されていません。 train() または load_model() を呼び出してください。")
            
        X = features_df[self.feature_names]
        
        # 予測 (予測ターゲットは分散 'sigma_target' を想定)
        predicted_variance = self.model.predict(X)
        
        # 負の分散をクリップし、標準偏差（σ）に変換
        predicted_sigma = np.sqrt(np.maximum(predicted_variance, 0.0))
        
        # 異常値をグローバル平均で置換
        predicted_sigma = np.nan_to_num(predicted_sigma, nan=self.global_sigma)
        predicted_sigma[predicted_sigma == 0] = self.global_sigma
        
        return predicted_sigma

    def save_model(self, model_dir: str):
        """
        学習済みモデルと特徴量リストを保存
        
        Args:
            model_dir: 保存先ディレクトリ (例: data/models/sigma_model)
        """
        output_path = Path(model_dir)
        output_path.mkdir(parents=True, exist_ok=True)
        
        joblib.dump(self.model, output_path / 'model.pkl')
        
        meta_data = {
            'feature_names': self.feature_names,
            'global_sigma': self.global_sigma
        }
        meta_path = output_path / 'metadata.json'
        with open(meta_path, 'w', encoding='utf-8') as f:
            import json
            json.dump(meta_data, f, ensure_ascii=False, indent=2)
            
        logging.info(f"σモデルを {model_dir} に保存しました")

    def load_model(self, model_dir: str):
        """
        モデルと特徴量リストをロード
        
        Args:
            model_dir: ロード元ディレクトリ
        """
        model_path = Path(model_dir)
        
        self.model = joblib.load(model_path / 'model.pkl')
        
        meta_path = model_path / 'metadata.json'
        with open(meta_path, 'r', encoding='utf-8') as f:
            import json
            meta_data = json.load(f)
            self.feature_names = meta_data['feature_names']
            self.global_sigma = meta_data['global_sigma']
            
        logging.info(f"σモデルを {model_dir} からロードしました")

```

### 7.7.3 νモデル (src/models/nu\_estimator.py)

```python
"""
src/models/nu_estimator.py (NuEstimator)
ν（レース荒れ度）を推定するモデルクラス
"""

import logging
from typing import Dict, List, Optional
from pathlib import Path
import pandas as pd
import numpy as np
import lightgbm as lgb
import joblib

class NuEstimator:
    """
    ν（レース全体の荒れ度）推定モデル
    
    仕様書 13.4章 (train_sigma_nu_models.py) に基づき、
    LGBMRegressor を使用してレース単位の分散（例: 着順の標準偏差）を予測する。
    """
    
    def __init__(self, config: Dict):
        """
        Args:
            config: configs/models.yaml の 'nu_estimator' セクション
        """
        self.config = config
        self.params = config.get('params', {
            'objective': 'regression',
            'metric': 'rmse',
            'boosting_type': 'gbdt'
        })
        
        self.model: Optional[lgb.LGBMRegressor] = None
        self.feature_names: List[str] = []
        self.global_nu: float = 1.0 # フォールバック用のグローバル平均

    def train(
        self,
        features_df: pd.DataFrame,
        target_nu: str = 'nu_target' # 目的変数 (例: finish_position の std)
    ):
        """
        νモデルを学習
        
        Args:
            features_df: 学習用特徴量DataFrame (レース単位で集約済みを想定)
            target_nu: レース荒れ度のターゲットカラム名
        """
        logging.info("νモデルの学習開始...")
        
        if not self.feature_names:
            self.feature_names = [
                col for col in features_df.columns 
                if col not in [target_nu, 'race_id']
            ]
        
        X = features_df[self.feature_names]
        y = features_df[target_nu]
        
        # グローバル平均を計算（予測失敗時のフォールバック用）
        self.global_nu = y.mean()
        
        self.model = lgb.LGBMRegressor(**self.params)
        self.model.fit(X, y)
        logging.info("νモデルの学習完了")

    def predict(
        self,
        features_df: pd.DataFrame
    ) -> np.ndarray:
        """
        ν（レース荒れ度）を予測
        
        Args:
            features_df: 予測対象の特徴量DataFrame (レース単位)
        
        Returns:
            ν（荒れ度スコア）の配列
        """
        if self.model is None:
            raise RuntimeError("モデルが学習されていません。 train() または load_model() を呼び出してください。")
            
        X = features_df[self.feature_names]
        
        predicted_nu = self.model.predict(X)
        
        # 異常値をグローバル平均で置換
        predicted_nu = np.nan_to_num(predicted_nu, nan=self.global_nu)
        
        # 負の値をクリップ
        return np.maximum(predicted_nu, 0.0)

    def save_model(self, model_path: str):
        """
        学習済みモデルと特徴量リストを保存
        
        Args:
            model_path: 保存先パス (例: data/models/nu_model.pkl)
        """
        output_path = Path(model_path)
        output_dir = output_path.parent
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # モデル本体を保存
        joblib.dump(self.model, output_path)
        
        # メタデータを同階層に保存
        meta_data = {
            'feature_names': self.feature_names,
            'global_nu': self.global_nu
        }
        meta_path = output_dir / f"{output_path.stem}_metadata.json"
        with open(meta_path, 'w', encoding='utf-8') as f:
            import json
            json.dump(meta_data, f, ensure_ascii=False, indent=2)
            
        logging.info(f"νモデルを {output_path} に保存しました")

    def load_model(self, model_path: str):
        """
        モデルと特徴量リストをロード
        
        Args:
            model_path: ロード元パス
        """
        model_file = Path(model_path)
        
        self.model = joblib.load(model_file)
        
        meta_path = model_file.parent / f"{model_file.stem}_metadata.json"
        try:
            with open(meta_path, 'r', encoding='utf-8') as f:
                import json
                meta_data = json.load(f)
                self.feature_names = meta_data['feature_names']
                self.global_nu = meta_data['global_nu']
        except FileNotFoundError:
            logging.warning(f"メタファイルが見つかりません: {meta_path}。グローバル値がデフォルトになります。")
            self.feature_names = self.model.feature_name_
            self.global_nu = 1.0
            
        logging.info(f"νモデルを {model_path} からロードしました")

```

## 8. シミュレーション（Monte Carlo と Plackett-Luce）

### 8.1 シミュレーション設計

#### 8.1.1 基本方針


目的: 複合券（馬連、3連単等）の的中確率を推定

手法: Plackett-Luce モデルによるランキング生成
- 各馬に性能スコア θ_i を割り当て
- θ_i ~ N(μ_i, σ_i^2 + ν_r^2)
- Plackett-Luce で順位を生成

K回試行: デフォルト K=1,000（軽量）
        必要に応じて K=5,000-10,000（夜間バッチ）


#### 8.1.2 Plackett-Luce モデル


順位生成の確率モデル:

P(順位 = [i1, i2, ..., in]) = ∏_{k=1}^{n} (θ_{i_k} / Σ_{j∈残り} θ_j)

実装:
1. 各馬の性能スコア θ_i をサンプリング
2. 残りの馬の中から、θに比例した確率で1頭ずつ選択
3. n頭すべてが選ばれるまで繰り返し


### 8.2 シミュレータ実装


"""
Monte Carlo シミュレーションモジュール
Numba最適化版
"""

import logging
from typing import Dict, List, Tuple
import numpy as np
import pandas as pd
from numba import jit, prange
import json


class RaceSimulator:
    """
    レースシミュレータ
    """
    
    def __init__(self, config: Dict):
        """
        Args:
            config: シミュレーション設定辞書
        """
        self.config = config
    
    def simulate_race(
        self,
        mu: np.ndarray,
        sigma: np.ndarray,
        nu: float,
        horse_numbers: np.ndarray,
        K: int = 1000,
        seed: int = 42
    ) -> Dict:
        """
        1レースのシミュレーション
        
        Args:
            mu: 各馬のμ値（性能スコア）
            sigma: 各馬のσ値（残差分散）
            nu: レース荒れ度
            horse_numbers: 馬番の配列
            K: シミュレーション回数
            seed: 乱数シード
        
        Returns:
            シミュレーション結果辞書
        """
        logging.info(f"シミュレーション開始: K={K}")
        
        n_horses = len(mu)
        
        # 乱数生成器
        rng = np.random.default_rng(seed)
        
        # Kレース分のシミュレーション
        rankings = simulate_plackett_luce_numba(
            mu=mu,
            sigma=sigma,
            nu=nu,
            n_horses=n_horses,
            K=K,
            seed=seed
        )
        
        # 結果を集計
        results = self._aggregate_results(
            rankings=rankings,
            horse_numbers=horse_numbers,
            K=K
        )
        
        logging.info("シミュレーション完了")
        
        return results
    
    def _aggregate_results(
        self,
        rankings: np.ndarray,
        horse_numbers: np.ndarray,
        K: int
    ) -> Dict:
        """
        シミュレーション結果を集計
        
        Args:
            rankings: ランキング配列 (K, n_horses)
            horse_numbers: 馬番配列
            K: シミュレーション回数
        
        Returns:
            集計結果辞書
        """
        n_horses = len(horse_numbers)
        
        # 1. 単勝確率（1着確率）
        win_probs = {}
        for i, horse_num in enumerate(horse_numbers):
            win_count = np.sum(rankings[:, 0] == i)
            win_probs[int(horse_num)] = float(win_count / K)
        
        # 2. 複勝確率（3着以内確率）
        place_probs = {}
        for i, horse_num in enumerate(horse_numbers):
            place_count = np.sum(np.any(rankings[:, :3] == i, axis=1))
            place_probs[int(horse_num)] = float(place_count / K)
        
        # 3. 馬連確率（上位2頭の組み合わせ）
        exacta_probs = {}
        for k in range(K):
            top2 = tuple(sorted([
                int(horse_numbers[rankings[k, 0]]),
                int(horse_numbers[rankings[k, 1]])
            ]))
            
            if top2 not in exacta_probs:
                exacta_probs[top2] = 0
            exacta_probs[top2] += 1
        
        # 確率に変換
        for key in exacta_probs:
            exacta_probs[key] = exacta_probs[key] / K
        
        # 4. 3連単確率（上位3頭の順序付き組み合わせ）
        trifecta_probs = {}
        for k in range(K):
            top3 = tuple([
                int(horse_numbers[rankings[k, 0]]),
                int(horse_numbers[rankings[k, 1]]),
                int(horse_numbers[rankings[k, 2]])
            ])
            
            if top3 not in trifecta_probs:
                trifecta_probs[top3] = 0
            trifecta_probs[top3] += 1
        
        for key in trifecta_probs:
            trifecta_probs[key] = trifecta_probs[key] / K
        
        # Top10のみ保持（メモリ節約）
        trifecta_probs_top = dict(
            sorted(trifecta_probs.items(), key=lambda x: x[1], reverse=True)[:10]
        )
        
        return {
            'win_probs': win_probs,
            'place_probs': place_probs,
            'exacta_probs': exacta_probs,
            'trifecta_probs': trifecta_probs_top,
            'rankings': rankings,  # 生の順位行列を追加
            'K': K
        }
    
    def save_simulation(
        self,
        race_id: str,
        model_id: str,
        simulation_results: Dict,
        output_dir: str = 'data/simulations'
    ):
        """
        シミュレーション結果を保存
        
        Args:
            race_id: レースID
            model_id: モデルID
            simulation_results: シミュレーション結果
            output_dir: 出力ディレクトリ
        """
        from pathlib import Path
        from datetime import datetime, timezone
        
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)
        
        # sim_id生成
        timestamp = datetime.now(timezone.utc).strftime('%Y%m%d_%H%M%S')
        sim_id = f"{timestamp}_{model_id}_{race_id}"
        
        # 保存データ
        save_data = {
            'sim_id': sim_id,
            'race_id': race_id,
            'model_id': model_id,
            'created_ts': datetime.now(timezone.utc).isoformat(),
            'K': simulation_results['K'],
            'win_probs': simulation_results['win_probs'],
            'place_probs': simulation_results['place_probs'],
            'exacta_probs': {
                str(k): v for k, v in simulation_results['exacta_probs'].items()
            },
            'trifecta_probs': {
                str(k): v for k, v in simulation_results['trifecta_probs'].items()
            }
        }
        
        # JSON保存
        output_file = output_path / f"{sim_id}.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(save_data, f, ensure_ascii=False, indent=2)
        
        logging.info(f"シミュレーション結果保存: {output_file}")


@jit(nopython=True, parallel=True)
def simulate_plackett_luce_numba(
    mu: np.ndarray,
    sigma: np.ndarray,
    nu: float,
    n_horses: int,
    K: int,
    seed: int
) -> np.ndarray:
    """
    Plackett-Luceモデルによるランキング生成（Numba最適化）
    
    Args:
        mu: 各馬のμ値
        sigma: 各馬のσ値
        nu: レース荒れ度
        n_horses: 馬数
        K: シミュレーション回数
        seed: 乱数シード
    
    Returns:
        ランキング配列 (K, n_horses)
        各行は馬のインデックス（0-indexed）の順位
    """
    np.random.seed(seed)
    
    rankings = np.zeros((K, n_horses), dtype=np.int32)
    
    for k in prange(K):
        # 各馬の性能スコアをサンプリング
        # θ_i ~ N(μ_i, σ_i^2 + ν^2)
        theta = np.zeros(n_horses)
        for i in range(n_horses):
            variance = sigma[i]**2 + nu**2
            theta[i] = np.random.normal(mu[i], np.sqrt(variance))
        
        # Plackett-Luceサンプリング
        remaining = np.arange(n_horses)
        
        for pos in range(n_horses):
            # 残りの馬の性能スコア
            remaining_theta = theta[remaining]
            
            # 指数変換（数値安定性のため正規化）
            exp_theta = np.exp(remaining_theta - np.max(remaining_theta))
            
            # 確率計算
            probs = exp_theta / np.sum(exp_theta)
            
            # サンプリング
            cumsum_probs = np.cumsum(probs)
            rand_val = np.random.random()
            
            selected_idx = 0
            for j in range(len(cumsum_probs)):
                if rand_val <= cumsum_probs[j]:
                    selected_idx = j
                    break
            
            # 選択された馬を記録
            rankings[k, pos] = remaining[selected_idx]
            
            # 残りから削除
            remaining = np.delete(remaining, selected_idx)
    
    return rankings


# 非Numba版（デバッグ用）
def simulate_plackett_luce_python(
    mu: np.ndarray,
    sigma: np.ndarray,
    nu: float,
    K: int,
    seed: int = 42
) -> np.ndarray:
    """
    Plackett-Luceモデル（Python版、デバッグ用）
    """
    rng = np.random.default_rng(seed)
    n_horses = len(mu)
    
    rankings = []
    
    for k in range(K):
        # 性能スコアサンプリング
        theta = rng.normal(
            loc=mu,
            scale=np.sqrt(sigma**2 + nu**2)
        )
        
        # Plackett-Luceサンプリング
        remaining = list(range(n_horses))
        ranking = []
        
        while remaining:
            remaining_theta = theta[remaining]
            
            # 確率計算
            exp_theta = np.exp(remaining_theta - np.max(remaining_theta))
            probs = exp_theta / np.sum(exp_theta)
            
            # サンプリング
            selected_idx = rng.choice(len(remaining), p=probs)
            selected_horse = remaining[selected_idx]
            
            ranking.append(selected_horse)
            remaining.pop(selected_idx)
        
        rankings.append(ranking)
    
    return np.array(rankings)


### 8.3 シミュレーション実行スクリプト


"""
シミュレーション実行スクリプト
"""

import logging
import argparse
from pathlib import Path
import pandas as pd
import numpy as np
import yaml

from src.models.model_train import MuEstimator
from src.models.sigma_estimator import SigmaEstimator
from src.models.nu_estimator import NuEstimator
from src.sim.simulator import RaceSimulator


def run_simulation_for_race(
    race_id: str,
    shutuba_df: pd.DataFrame,
    features_df: pd.DataFrame,
    mu_model: MuEstimator,
    sigma_estimates: pd.Series,
    nu_model: NuEstimator,
    config: dict
):
    """
    1レースのシミュレーションを実行
    
    Args:
        race_id: レースID
        shutuba_df: 出馬表DataFrame
        features_df: 特徴量DataFrame
        mu_model: μ推定モデル
        sigma_estimates: σ推定値
        nu_model: ν推定モデル
        config: 設定辞書
    """
    logging.info(f"レース {race_id} のシミュレーション開始")
    
    # 該当レースのデータを抽出
    race_data = features_df[features_df['race_id'] == race_id].copy()
    
    if len(race_data) == 0:
        logging.error(f"レース {race_id} のデータが見つかりません")
        return
    
    # μ予測
    mu_pred = mu_model.predict(race_data)
    
    # σ取得
    sigma_values = []
    for horse_id in race_data['horse_id']:
        sigma = sigma_estimates.get(horse_id, sigma_estimates.mean())
        sigma_values.append(sigma)
    
    sigma_array = np.array(sigma_values)
    
    # ν予測
    race_features = race_data.groupby('race_id').first().reset_index()
    nu_pred = nu_model.predict(race_features)[0]
    
    # 馬番
    horse_numbers = race_data['horse_number'].values
    
    # シミュレーション実行
    simulator = RaceSimulator(config)
    
    K = config.get('simulation', {}).get('K', 1000)
    seed = config.get('simulation', {}).get('seed', 42)
    
    sim_results = simulator.simulate_race(
        mu=mu_pred,
        sigma=sigma_array,
        nu=nu_pred,
        horse_numbers=horse_numbers,
        K=K,
        seed=seed
    )
    
    # 結果を保存
    model_id = config.get('model_id', 'model_default')
    simulator.save_simulation(
        race_id=race_id,
        model_id=model_id,
        simulation_results=sim_results
    )
    
    logging.info(f"レース {race_id} のシミュレーション完了")
    
    return sim_results


def main():
    """メイン実行関数"""
    parser = argparse.ArgumentParser(description='レースシミュレーション')
    parser.add_argument('--race_id', type=str, required=True, help='レースID')
    parser.add_argument('--config', type=str, default='configs/default.yaml')
    parser.add_argument('--K', type=int, default=1000, help='シミュレーション回数')
    
    args = parser.parse_args()
    
    # ロギング設定
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s [%(levelname)s] %(message)s'
    )
    
    # 設定ロード
    with open(args.config, 'r') as f:
        config = yaml.safe_load(f)
    
    config['simulation'] = {'K': args.K}
    
    # モデルロード
    logging.info("モデルロード中")
    
    mu_model = MuEstimator(config)
    mu_model.load_model('data/models/latest_mu')
    
    sigma_estimator = SigmaEstimator(config)
    sigma_estimates = pd.read_parquet('data/models/sigma_estimates.parquet')['sigma']
    
    nu_model = NuEstimator(config)
    nu_model.load_model('data/models/nu_model.pkl')
    
    # データロード
    logging.info("データロード中")
    
    features_df = pd.read_parquet('data/features/parquet/latest/')
    shutuba_df = pd.read_parquet('data/parsed/parquet/shutuba/')
    
    # シミュレーション実行
    run_simulation_for_race(
        race_id=args.race_id,
        shutuba_df=shutuba_df,
        features_df=features_df,
        mu_model=mu_model,
        sigma_estimates=sigma_estimates,
        nu_model=nu_model,
        config=config
    )


if __name__ == '__main__':
    main()


---

## 9. 確率キャリブレーション

### 9.1 キャリブレーション手法

#### 9.1.1 Temperature Scaling（推奨）


"""
Temperature Scalingによる確率キャリブレーション
"""

import logging
from typing import Dict, Tuple
import numpy as np
import pandas as pd
from scipy.optimize import minimize
from sklearn.metrics import log_loss


class TemperatureScaling:
    """
    Temperature Scalingクラス
    """
    
    def __init__(self):
        """
        初期化
        """
        self.temperature = 1.0
    
    def fit(
        self,
        logits: np.ndarray,
        labels: np.ndarray,
        initial_temp: float = 1.0
    ):
        """
        最適な温度パラメータを学習
        
        Args:
            logits: モデルのロジット（スコア）
            labels: 正解ラベル（0 or 1）
            initial_temp: 初期温度
        """
        logging.info("Temperature Scaling学習開始")
        
        def objective(temp):
            """負の対数尤度を最小化"""
            scaled_logits = logits / temp
            probs = self._softmax(scaled_logits)
            return log_loss(labels, probs)
        
        # 最適化
        result = minimize(
            objective,
            x0=[initial_temp],
            method='L-BFGS-B',
            bounds=[(0.1, 10.0)]
        )
        
        self.temperature = result.x[0]
        
        logging.info(f"Temperature Scaling学習完了: T={self.temperature:.4f}")
    
    def transform(self, logits: np.ndarray) -> np.ndarray:
        """
        温度スケーリングを適用
        
        Args:
            logits: モデルのロジット
        
        Returns:
            キャリブレーション済み確率
        """
        scaled_logits = logits / self.temperature
        probs = self._softmax(scaled_logits)
        return probs
    
    def _softmax(self, logits: np.ndarray) -> np.ndarray:
        """
        Softmax関数
        """
        exp_logits = np.exp(logits - np.max(logits, axis=-1, keepdims=True))
        return exp_logits / np.sum(exp_logits, axis=-1, keepdims=True)


class CalibrationEvaluator:
    """
    キャリブレーション評価クラス
    """
    
    def __init__(self):
        """初期化"""
        pass
    
    def calculate_brier_score(
        self,
        predictions: np.ndarray,
        outcomes: np.ndarray
    ) -> float:
        """
        Brier Scoreを計算
        
        Args:
            predictions: 予測確率
            outcomes: 実際の結果（0 or 1）
        
        Returns:
            Brier Score
        """
        return np.mean((predictions - outcomes) ** 2)
    
    def calculate_ece(
        self,
        predictions: np.ndarray,
        outcomes: np.ndarray,
        n_bins: int = 10
    ) -> float:
        """
        Expected Calibration Error (ECE)を計算
        
        Args:
            predictions: 予測確率
            outcomes: 実際の結果
            n_bins: ビン数
        
        Returns:
            ECE
        """
        bin_boundaries = np.linspace(0, 1, n_bins + 1)
        
        ece = 0.0
        
        for i in range(n_bins):
            bin_lower = bin_boundaries[i]
            bin_upper = bin_boundaries[i + 1]
            
            # ビン内のデータを抽出
            in_bin = (predictions >= bin_lower) & (predictions < bin_upper)
            
            if np.sum(in_bin) == 0:
                continue
            
            # ビン内の平均確率と実際の正解率
            avg_confidence = np.mean(predictions[in_bin])
            avg_accuracy = np.mean(outcomes[in_bin])
            
            # 重み付きで加算
            weight = np.sum(in_bin) / len(predictions)
            ece += weight * np.abs(avg_confidence - avg_accuracy)
        
        return ece
    
    def plot_reliability_diagram(
        self,
        predictions: np.ndarray,
        outcomes: np.ndarray,
        n_bins: int = 10,
        save_path: str = None
    ):
        """
        Reliability Diagramを描画
        
        Args:
            predictions: 予測確率
            outcomes: 実際の結果
            n_bins: ビン数
            save_path: 保存先パス
        """
        import matplotlib.pyplot as plt
        
        bin_boundaries = np.linspace(0, 1, n_bins + 1)
        
        bin_centers = []
        bin_accuracies = []
        bin_counts = []
        
        for i in range(n_bins):
            bin_lower = bin_boundaries[i]
            bin_upper = bin_boundaries[i + 1]
            
            in_bin = (predictions >= bin_lower) & (predictions < bin_upper)
            
            if np.sum(in_bin) == 0:
                continue
            
            bin_center = (bin_lower + bin_upper) / 2
            bin_accuracy = np.mean(outcomes[in_bin])
            bin_count = np.sum(in_bin)
            
            bin_centers.append(bin_center)
            bin_accuracies.append(bin_accuracy)
            bin_counts.append(bin_count)
        
        # プロット
        fig, ax = plt.subplots(figsize=(8, 8))
        
        # 完全キャリブレーション線
        ax.plot([0, 1], [0, 1], 'k--', label='Perfect Calibration')
        
        # 実際のキャリブレーション
        ax.bar(
            bin_centers,
            bin_accuracies,
            width=1/n_bins,
            alpha=0.7,
            edgecolor='black',
            label='Model Calibration'
        )
        
        ax.set_xlabel('Predicted Probability')
        ax.set_ylabel('Actual Frequency')
        ax.set_title('Reliability Diagram')
        ax.legend()
        ax.grid(alpha=0.3)
        
        if save_path:
            plt.savefig(save_path, dpi=150, bbox_inches='tight')
            logging.info(f"Reliability Diagram保存: {save_path}")
        
        plt.close()


### 9.2 キャリブレーション実行スクリプト


"""
確率キャリブレーション実行スクリプト
"""

import logging
import argparse
from pathlib import Path
import pandas as pd
import numpy as np
import joblib

from src.models.calibration import TemperatureScaling, CalibrationEvaluator


def run_calibration(
    predictions_df: pd.DataFrame,
    results_df: pd.DataFrame,
    output_dir: str = 'data/models/calibration'
):
    """
    キャリブレーションを実行
    
    Args:
        predictions_df: 予測結果DataFrame
        results_df: 実際の結果DataFrame
        output_dir: 出力ディレクトリ
    """
    logging.info("キャリブレーション開始")
    
    # データをマージ
    merged_df = predictions_df.merge(
        results_df[['race_id', 'horse_id', 'finish_position']],
        on=['race_id', 'horse_id'],
        how='inner'
    )
    
    # 単勝予測確率と実際の勝利
    win_predictions = merged_df['win_prob'].values
    win_outcomes = (merged_df['finish_position'] == 1).astype(int).values
    
    # キャリブレーション前の評価
    evaluator = CalibrationEvaluator()
    
    brier_before = evaluator.calculate_brier_score(win_predictions, win_outcomes)
    ece_before = evaluator.calculate_ece(win_predictions, win_outcomes)
    
    logging.info(f"キャリブレーション前 - Brier: {brier_before:.4f}, ECE: {ece_before:.4f}")
    
    # Temperature Scaling学習
    ts = TemperatureScaling()
    
    # ロジットに変換（確率→ロジット）
    epsilon = 1e-7
    logits = np.log(win_predictions + epsilon) - np.log(1 - win_predictions + epsilon)
    
    ts.fit(logits, win_outcomes)
    
    # キャリブレーション適用
    calibrated_probs = ts.transform(logits)
    
    # キャリブレーション後の評価
    brier_after = evaluator.calculate_brier_score(calibrated_probs, win_outcomes)
    ece_after = evaluator.calculate_ece(calibrated_probs, win_outcomes)
    
    logging.info(f"キャリブレーション後 - Brier: {brier_after:.4f}, ECE: {ece_after:.4f}")
    
    # Reliability Diagram作成
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    
    evaluator.plot_reliability_diagram(
        predictions=win_predictions,
        outcomes=win_outcomes,
        save_path=str(output_path / 'reliability_before.png')
    )
    
    evaluator.plot_reliability_diagram(
        predictions=calibrated_probs,
        outcomes=win_outcomes,
        save_path=str(output_path / 'reliability_after.png')
    )
    
    # モデル保存
    joblib.dump(ts, output_path / 'temperature_scaling.pkl')
    
    # メトリクス保存
    metrics = {
        'temperature': float(ts.temperature),
        'brier_before': float(brier_before),
        'brier_after': float(brier_after),
        'ece_before': float(ece_before),
        'ece_after': float(ece_after)
    }
    
    import json
    with open(output_path / 'calibration_metrics.json', 'w') as f:
        json.dump(metrics, f, indent=2)
    
    logging.info("キャリブレーション完了")
    
    return ts, metrics


def main():
    """メイン実行関数"""
    parser = argparse.ArgumentParser(description='確率キャリブレーション')
    parser.add_argument('--predictions', type=str, required=True, help='予測結果のパス')
    parser.add_argument('--results', type=str, required=True, help='実結果のパス')
    parser.add_argument('--output_dir', type=str, default='data/models/calibration')
    
    args = parser.parse_args()
    
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s [%(levelname)s] %(message)s'
    )
    
    # データロード
    predictions_df = pd.read_parquet(args.predictions)
    results_df = pd.read_parquet(args.results)
    
    # キャリブレーション実行
    run_calibration(
        predictions_df=predictions_df,
        results_df=results_df,
        output_dir=args.output_dir
    )


if __name__ == '__main__':
    main()


---

## 10. 日次ポートフォリオ最適化

### 10.1 最適化問題の数学的定義

#### 10.1.1 決定変数


x_j: 券種jへの投資額 (j = 1, 2, ..., M)

M: 投資候補券種の総数


#### 10.1.2 目的関数


Fractional Kelly基準:

maximize E[log(1 + R)]

where:
  R = Σ_j (x_j * payoff_j) / W_0 - 1
  W_0: 初期資金
  payoff_j: 券種jの払い戻し（的中時）


#### 10.1.3 制約条件


1. 予算制約:
   Σ_j x_j <= W_0 * c_max
   
   c_max: 最大投資比率（例: 0.1 = 資金の10%まで）

2. 非負制約:
   x_j >= 0  ∀j

3. 流動性制約:
   x_j <= L_j  ∀j
   
   L_j: 券種jの流動性上限（券種ごとに設定）

4. 集中度制約:
   x_j / Σ_j x_j <= c_concentration  ∀j
   
   c_concentration: 単一券種への最大集中度（例: 0.3）

5. Maximum Drawdown制約（オプション、近似）:
   MDD_simulated <= MDD_threshold


### 10.2 最適化実装


"""
ポートフォリオ最適化モジュール
scipy.optimizeベース
"""

import logging
from typing import Dict, List, Tuple
import numpy as np
import pandas as pd
from scipy.optimize import minimize, LinearConstraint, Bounds


class PortfolioOptimizer:
    """
    ポートフォリオ最適化クラス
    """
    
    def __init__(self, config: Dict):
        """
        Args:
            config: 最適化設定辞書
        """
        self.config = config
    
    def optimize(
        self,
        simulation_results: Dict,
        odds_data: Dict,
        W_0: float,
        c_max: float = 0.1,
        fraction: float = 0.5
    ) -> Dict:
        """
        ポートフォリオ最適化を実行
        
        Args:
            simulation_results: シミュレーション結果
            odds_data: オッズデータ
            W_0: 初期資金
            c_max: 最大投資比率
            fraction: Fractional Kelly の分数（0.5推奨）
        
        Returns:
            最適化結果辞書
        """
        logging.info("ポートフォリオ最適化開始")
        
        # 投資候補の作成
        candidates = self._create_candidates(simulation_results, odds_data)
        
        if len(candidates) == 0:
            logging.warning("投資候補が見つかりませんでした")
            return {'allocation': {}, 'expected_return': 0.0}
        
        # シミュレーションペイオフ行列の作成
        sim_payoffs = self._create_simulation_payoffs(
            candidates, simulation_results
        )
        
        # 最適化実行
        allocation = self._solve_optimization(
            candidates=candidates,
            sim_payoffs=sim_payoffs,
            W_0=W_0,
            c_max=c_max,
            fraction=fraction
        )
        
        # 結果を整形
        result = self._format_result(allocation, candidates, sim_payoffs, W_0)
        
        logging.info(f"ポートフォリオ最適化完了: {len(result['bets'])}件の投資")
        
        return result
    
    def _create_candidates(
        self,
        simulation_results: Dict,
        odds_data: Dict
    ) -> List[Dict]:
        """
        投資候補を作成
        
        Args:
            simulation_results: シミュレーション結果
            odds_data: オッズデータ
        
        Returns:
            候補リスト
        """
        candidates = []
        
        ev_threshold = self.config.get('ev_threshold', 1.05)
        prob_threshold = self.config.get('prob_threshold', 0.01)
        
        # 単勝候補
        for horse_num, prob in simulation_results['win_probs'].items():
            odds = odds_data.get('win', {}).get(horse_num)
            
            if odds is None or odds < 1.0:
                continue
            
            ev = prob * odds
            
            if ev >= ev_threshold and prob >= prob_threshold:
                candidates.append({
                    'type': 'win',
                    'selection': (horse_num,),
                    'prob': prob,
                    'odds': odds,
                    'ev': ev,
                    'liquidity': self._estimate_liquidity('win', odds)
                })
        
        # 複勝候補
        for horse_num, prob in simulation_results['place_probs'].items():
            odds = odds_data.get('place', {}).get(horse_num)
            
            if odds is None or odds < 1.0:
                continue
            
            ev = prob * odds
            
            if ev >= ev_threshold and prob >= prob_threshold:
                candidates.append({
                    'type': 'place',
                    'selection': (horse_num,),
                    'prob': prob,
                    'odds': odds,
                    'ev': ev,
                    'liquidity': self._estimate_liquidity('place', odds)
                })
        
        # 馬連候補（Top 20のみ）
        exacta_probs = simulation_results.get('exacta_probs', {})
        
        for selection, prob in sorted(
            exacta_probs.items(), key=lambda x: x[1], reverse=True
        )[:20]:
            odds = odds_data.get('exacta', {}).get(selection)
            
            if odds is None or odds < 1.0:
                continue
            
            ev = prob * odds
            
            if ev >= ev_threshold and prob >= prob_threshold:
                candidates.append({
                    'type': 'exacta',
                    'selection': selection,
                    'prob': prob,
                    'odds': odds,
                    'ev': ev,
                    'liquidity': self._estimate_liquidity('exacta', odds)
                })
        
        logging.info(f"{len(candidates)}個の投資候補を作成")
        
        return candidates
    
    def _estimate_liquidity(self, bet_type: str, odds: float) -> float:
        """
        流動性上限を推定
        
        Args:
            bet_type: 券種
            odds: オッズ
        
        Returns:
            流動性上限（円）
        """
        # 簡易版: オッズが低いほど流動性が高いと仮定
        base_liquidity = {
            'win': 100000,    # 10万円
            'place': 50000,   # 5万円
            'exacta': 30000,  # 3万円
            'trifecta': 10000 # 1万円
        }
        
        base = base_liquidity.get(bet_type, 10000)
        
        # オッズによる調整
        if odds < 5.0:
            return base * 2.0
        elif odds < 10.0:
            return base * 1.0
        else:
            return base * 0.5
    
    def _create_simulation_payoffs(
        self,
        candidates: List[Dict],
        simulation_results: Dict
    ) -> np.ndarray:
        """
        シミュレーションペイオフ行列を作成
        
        各行: シミュレーション試行
        各列: 投資候補
        要素: 的中時の払い戻し倍率（外れ時は0）
        
        [修正] 独立試行ではなく、シミュレーションされた順位行列に基づいて
               相関を考慮したペイオフを計算する。
        
        Args:
            candidates: 投資候補リスト
            simulation_results: シミュレーション結果 (生の 'rankings' を含む)
        
        Returns:
            ペイオフ行列 (K, M)
        """
        K = simulation_results['K']
        M = len(candidates)
        rankings = simulation_results['rankings'] # (K, n_horses) の順位行列
        
        # 馬番とインデックスの対応辞書を作成
        # candidates から馬番リストを取得し、それに基づいてマッピングを作成
        horse_numbers_in_race = sorted(list(set(np.array([c['selection'] for c in candidates if c['type'] in ['win', 'place']]).flatten())))
        horse_num_to_idx = {num: i for i, num in enumerate(horse_numbers_in_race)}

        payoffs = np.zeros((K, M))
        
        for j, candidate in enumerate(candidates):
            odds = candidate['odds']
            bet_type = candidate['type']
            selection = candidate['selection']
            
            for k in range(K):
                simulated_ranking_indices = rankings[k, :]
                
                # 実際の馬番に変換
                # この部分はシミュレータの出力（インデックスか馬番か）に依存
                # ここではインデックスと仮定
                
                is_hit = False
                if bet_type == 'win':
                    winner_idx = simulated_ranking_indices[0]
                    if horse_num_to_idx.get(selection[0]) == winner_idx:
                        is_hit = True
                
                elif bet_type == 'place':
                    top3_indices = simulated_ranking_indices[:3]
                    if horse_num_to_idx.get(selection[0]) in top3_indices:
                        is_hit = True

                elif bet_type == 'exacta':
                    # 馬連
                    top2_indices = set(simulated_ranking_indices[:2])
                    selection_indices = set([horse_num_to_idx.get(s) for s in selection])
                    if selection_indices.issubset(top2_indices):
                        is_hit = True
                
                if is_hit:
                    payoffs[k, j] = odds
        
        return payoffs
    
    def _solve_optimization(
        self,
        candidates: List[Dict],
        sim_payoffs: np.ndarray,
        W_0: float,
        c_max: float,
        fraction: float
    ) -> np.ndarray:
        """
        最適化問題を解く
        
        Args:
            candidates: 投資候補リスト
            sim_payoffs: ペイオフ行列
            W_0: 初期資金
            c_max: 最大投資比率
            fraction: Fractional Kelly
        
        Returns:
            最適投資額配分 (M,)
        """
        K, M = sim_payoffs.shape
        
        # 目的関数: 負の期待対数リターン
        def objective(x):
            """
            期待対数リターンの負値
            """
            # 各試行のリターン
            returns = np.dot(sim_payoffs, x) / W_0 - np.sum(x) / W_0
            
            # 対数リターン
            log_returns = np.log(1 + returns)
            
            # 期待値
            expected_log_return = np.mean(log_returns)
            
            # Fractional Kelly
            return -fraction * expected_log_return
        
        # 制約条件
        # 1. 予算制約: Σx_j <= W_0 * c_max
        budget_constraint = LinearConstraint(
            A=np.ones(M),
            lb=0,
            ub=W_0 * c_max
        )
        
        # 2. 流動性制約: x_j <= L_j
        liquidity_limits = np.array([c['liquidity'] for c in candidates])
        
        # 3. 非負制約
        bounds = Bounds(lb=0, ub=liquidity_limits)
        
        # 初期値（均等配分）
        x0 = np.ones(M) * (W_0 * c_max / M / 2)
        x0 = np.minimum(x0, liquidity_limits)
        
        # 最適化実行
        result = minimize(
            objective,
            x0=x0,
            method='SLSQP',
            bounds=bounds,
            constraints=[budget_constraint],
            options={'maxiter': 1000, 'ftol': 1e-6}
        )
        
        if not result.success:
            logging.warning(f"最適化が収束しませんでした: {result.message}")
        
        # 集中度制約の事後チェック
        allocation = result.x
        total_investment = np.sum(allocation)
        
        c_concentration = self.config.get('c_concentration', 0.3)
        
        if total_investment > 0:
            concentrations = allocation / total_investment
            
            if np.max(concentrations) > c_concentration:
                logging.warning(
                    f"集中度制約違反: {np.max(concentrations):.2%} > {c_concentration:.2%}"
                )
                
                # 再最適化（集中度制約を追加）
                # 簡易版: 超過分を他に再配分
                for j in np.where(concentrations > c_concentration)[0]:
                    excess = allocation[j] - c_concentration * total_investment
                    allocation[j] -= excess
                    
                    # 残りに均等配分
                    other_indices = np.where(concentrations <= c_concentration)[0]
                    if len(other_indices) > 0:
                        allocation[other_indices] += excess / len(other_indices)
        
        return allocation
    
    def _format_result(
        self,
        allocation: np.ndarray,
        candidates: List[Dict],
        sim_payoffs: np.ndarray,
        W_0: float
    ) -> Dict:
        """
        最適化結果を整形
        
        Args:
            allocation: 投資額配分
            candidates: 投資候補リスト
            sim_payoffs: ペイオフ行列
            W_0: 初期資金
        
        Returns:
            結果辞書
        """
        bets = []
        
        for j, amount in enumerate(allocation):
            if amount < 100:  # 100円未満は除外
                continue
            
            candidate = candidates[j]
            
            bets.append({
                'type': candidate['type'],
                'selection': candidate['selection'],
                'amount': float(amount),
                'odds': candidate['odds'],
                'prob': candidate['prob'],
                'ev': candidate['ev']
            })
        
        # 期待リターン計算
        total_investment = np.sum(allocation)
        
        if total_investment > 0:
            returns = np.dot(sim_payoffs, allocation) / W_0 - total_investment / W_0
            expected_return = np.mean(returns)
            std_return = np.std(returns)
            
            # Sharpe Ratio（簡易版）
            sharpe_ratio = expected_return / std_return if std_return > 0 else 0
        else:
            expected_return = 0.0
            std_return = 0.0
            sharpe_ratio = 0.0
        
        return {
            'bets': bets,
            'total_investment': float(total_investment),
            'expected_return': float(expected_return),
            'std_return': float(std_return),
            'sharpe_ratio': float(sharpe_ratio),
            'W_0': W_0
        }
    
    def save_allocation(
        self,
        race_id: str,
        allocation_result: Dict,
        output_dir: str = 'data/orders'
    ):
        """
        配分結果を保存
        
        Args:
            race_id: レースID
            allocation_result: 配分結果
            output_dir: 出力ディレクトリ
        """
        from pathlib import Path
        from datetime import datetime, timezone
        import json
        
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)
        
        # order_id生成
        timestamp = datetime.now(timezone.utc).strftime('%Y%m%d_%H%M%S')
        order_id = f"{timestamp}_{race_id}"
        
        # 保存データ
        order_data = {
            'order_id': order_id,
            'race_id': race_id,
            'created_ts': datetime.now(timezone.utc).isoformat(),
            'status': 'pending_manual',
            'bets': allocation_result['bets'],
            'total_investment': allocation_result['total_investment'],
            'expected_return': allocation_result['expected_return'],
            'std_return': allocation_result['std_return'],
            'sharpe_ratio': allocation_result['sharpe_ratio'],
            'W_0': allocation_result['W_0']
        }
        
        # JSON保存
        output_file = output_path / f"{order_id}.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(order_data, f, ensure_ascii=False, indent=2)
        
        logging.info(f"配分結果保存: {output_file}")


### 10.3 最適化実行スクリプト


"""
ポートフォリオ最適化実行スクリプト
"""

import logging
import argparse
from pathlib import Path
import json
import yaml

from src.optimizer.optimizer import PortfolioOptimizer


def run_optimization_for_race(
    race_id: str,
    simulation_file: str,
    odds_file: str,
    W_0: float,
    config: dict
):
    """
    1レースの最適化を実行
    
    Args:
        race_id: レースID
        simulation_file: シミュレーション結果ファイル
        odds_file: オッズファイル
        W_0: 初期資金
        config: 設定辞書
    """
    logging.info(f"レース {race_id} の最適化開始")
    
    # データロード
    with open(simulation_file, 'r') as f:
        simulation_results = json.load(f)
    
    with open(odds_file, 'r') as f:
        odds_data = json.load(f)
    
    # 最適化実行
    optimizer = PortfolioOptimizer(config)
    
    c_max = config.get('optimization', {}).get('c_max', 0.1)
    fraction = config.get('optimization', {}).get('fraction', 0.5)
    
    allocation_result = optimizer.optimize(
        simulation_results=simulation_results,
        odds_data=odds_data,
        W_0=W_0,
        c_max=c_max,
        fraction=fraction
    )
    
    # 結果を保存
    optimizer.save_allocation(
        race_id=race_id,
        allocation_result=allocation_result
    )
    
    # 結果を表示
    print(f"\n{'='*60}")
    print(f"レース {race_id} の最適化結果")
    print(f"{'='*60}")
    print(f"総投資額: ¥{allocation_result['total_investment']:,.0f}")
    print(f"期待リターン: {allocation_result['expected_return']:.2%}")
    print(f"標準偏差: {allocation_result['std_return']:.2%}")
    print(f"Sharpe Ratio: {allocation_result['sharpe_ratio']:.3f}")
    print(f"\n投資内訳:")
    
    for bet in allocation_result['bets']:
        selection_str = '-'.join(map(str, bet['selection']))
        print(f"  {bet['type']:8s} {selection_str:15s} ¥{bet['amount']:>8,.0f} "
              f"(オッズ:{bet['odds']:>6.1f}, EV:{bet['ev']:>5.2f})")
    
    print(f"{'='*60}\n")
    
    logging.info(f"レース {race_id} の最適化完了")


def main():
    """メイン実行関数"""
    parser = argparse.ArgumentParser(description='ポートフォリオ最適化')
    parser.add_argument('--race_id', type=str, required=True, help='レースID')
    parser.add_argument('--simulation', type=str, required=True, help='シミュレーション結果ファイル')
    parser.add_argument('--odds', type=str, required=True, help='オッズファイル')
    parser.add_argument('--W_0', type=float, default=10000, help='初期資金')
    parser.add_argument('--config', type=str, default='configs/optimization.yaml')
    
    args = parser.parse_args()
    
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s [%(levelname)s] %(message)s'
    )
    
    # 設定ロード
    with open(args.config, 'r') as f:
        config = yaml.safe_load(f)
    
    # 最適化実行
    run_optimization_for_race(
        race_id=args.race_id,
        simulation_file=args.simulation,
        odds_file=args.odds,
        W_0=args.W_0,
        config=config
    )


if __name__ == '__main__':
    main()


### 10.4 最適化設定（configs/optimization.yaml）

yaml
# ポートフォリオ最適化設定

optimization:
  # 基本パラメータ
  c_max: 0.1  # 最大投資比率（資金の10%）
  fraction: 0.5  # Fractional Kelly（0.5 = Half Kelly）
  c_concentration: 0.3  # 単一券種への最大集中度
  
  # 投資候補フィルタ
  ev_threshold: 1.05  # 期待値の下限（105%以上）
  prob_threshold: 0.01  # 確率の下限（1%以上）
  
  # 流動性設定
  liquidity:
    win: 100000    # 単勝の流動性上限（10万円）
    place: 50000   # 複勝の流動性上限（5万円）
    exacta: 30000  # 馬連の流動性上限（3万円）
    trifecta: 10000  # 3連単の流動性上限（1万円）
  
  # 最適化ソルバー設定
  solver:
    method: "SLSQP"
    maxiter: 1000
    ftol: 1.0e-6
  
  # リスク管理
  risk:
    max_drawdown_threshold: 0.2  # 最大ドローダウン許容値（20%）
    var_confidence: 0.95  # VaR信頼区間

# 11. バックテスト設計

## 11.1 目的と前提

バックテストの目的は、**実運用で使うタイミング（締切直前）における市場オッズ**と **AI が推定する公正オッズ（AI_FairOdds）** の比較に基づいて、システムが現実に期待値を生むかを検証することです。したがって、**バックテストには過去の締切直前オッズ（可能なら「締切の約5分前」スナップショット）が必須**です。これが無ければ `EV_Ratio` および投資シミュレーションは現実的な検証になりません。

---

## 11.2 入力データ要件（必須）

**必須入力**:

1. `features_df`：モデル入力特徴量（過去にリークが無い形で生成されたもの）。
2. `results_df`：実際のレース結果（着順等）。
3. **`odds_df`（バックテスト専用）**：過去の JRA オッズスナップショット dataframe。各行は `race_id, snapshot_time, market_type, selection, odds, source` を含む（`snapshot_time` が締切 5 分前相当であることが望ましい）。

**注意**: `odds_df` に **締切 5 分前の snapshot が含まれていない場合はバックテストの結果解釈が変わる**。その際は欠損統計を出力し、代替ルールを適用（下述）する。

---

## 11.3 欠損時の代替戦略（仕様）

1. **最優先**: 公式アーカイブ／JRA 提供データからの直接取得（推奨）。
2. **次善**: 近傍の snapshot（例: 締切 1 分前、10 分前）から 5 分前に最も近い記録を採用する。採用した差分（実際の snapshot_time と「5 分前」との差）を `odds_df.source='estimated_from_nearest'` としてメタデータに残す。
3. **最終手段**: `odds_df` が手当てできないレースはバックテスト対象外とするか、除外率を明示して解析結果に補正を入れる（推奨：除外してレポート）。

**必須レポート**: バックテストごとに「処理対象レース数」「欠損ルールで代替したレース数」「完全欠落で除外したレース数」を出力し、バックテストの有効性を監査可能にする。

---

## 11.4 EV 計算とギャップ指標（統一）

バックテストでは、AI の出力 `p_hat`（Task B の結果）を使い、AI 公正オッズ `AI_FairOdds = 1 / p_hat` を算出し、以下を用いる。

* **EV（単券 j）**： `EV_j = p_hat_j * JRA_5min_Odds_j - 1`（既存算出式のまま）
  （注：既存の EV 式が `P_j × Odds_j - 1` であったため、`P_j` はモデルの `p_hat`、`Odds_j` は `JRA_5min_Odds` に統一）

* **EV_Ratio（ギャップ指標）**： `EV_Ratio = JRA_5min_Odds / AI_FairOdds = JRA_5min_Odds * p_hat`

  * `EV_Ratio > 1`：市場が割安（＝期待値ありの候補）
  * `EV_Ratio < 1`：市場が割高（＝期待値なし）
  * **この指標を基準に投資候補のフィルタリング（ev_threshold 等）を行う**。

---

## 11.5 バックテスト実行フロー（実装）

1. **前処理**: `features_df` と `odds_df`、`results_df` を日付でフィルタ（start_date, end_date）。`odds_df` は `snapshot_time` が締切 5 分前相当のレコードを優先的に選ぶ。
2. **予測呼出し**: 各レースについて `p_hat` を生成するために `model.predict(race_features)` を呼ぶ。モデルは **締切前の利用可能な情報のみ** を使用していることを再確認する（リーク禁止）。
3. **オッズマッチ**: `odds_df` の `race_id` をキーにして `JRA_5min_Odds` をマージする。券種（単勝/複勝等）ごとにマージを行う。
4. **EV / 投資候補生成**: `EV_j` と `EV_Ratio` を計算し、設定した閾値（例: `EV_Ratio >= 1.05` や `EV >= 1.05`、`prob_threshold`）で候補を抽出する。
5. **最適化シミュレーション**: 候補に対してシミュレーション（Monte Carlo）または最適化（Fractional Kelly）を実行する（既存の optimizer モジュールを利用）。
6. **結果の集計**: ROI、Sharpe、MDD、収益分布、キャリブレーション指標（Brier/ECE）を算出。欠損や代替処理の影響も同時にレポートする。

---

## 11.6 バックテスト用データの運用チェックリスト（必須）

* [ ] `data/raw/json/jra_odds/` に対象期間の snapshot ファイルが存在する。
* [ ] `snapshot_time` が記録されていること。
* [ ] `odds_df` の `source` に `official` または `estimated_from_nearest` のいずれかが入っている。
* [ ] 代替処理が行われた場合はそのカウントと影響をレポートしている。

---

## 11.7 実装例（参考：既存 Backtester の改修ポイント）

既存の `Backtester.run_backtest` の主な改修点：

* `odds_df` の読み込み時に `snapshot_time` をチェックし、利用可能な snapshot のうち **締切に最も近い 1 レコード** を `JRA_5min_Odds` として選択するロジックを追加する。
* `model.predict()` 呼出しは `race_features` のみを引数とし、**モデルが市場オッズを参照していないことを検証するアサーション**を追加する（デバッグ・監査用途）。
* バックテスト完了後に `missing_odds_summary` を出力する。



---

## 12. モニタリングとダッシュボード

### 12.1 Streamlitダッシュボード


"""
Streamlitダッシュボード
ローカル実行版
"""

import streamlit as st
import pandas as pd
import numpy as np
import plotly.graph_objects as go
import plotly.express as px
from pathlib import Path
from datetime import datetime, timedelta
import json


def load_data():
    """
    データをロード
    """
    # メトリクスデータ
    metrics_path = Path('data/metrics/daily_metrics.parquet')
    
    if metrics_path.exists():
        metrics_df = pd.read_parquet(metrics_path)
    else:
        metrics_df = pd.DataFrame()
    
    # バックテスト結果
    backtest_path = Path('data/backtest/results.json')
    
    if backtest_path.exists():
        with open(backtest_path, 'r') as f:
            backtest_results = json.load(f)
    else:
        backtest_results = {}
    
    return metrics_df, backtest_results


def render_header():
    """
    ヘッダーをレンダリング
    """
    st.title('🏇 Keiba AI Dashboard')
    st.markdown('---')
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        st.metric(
            label="システム状態",
            value="🟢 稼働中",
            delta=None
        )
    
    with col2:
        last_update = datetime.now().strftime('%Y-%m-%d %H:%M')
        st.metric(
            label="最終更新",
            value=last_update,
            delta=None
        )
    
    with col3:
        st.metric(
            label="データ期間",
            value="2020-01 ~ 2023-12",
            delta=None
        )


def render_performance_overview(backtest_results: Dict):
    """
    パフォーマンス概要をレンダリング
    """
    st.header('📊 パフォーマンス概要')
    
    if not backtest_results:
        st.warning('バックテスト結果がありません')
        return
    
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric(
            label="総リターン",
            value=f"{backtest_results.get('total_return', 0):.2%}",
            delta=None
        )
    
    with col2:
        st.metric(
            label="ROI",
            value=f"{backtest_results.get('roi', 0):.2%}",
            delta=None
        )
    
    with col3:
        st.metric(
            label="Sharpe Ratio",
            value=f"{backtest_results.get('sharpe_ratio_annual', 0):.3f}",
            delta=None
        )
    
    with col4:
        st.metric(
            label="Max Drawdown",
            value=f"{backtest_results.get('max_drawdown', 0):.2%}",
            delta=None,
            delta_color="inverse"
        )
    
    col5, col6, col7, col8 = st.columns(4)
    
    with col5:
        st.metric(
            label="勝率",
            value=f"{backtest_results.get('win_rate', 0):.2%}",
            delta=None
        )
    
    with col6:
        st.metric(
            label="総レース数",
            value=f"{backtest_results.get('num_races', 0):,}",
            delta=None
        )
    
    with col7:
        st.metric(
            label="総投資額",
            value=f"¥{backtest_results.get('total_investment', 0):,.0f}",
            delta=None
        )
    
    with col8:
        st.metric(
            label="総利益",
            value=f"¥{backtest_results.get('total_profit', 0):,.0f}",
            delta=None
        )


def render_equity_curve(metrics_df: pd.DataFrame):
    """
    資金曲線をレンダリング
    """
    st.header('📈 資金曲線')
    
    if len(metrics_df) == 0:
        st.warning('メトリクスデータがありません')
        return
    
    # ダミーデータ（実際にはmetrics_dfから取得）
    dates = pd.date_range('2023-01-01', '2023-12-31', freq='D')
    capital = 100000 * np.exp(np.cumsum(np.random.randn(len(dates)) * 0.01))
    
    fig = go.Figure()
    
    fig.add_trace(go.Scatter(
        x=dates,
        y=capital,
        mode='lines',
        name='資金',
        line=dict(color='#1f77b4', width=2)
    ))
    
    fig.update_layout(
        xaxis_title='日付',
        yaxis_title='資金 (¥)',
        hovermode='x unified',
        height=400
    )
    
    st.plotly_chart(fig, use_container_width=True)


def render_calibration_metrics(metrics_df: pd.DataFrame):
    """
    キャリブレーションメトリクスをレンダリング
    """
    st.header('🎯 キャリブレーション指標')
    
    col1, col2 = st.columns(2)
    
    with col1:
        # Brier Score推移
        st.subheader('Brier Score（7日移動平均）')
        
        # ダミーデータ
        dates = pd.date_range('2023-01-01', '2023-12-31', freq='D')
        brier_scores = 0.15 + np.random.randn(len(dates)) * 0.02
        brier_scores = pd.Series(brier_scores).rolling(7).mean()
        
        fig = go.Figure()
        fig.add_trace(go.Scatter(
            x=dates,
            y=brier_scores,
            mode='lines',
            name='Brier Score',
            line=dict(color='green')
        ))
        
        # 閾値ライン
        fig.add_hline(
            y=0.20,
            line_dash="dash",
            line_color="red",
            annotation_text="閾値"
        )
        
        fig.update_layout(
            yaxis_title='Brier Score',
            height=300
        )
        
        st.plotly_chart(fig, use_container_width=True)
    
    with col2:
        # ECE推移
        st.subheader('ECE（7日移動平均）')
        
        # ダミーデータ
        ece_scores = 0.05 + np.random.randn(len(dates)) * 0.01
        ece_scores = pd.Series(ece_scores).rolling(7).mean()
        
        fig = go.Figure()
        fig.add_trace(go.Scatter(
            x=dates,
            y=ece_scores,
            mode='lines',
            name='ECE',
            line=dict(color='purple')
        ))
        
        fig.add_hline(
            y=0.08,
            line_dash="dash",
            line_color="red",
            annotation_text="閾値"
        )
        
        fig.update_layout(
            yaxis_title='ECE',
            height=300
        )
        
        st.plotly_chart(fig, use_container_width=True)


def render_feature_importance():
    """
    特徴量重要度をレンダリング
    """
    st.header('🔍 特徴量重要度（Top 20）')
    
    # ダミーデータ
    features = [
        'past_3_finish_position_mean',
        'jockey_win_rate',
        'horse_weight',
        'adjusted_speed',
        'sire_target_encoded',
        'distance_m',
        'basis_weight',
        'past_5_last_3f_time_mean',
        'trainer_win_rate',
        'age',
        'days_since_last_race',
        'horse_weight_change',
        'form_trend_slope',
        'odds_mean',
        'popularity_std',
        'bracket_is_inner',
        'track_surface_芝',
        'track_condition_良',
        'jockey_trainer_win_rate',
        'past_1_finish_position_mean'
    ]
    
    importance = np.random.rand(20) * 1000
    importance = np.sort(importance)[::-1]
    
    fig = go.Figure()
    fig.add_trace(go.Bar(
        y=features,
        x=importance,
        orientation='h',
        marker=dict(
            color=importance,
            colorscale='Viridis'
        )
    ))
    
    fig.update_layout(
        xaxis_title='重要度',
        height=600
    )
    
    st.plotly_chart(fig, use_container_width=True)


def render_recent_orders():
    """
    最近の発注履歴をレンダリング
    """
    st.header('📋 最近の発注履歴')
    
    # ダミーデータ
    orders_data = {
        '日時': pd.date_range('2023-12-01', periods=10, freq='D'),
        'レースID': [f'202312{i:02d}0501' for i in range(1, 11)],
        '券種': np.random.choice(['単勝', '複勝', '馬連'], 10),
        '投資額': np.random.randint(1000, 10000, 10),
        '的中': np.random.choice(['○', '×'], 10, p=[0.3, 0.7]),
        '払戻額': [np.random.randint(0, 30000) if x == '○' else 0 for x in np.random.choice(['○', '×'], 10, p=[0.3, 0.7])],
        '収支': lambda df: df['払戻額'] - df['投資額']
    }
    
    orders_df = pd.DataFrame(orders_data)
    orders_df['収支'] = orders_df['払戻額'] - orders_df['投資額']
    
    # スタイリング
    def color_profit(val):
        color = 'green' if val > 0 else 'red' if val < 0 else 'gray'
        return f'color: {color}'
    
    styled_df = orders_df.style.applymap(
        color_profit,
        subset=['収支']
    )
    
    st.dataframe(styled_df, use_container_width=True)
    
    # 集計
    col1, col2, col3 = st.columns(3)
    
    with col1:
        st.metric(
            label="総投資額",
            value=f"¥{orders_df['投資額'].sum():,}",
            delta=None
        )
    
    with col2:
        st.metric(
            label="総払戻額",
            value=f"¥{orders_df['払戻額'].sum():,}",
            delta=None
        )
    
    with col3:
        profit_sum = orders_df['収支'].sum()
        st.metric(
            label="純利益",
            value=f"¥{profit_sum:,}",
            delta=None
        )


def render_system_status():
    """
    システムステータスをレンダリング
    """
    st.header('⚙️ システムステータス')
    
    status_data = {
        'コンポーネント': [
            'スクレイピング',
            'データパース',
            'モデル推論',
            'シミュレーション',
            '最適化',
            'キャリブレーション'
        ],
        'ステータス': ['✅ 正常'] * 5 + ['⚠️ 警告'],
        '最終実行': [
            '2023-12-31 03:00',
            '2023-12-31 04:30',
            '2023-12-31 10:00',
            '2023-12-31 10:15',
            '2023-12-31 10:30',
            '2023-12-20 00:00'
        ],
        '実行時間': ['15分', '30分', '5分', '10分', '2分', '5分']
    }
    
    status_df = pd.DataFrame(status_data)
    
    st.dataframe(status_df, use_container_width=True)
    
    # アラート
    st.subheader('🚨 アラート')
    
    st.warning('⚠️ キャリブレーションが10日以上実行されていません。再実行を推奨します。')


def main():
    """
    メイン関数
    """
    st.set_page_config(
        page_title='Keiba AI Dashboard',
        page_icon='🏇',
        layout='wide',
        initial_sidebar_state='expanded'
    )
    
    # サイドバー
    st.sidebar.title('📊 ナビゲーション')
    
    page = st.sidebar.radio(
        '表示ページ',
        [
            'パフォーマンス概要',
            '資金曲線',
            'キャリブレーション',
            '特徴量重要度',
            '発注履歴',
            'システムステータス'
        ]
    )
    
    st.sidebar.markdown('---')
    st.sidebar.info(
        '💡 このダッシュボードはローカル環境で動作します。\n\n'
        'データは `data/metrics/` から自動的にロードされます。'
    )
    
    # データロード
    metrics_df, backtest_results = load_data()
    
    # ヘッダー
    render_header()
    
    # ページレンダリング
    if page == 'パフォーマンス概要':
        render_performance_overview(backtest_results)
    
    elif page == '資金曲線':
        render_equity_curve(metrics_df)
    
    elif page == 'キャリブレーション':
        render_calibration_metrics(metrics_df)
    
    elif page == '特徴量重要度':
        render_feature_importance()
    
    elif page == '発注履歴':
        render_recent_orders()
    
    elif page == 'システムステータス':
        render_system_status()


if __name__ == '__main__':
    main()


### 12.2 メトリクス収集モジュール


"""
メトリクス収集モジュール
"""

import logging
from typing import Dict
from datetime import datetime, timezone
from pathlib import Path
import pandas as pd
import sqlite3


class MetricsCollector:
    """
    メトリクス収集クラス
    """
    
    def __init__(self, db_path: str = 'data/metadata/db.sqlite3'):
        """
        Args:
            db_path: SQLiteデータベースパス
        """
        self.db_path = db_path
        self._ensure_metrics_table()
    
    def _ensure_metrics_table(self):
        """
        メトリクステーブルを作成
        """
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS metrics (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                metric_date TEXT NOT NULL,
                metric_name TEXT NOT NULL,
                metric_value REAL,
                metric_details TEXT,
                created_ts TEXT NOT NULL,
                UNIQUE(metric_date, metric_name)
            )
        ''')
        
        conn.commit()
        conn.close()
    
    def record_metric(
        self,
        metric_name: str,
        metric_value: float,
        metric_date: str = None,
        metric_details: Dict = None
    ):
        """
        メトリクスを記録
        
        Args:
            metric_name: メトリクス名
            metric_value: メトリクス値
            metric_date: メトリクス日付（YYYY-MM-DD）
            metric_details: 詳細情報（JSON）
        """
        if metric_date is None:
            metric_date = datetime.now(timezone.utc).strftime('%Y-%m-%d')
        
        created_ts = datetime.now(timezone.utc).isoformat()
        
        import json
        metric_details_json = json.dumps(metric_details) if metric_details else None
        
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute('''
            INSERT OR REPLACE INTO metrics (
                metric_date, metric_name, metric_value, 
                metric_details, created_ts
            ) VALUES (?, ?, ?, ?, ?)
        ''', (
            metric_date, metric_name, metric_value,
            metric_details_json, created_ts
        ))
        
        conn.commit()
        conn.close()
        
        logging.info(f"メトリクス記録: {metric_name} = {metric_value} ({metric_date})")
    
    def get_metrics(
        self,
        start_date: str = None,
        end_date: str = None,
        metric_names: list = None
    ) -> pd.DataFrame:
        """
        メトリクスを取得
        
        Args:
            start_date: 開始日（YYYY-MM-DD）
            end_date: 終了日（YYYY-MM-DD）
            metric_names: メトリクス名リスト
        
        Returns:
            メトリクスDataFrame
        """
        conn = sqlite3.connect(self.db_path)
        
        query = 'SELECT * FROM metrics WHERE 1=1'
        params = []
        
        if start_date:
            query += ' AND metric_date >= ?'
            params.append(start_date)
        
        if end_date:
            query += ' AND metric_date <= ?'
            params.append(end_date)
        
        if metric_names:
            placeholders = ','.join(['?'] * len(metric_names))
            query += f' AND metric_name IN ({placeholders})'
            params.extend(metric_names)
        
        query += ' ORDER BY metric_date, metric_name'
        
        df = pd.read_sql_query(query, conn, params=params)
        
        conn.close()
        
        return df
    
    def export_to_parquet(
        self,
        output_path: str = 'data/metrics/daily_metrics.parquet'
    ):
        """
        メトリクスをParquetにエクスポート
        
        Args:
            output_path: 出力パス
        """
        df = self.get_metrics()
        
        if len(df) == 0:
            logging.warning('エクスポートするメトリクスがありません')
            return
        
        output_file = Path(output_path)
        output_file.parent.mkdir(parents=True, exist_ok=True)
        
        df.to_parquet(output_file, engine='pyarrow', compression='snappy')
        
        logging.info(f"メトリクスエクスポート完了: {output_file} ({len(df)}行)")

## 13. 実務オペレーション（改訂版）

### 13.0 概要（改訂方針）

実運用フローは「深夜バッチ（静的データ収集）」→「当日午前（出馬表確定 & 締切直前オッズ取得）」→「シミュレーション/最適化/発注（手動またはペーパー）」という二段階（実質的には三段）の流れで厳密に運用するものとする。これにより、出馬表未取得によるオッズ反映ミス・推論欠落を防止します。

---

### 13.1 日次運用フロー（完全版スクリプト）

以下は `scripts/daily_operation.sh` の**完全版（置換用）**です。JRAオッズ取得は当日午前に実行するため、日次フローは「深夜 → 当日午前」の二段階で構成されます。`run_scrape_jra_odds_daily.py` は当日午前に呼び出されることを明確にしています（このスクリプトは第4章に追加した完全実装を参照）。

```bash
#!/bin/bash
# scripts/daily_operation.sh
# 日次運用スクリプト（完全版）
# 実行想定: 深夜〜午前のスケジュールで順次実行
# 注意: JRAオッズ取得は午前に実行（Seleniumが動作する環境を事前確認）

set -euo pipefail

# 当日/対象日
TARGET_DATE=$(date +%Y-%m-%d)
YESTERDAY_DATE=$(date -d '1 day ago' +%Y-%m-%d)

echo "=========================================="
echo "Keiba AI 日次運用開始"
echo "対象日: ${TARGET_DATE}"
echo "=========================================="

# ---- 深夜バッチ: 静的データ取得（03:00 実行想定） ----
echo "[1/11] データ取得中 (過去データ / 静的データ)..."
python src/run_scraping_pipeline_local.py \
    --from_date ${YESTERDAY_DATE} \
    --to_date ${TARGET_DATE} \
    --skip_race_html

echo "[2/11] データパース中..."
python src/parsers/parse_all.py \
    --date ${TARGET_DATE}

echo "[3/11] 特徴量生成中..."
python src/features/generate_features.py \
    --date ${TARGET_DATE}

echo "[4/11] モデル推論 (μ の事前推論のみ: σ/ν は後続学習/更新で利用)..."
# μ の推論は出馬表が確定している場合はここで行うが、
# 出馬表未確定のケースに備え、当日午前に再実行するフローを残す
python src/models/predict.py \
    --date ${TARGET_DATE} \
    --model_dir data/models/latest \
    --predict_mu_only

# ---- 当日午前: 出馬表確定確認 & JRAオッズ取得 ----
# 出馬表が確定しているか確認。未確定であれば当日午前に再度チェックする運用を行う。
echo "[5/11] 当日出馬表取得/確認..."
python src/scripts/fetch_today_shutuba_if_missing.py \
    --date ${TARGET_DATE}

echo "[6/11] JRAオッズ取得中 (締切直前オッズ - Selenium / 一括処理)..."
# run_scrape_jra_odds_daily.py は第4章に完全実装を追加済み
python src/scripts/run_scrape_jra_odds_daily.py \
    --date ${TARGET_DATE}

echo "[7/11] オッズデータのパース（JSON -> Parquet 等）..."
python src/parsers/parse_all_odds.py \
    --date ${TARGET_DATE}

# ---- オッズ反映後の最終推論・シミュレーション・最適化 ----
echo "[8/11] モデル推論（オッズ反映版: μ/σ/ν を用いて最終推論）..."
python src/models/predict.py \
    --date ${TARGET_DATE} \
    --model_dir data/models/latest \
    --use_odds

echo "[9/11] シミュレーション（オッズ反映）実行..."
python src/sim/simulate_daily_races.py \
    --date ${TARGET_DATE} \
    --K 1000 \
    --model_id latest

echo "[10/11] ポートフォリオ最適化中..."
python src/optimizer/optimize_daily_races.py \
    --date ${TARGET_DATE} \
    --W_0 10000

echo "[11/11] メトリクス更新（Brier, ECE, ROI 等）..."
python src/monitoring/update_metrics.py \
    --date ${TARGET_DATE}

echo "=========================================="
echo "日次運用完了"
echo "$(date '+%Y-%m-%d %H:%M:%S')"
echo "=========================================="
```

**重要事項（運用）**

* 当日出馬表の未確定問題に対応するため、深夜の μ 推論は「前処理」扱いとし、**オッズ取得後に再度完全な推論（μ/σ/ν）を実行**する運用としました（上記ステップ 4 と 8）。これにより、出馬表が深夜時点で未確定でも最終的に正しい出走馬情報で推論できます。
* `fetch_today_shutuba_if_missing.py` は当日午前に出馬表が存在しない場合に再取得するユーティリティです（別ファイルとして実装してください）。

---

### 13.2 週次運用フロー（再掲）

（既存仕様を継承。キャリブレーション確認等は週次で実施し、閾値を超えた場合は担当者に通知する。）

```bash
#!/bin/bash
# scripts/weekly_operation.sh
# 週次運用スクリプト（毎週月曜実行を想定）
set -euo pipefail

echo "=========================================="
echo "Keiba AI 週次運用開始"
echo "$(date '+%Y-%m-%d %H:%M:%S')"
echo "==========================================""

# 1. データ整合性チェック
python scripts/check_data_integrity.py --lookback_days 7

# 2. モデルパフォーマンス評価
python src/models/evaluate_recent_performance.py --lookback_days 7

# 3. キャリブレーション確認（Brier, ECE 等）
python src/models/check_calibration.py --lookback_days 7 --monitoring_config configs/monitoring.yaml

# 4. バックアップ実行
bash scripts/backup_data.sh

echo "=========================================="
echo "週次運用完了"
echo "$(date '+%Y-%m-%d %H:%M:%S')"
echo "=========================================="
```

---

### 13.3 月次運用フロー（σ/ν モデル再学習の追加）

`修正内容.txt` の指摘に従い、**σ（馬ごとの残差分散）/ν（レース荒れ度）モデル**の再学習を月次フローに明確に追加します。以下は `scripts/monthly_operation.sh` の関連部分。

```bash
#!/bin/bash
# scripts/monthly_operation.sh
set -euo pipefail

echo "=========================================="
echo "Keiba AI 月次運用開始"
echo "$(date '+%Y-%m-%d %H:%M:%S')"
echo "=========================================="

# 1. フルデータ整備（過去 N ヶ月分）
python scripts/prepare_monthly_training_data.py --months 6

# 2. μモデルの必要に応じた再学習（オプション）
python src/models/train_mu_model.py --data_dir data/features/ --output_dir data/models/mu_$(date +%Y%m%d)

# 3. σ/ν モデルの再学習（必須）
python src/models/train_sigma_nu_models.py --training_window_months 6 --output_dir data/models/sigma_nu_$(date +%Y%m%d)

# 4. キャリブレーション再実行（必要に応じて）
python src/models/recalibrate.py --model_dir data/models/latest

# 5. バックテスト / レポート生成
python scripts/run_monthly_backtests.py --start_date $(date -d '6 months ago' +%Y-%m-%d) --end_date $(date +%Y-%m-%d)

echo "=========================================="
echo "月次運用完了"
echo "$(date '+%Y-%m-%d %H:%M:%S')"
echo "=========================================="
```

---

### 13.4 `train_sigma_nu_models.py`（新規追加） — 完全実装

`修正内容.txt` の指摘に基づき、`src/models/train_sigma_nu_models.py` を新規追加します。以下は**そのままコピペ可能な実装**です。目的は：

* μ モデルの予測を取り込み、残差分散（σ）モデルを学習する。
* レースレベルの荒れ度（ν）を学習する。
* 両モデルを `output_dir` に保存する（pickle または joblib）。

```python
#!/usr/bin/env python3
# src/models/train_sigma_nu_models.py
"""
σ/ν モデル再学習スクリプト
- 入力: parsed/results, features, mu_predictions
- 出力: sigma_model.pkl, nu_model.pkl を output_dir に保存
- 引数:
    --training_window_months: 過去何ヶ月分のデータで学習するか
    --output_dir: 保存先
"""

import argparse
import logging
import os
import pickle
from datetime import datetime, timedelta
from pathlib import Path

import pandas as pd
import numpy as np
import lightgbm as lgb
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error

# ロギング
logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s')

def load_training_data(months: int):
    """
    データ読み込みのプレースホルダ:
    - parsed/results/ に過去レースの結果が Parquet 等で格納されている前提
    - features/ に馬・レース特徴量が保存されている前提
    実運用ではデータパスを configs から取得する実装に差し替えること
    """
    # ここは例示的に実装。運用ではデータフォーマットに合わせて修正。
    end_date = datetime.now()
    start_date = end_date - timedelta(days=30*months)
    # 例: data/parsed/parquet/results/year=YYYY/... の読み込みを行う
    # 簡易実装: 「全データ」を読み込み後フィルタ
    results_path = Path("data/parsed/parquet/results")
    all_dfs = []
    if not results_path.exists():
        raise FileNotFoundError("parsed results path not found: data/parsed/parquet/results")
    for p in results_path.rglob("*.parquet"):
        try:
            df = pd.read_parquet(p)
            if 'race_date' in df.columns:
                df['race_date'] = pd.to_datetime(df['race_date'])
                df = df[(df['race_date'] >= start_date) & (df['race_date'] <= end_date)]
            all_dfs.append(df)
        except Exception as e:
            logging.warning(f"Failed to read {p}: {e}")
    if not all_dfs:
        raise RuntimeError("No training data found in parsed results for given window")
    results_df = pd.concat(all_dfs, ignore_index=True)
    return results_df

def prepare_sigma_training(results_df: pd.DataFrame, mu_preds: pd.Series):
    """
    σモデル用データ整備
    - マッチング: results_df に mu_preds (horse_id indexed series) をマージ
    - 目的変数: squared_error = (actual_finish_metric - mu_pred)^2
      ※ 実務では残差の分散をモデル化。ここでは finish_time 等の実数値を想定。
    """
    df = results_df.copy()
    # 仮定: df に 'horse_id' と 'performance_target' が存在（例: finish_time, or ranking converted）
    if 'horse_id' not in df.columns:
        raise ValueError("results_df must contain horse_id")
    # mu_preds: Series indexed by horse_id, containing predicted expectation (mu)
    df['mu_pred'] = df['horse_id'].map(mu_preds.to_dict())
    # 実際の目標値を performance_target 列と仮定（運用でフィールド名を合わせること）
    if 'performance_target' not in df.columns:
        # 代替: 着順から擬似的に数値化する（小さい値が良い）
        df['performance_target'] = df['finish_position'].astype(float)
    df['squared_error'] = (df['performance_target'] - df['mu_pred']) ** 2
    # 馬レベルで集約（馬ごとの分散を予測するため）
    horse_agg = df.groupby('horse_id').agg({
        'squared_error': 'mean',
    }).reset_index().rename(columns={'squared_error': 'sigma_target'})
    # 特徴量の準備（例）: 過去走集約値等を features テーブルから結合する必要あり
    # 簡易に results_df の代表値を使う（運用で features を結合すること）
    horse_features = df.groupby('horse_id').agg({
        'age': 'first',
        'sex': 'first',
        'weight': 'mean',
        'finish_position': 'mean'
    }).reset_index()
    train_df = horse_features.merge(horse_agg, on='horse_id', how='inner')
    # 欠損埋め
    train_df = train_df.fillna(0)
    return train_df

def prepare_nu_training(results_df: pd.DataFrame, mu_preds: pd.Series):
    """
    νモデル用データ整備（レース荒れ度）
    - レースごとに実際の順位分散 / 着順の標準偏差 を計算し、レース特徴量から予測する
    """
    df = results_df.copy()
    df['mu_pred'] = df['horse_id'].map(mu_preds.to_dict())
    # レースごとの実際の順位分散を計算
    race_variance = df.groupby('race_id').agg({
        'finish_position': 'std'
    }).reset_index().rename(columns={'finish_position': 'nu_target'})
    # レース特徴量の作成（例）
    race_features = df.groupby('race_id').agg({
        'distance_m': 'first',
        'track_surface': 'first',
        'track_condition': 'first',
        'weather': 'first',
        'head_count': 'first',
        'win_odds': ['mean', 'std'],
    }).reset_index()
    # flatten columns
    race_features.columns = ['race_id', 'distance_m', 'track_surface', 'track_condition', 'weather', 'head_count', 'odds_mean', 'odds_std']
    train_df = race_features.merge(race_variance, on='race_id', how='inner')
    # カテゴリダミー化
    categorical_cols = ['track_surface', 'track_condition', 'weather']
    for col in categorical_cols:
        if col in train_df.columns:
            dummies = pd.get_dummies(train_df[col], prefix=col)
            train_df = pd.concat([train_df.drop(columns=[col]), dummies], axis=1)
    train_df = train_df.fillna(0)
    return train_df

def train_model_lgb(train_X, train_y, params=None):
    """
    LightGBM 回帰モデル 学習（シンプル実装）
    """
    if params is None:
        params = {
            'objective': 'regression',
            'metric': 'rmse',
            'verbosity': -1,
            'boosting_type': 'gbdt',
            'num_leaves': 31,
            'learning_rate': 0.05,
            'n_estimators': 100
        }
    model = lgb.LGBMRegressor(**params)
    model.fit(train_X, train_y)
    return model

def main():
    parser = argparse.ArgumentParser(description="Train sigma and nu models")
    parser.add_argument("--training_window_months", type=int, default=6, help="Months of history to train on")
    parser.add_argument("--output_dir", type=str, default="data/models/sigma_nu_latest")
    args = parser.parse_args()

    logging.info("Loading training data...")
    results_df = load_training_data(args.training_window_months)

    # μ の予測が既に事前生成されていることを想定（data/predictions 等）
    # ここでは mu_predictions の取得方法を示す（実運用でパスを合わせる）
    mu_preds_path = Path("data/predictions/mu_latest.parquet")
    if not mu_preds_path.exists():
        # fallback: 事前に mu を推論しておくことを推奨
        logging.error("mu predictions not found at data/predictions/mu_latest.parquet - please run mu prediction first")
        raise FileNotFoundError("mu predictions missing")

    mu_preds_df = pd.read_parquet(mu_preds_path)
    # mu_preds_df は columns: horse_id, mu_pred を想定
    mu_series = pd.Series(mu_preds_df['mu_pred'].values, index=mu_preds_df['horse_id'].astype(str))

    # σ モデル学習
    logging.info("Preparing sigma training data...")
    sigma_train_df = prepare_sigma_training(results_df, mu_series)
    sigma_feature_cols = [c for c in sigma_train_df.columns if c not in ('horse_id', 'sigma_target')]
    X_sigma = sigma_train_df[sigma_feature_cols]
    y_sigma = sigma_train_df['sigma_target']
    X_train, X_val, y_train, y_val = train_test_split(X_sigma, y_sigma, test_size=0.2, random_state=42)
    logging.info("Training sigma model...")
    sigma_model = train_model_lgb(X_train, y_train)
    y_pred = sigma_model.predict(X_val)
    logging.info(f"Sigma model RMSE: {mean_squared_error(y_val, y_pred, squared=False):.6f}")

    # ν モデル学習
    logging.info("Preparing nu training data...")
    nu_train_df = prepare_nu_training(results_df, mu_series)
    nu_feature_cols = [c for c in nu_train_df.columns if c not in ('race_id', 'nu_target')]
    X_nu = nu_train_df[nu_feature_cols]
    y_nu = nu_train_df['nu_target']
    Xn_train, Xn_val, yn_train, yn_val = train_test_split(X_nu, y_nu, test_size=0.2, random_state=42)
    logging.info("Training nu model...")
    nu_model = train_model_lgb(Xn_train, yn_train)
    yn_pred = nu_model.predict(Xn_val)
    logging.info(f"Nu model RMSE: {mean_squared_error(yn_val, yn_pred, squared=False):.6f}")

    # 保存
    out_dir = Path(args.output_dir)
    out_dir.mkdir(parents=True, exist_ok=True)
    sigma_path = out_dir / "sigma_model.pkl"
    nu_path = out_dir / "nu_model.pkl"
    logging.info(f"Saving sigma model to {sigma_path}")
    with open(sigma_path, "wb") as f:
        pickle.dump(sigma_model, f)
    logging.info(f"Saving nu model to {nu_path}")
    with open(nu_path, "wb") as f:
        pickle.dump(nu_model, f)

    logging.info("Training completed.")

if __name__ == "__main__":
    main()
```

**運用ノート**

* 上記は実装例です。`mu_predictions` の取り込み部分は、実際の `data/predictions/` 構造に合わせて調整してください。
* 学習データのフィールド名（`performance_target` 等）も実運用のスキーマに合わせる必要があります。
* 学習後は `model_metadata` テーブルに `commit_hash`, `training_date`, `hyperparams`, `library_versions` 等を記録する運用を推奨します。

# 14. 再現性・CI/CD・インフラ（改訂版 — Dockerfile 関連全文）

**改訂理由（要約）**
`修正内容.txt` において「Selenium を Docker コンテナ内で実行する場合の前提が不明確（xvfb 等）」が指摘されました。これを踏まえ、`infra/docker/Dockerfile` と `docker-compose.yml` の想定内容を明確化し、Selenium（Chrome）をヘッドレスで安定して動かすための追加パッケージ（`xvfb`, `fonts`, `chromedriver` の扱い）や、運用時の推奨起動コマンド（`xvfb-run`）を記載します。

（以下、該当の infra/docker セクション全文。Dockerfile はそのまま `infra/docker/Dockerfile` に置換可能。）

---

## 14. 再現性・CI/CD・インフラ（Docker 関連改訂）

### 14.1 Docker イメージ設計（Selenium 実行をサポート）

Docker イメージはローカル開発 / CI 環境で Selenium ベースのスクレイピングを実行できるように最小限の構成を提供します。**注意**: ブラウザ自体（Google Chrome）や Chromedriver のバージョンは一致させる必要があります。以下は推奨 Dockerfile の例です。

```dockerfile
# infra/docker/Dockerfile
FROM python:3.10-slim

ENV DEBIAN_FRONTEND=noninteractive
RUN apt-get update && apt-get install -y --no-install-recommends \
    wget \
    gnupg \
    ca-certificates \
    unzip \
    xvfb \
    fonts-liberation \
    fonts-noto-cjk \
    libnss3 \
    libxss1 \
    libx11-xcb1 \
    libx11-6 \
    libglib2.0-0 \
    libgtk-3-0 \
    libgbm1 \
    libasound2 \
    && rm -rf /var/lib/apt/lists/*

# install google-chrome-stable
RUN wget -q -O - https://dl.google.com/linux/linux_signing_key.pub | apt-key add - \
  && echo "deb [arch=amd64] http://dl.google.com/linux/chrome/deb/ stable main" >> /etc/apt/sources.list.d/google-chrome.list \
  && apt-get update && apt-get install -y google-chrome-stable \
  && rm -rf /var/lib/apt/lists/*

# install chromedriver matching installed chrome version (運用ではバージョン管理必須)
# ここは簡易的に chrome のバージョンに合わせた driver を取得する処理を入れるか、明示的にバージョンを合わせてビルドすること
RUN CHROME_VERSION=$(google-chrome --version | awk '{print $3}' | cut -d'.' -f1) \
  && echo "Chrome major version: $CHROME_VERSION" \
  || true

# Set workdir
WORKDIR /workspace

# copy requirements & install
COPY pyproject.toml poetry.lock* requirements.txt* /workspace/
# If using pip requirements
RUN if [ -f requirements.txt ]; then pip install --no-cache-dir -r requirements.txt; fi

# copy source
COPY . /workspace

# create non-root user
RUN useradd -m keiba && chown -R keiba:keiba /workspace
USER keiba

# entrypoint: recommend using xvfb-run when invoking scripts that use selenium in headless env
ENTRYPOINT ["/bin/bash", "-lc"]
CMD ["echo 'Container ready. Use xvfb-run for Selenium tasks or run scripts directly in non-Docker environment.'"]
```

**運用ガイド**

* Docker 上で Selenium を使う場合は、`xvfb-run` を利用して X バッファを提供するか、Chrome の `--headless=new` オプションで動かす。`xvfb` をインストール済みであれば、コンテナ内で `xvfb-run python src/scripts/run_scrape_jra_odds_daily.py` と実行できます。`修正内容.txt` の指摘に従い、この記載を Dockerfile に明示しました。
* Chromedriver のバージョン管理は厳密に行ってください（Chrome とドライバの major version が一致していること）。CI ではビルドパイプラインで対応バージョンの chromedriver をダウンロードしてインストールするステップを追加してください。

---

### 14.2 docker-compose 例（Selenium タスク用）

`infra/docker/docker-compose.yml` はワークフローによって optional です。Selenium タスク専用に以下のようなサービスを用意すると実行が楽になります。

```yaml
version: '3.8'
services:
  keibaai:
    build: .
    volumes:
      - .:/workspace
    environment:
      - KEIBAAI_METADATA_DB=/workspace/data/keibaai/metadata/db.sqlite3
      - JRA_ODDS_OUTPUT_DIR=/workspace/data/raw/json/jra_odds
    command: bash -lc "xvfb-run -a python src/scripts/run_scrape_jra_odds_daily.py --date $(date +%Y-%m-%d)"


## 15. リスクと緩和策

### 15.1 技術的リスク

| リスク | 影響度 | 発生確率 | 緩和策 |
|--------|--------|----------|--------|
| HTML構造変更 | 高 | 中 | 回帰テスト、パーサバージョニング、アラート |
| モデルドリフト | 高 | 中 | 定期的な再学習、キャリブレーション監視 |
| 計算リソース不足 | 中 | 低 | K値の調整、Numba最適化、夜間バッチ |
| データ欠損 | 中 | 中 | 欠損値処理、データ整合性チェック |
| スクレイピングBAN | 高 | 中 | 遅延・User-Agent・robots.txt遵守 |

### 15.2 運用的リスク

| リスク | 影響度 | 発生確率 | 緩和策 |
|--------|--------|----------|--------|
| 発注ミス | 高 | 低 | 手動確認、二段階承認、小額テスト |
| 流動性不足 | 中 | 中 | 流動性上限設定、複数券種への分散 |
| オッズ急変 | 中 | 中 | 締切直前の再取得、EV再計算 |
| システム停止 | 中 | 低 | バックアップ、アラート、手動フォールバック |
| データ消失 | 高 | 低 | 定期バックアップ、外付けHDD、冗長化 |

### 15.3 法務・規制リスク

| リスク | 影響度 | 発生確率 | 緩和策 |
|--------|--------|----------|--------|
| スクレイピング利用規約違反 | 高 | 中 | 規約確認、問い合わせ、公式API検討 |
| 賭博法抵触 | 高 | 低 | 個人利用の範囲内、自動発注無効化 |
| 著作権侵害 | 中 | 低 | データの二次利用なし、個人利用のみ |

### 15.4 リスク管理実装


"""
リスク管理モジュール
"""

import logging
from typing import Dict, List
import pandas as pd
import numpy as np


class RiskManager:
    """
    リスク管理クラス
    """
    
    def __init__(self, config: Dict):
        """
        Args:
            config: リスク管理設定辞書
        """
        self.config = config
    
    def check_allocation_risk(
        self,
        allocation: Dict,
        capital: float
    ) -> Dict:
        """
        投資配分のリスクをチェック
        
        Args:
            allocation: 投資配分
            capital: 現在資金
        
        Returns:
            リスク評価結果
        """
        logging.info("投資配分リスクチェック開始")
        
        risks = []
        
        total_investment = allocation['total_investment']
        
        # 1. 投資比率チェック
        investment_ratio = total_investment / capital
        max_ratio = self.config.get('max_investment_ratio', 0.1)
        
        if investment_ratio > max_ratio:
            risks.append({
                'type': 'over_investment',
                'severity': 'high',
                'message': f'投資比率が上限を超えています: {investment_ratio:.2%} > {max_ratio:.2%}',
                'action': 'reduce_investment'
            })
        
        # 2. 単一券種への集中度チェック
        if len(allocation['bets']) > 0:
            max_bet = max([bet['amount'] for bet in allocation['bets']])
            concentration = max_bet / total_investment if total_investment > 0 else 0
            max_concentration = self.config.get('max_concentration', 0.3)
            
            if concentration > max_concentration:
                risks.append({
                    'type': 'concentration',
                    'severity': 'medium',
                    'message': f'単一券種への集中度が高すぎます: {concentration:.2%} > {max_concentration:.2%}',
                    'action': 'diversify'
                })
        
        # 3. 期待リターンの妥当性チェック
        expected_return = allocation.get('expected_return', 0)
        
        if expected_return < 0:
            risks.append({
                'type': 'negative_expectation',
                'severity': 'high',
                'message': f'期待リターンが負です: {expected_return:.2%}',
                'action': 'cancel_all'
            })
        
        # 4. 流動性チェック
        for bet in allocation['bets']:
            if bet['amount'] > bet.get('liquidity', float('inf')):
                risks.append({
                    'type': 'liquidity',
                    'severity': 'medium',
                    'message': f"券種 {bet['type']} {bet['selection']} の投資額が流動性上限を超えています",
                    'action': 'reduce_bet'
                })
        
        result = {
            'risks': risks,
            'risk_level': self._calculate_risk_level(risks),
            'approved': len([r for r in risks if r['severity'] == 'high']) == 0
        }
        
        logging.info(f"リスクチェック完了: {len(risks)}件のリスク検出")
        
        return result
    
    def _calculate_risk_level(self, risks: List[Dict]) -> str:
        """
        総合リスクレベルを計算
        
        Args:
            risks: リスクリスト
        
        Returns:
            リスクレベル（'low', 'medium', 'high'）
        """
        if len(risks) == 0:
            return 'low'
        
        high_count = len([r for r in risks if r['severity'] == 'high'])
        medium_count = len([r for r in risks if r['severity'] == 'medium'])
        
        if high_count > 0:
            return 'high'
        elif medium_count > 2:
            return 'high'
        elif medium_count > 0:
            return 'medium'
        else:
            return 'low'
    
    def calculate_var(
        self,
        returns: np.ndarray,
        confidence: float = 0.95
    ) -> float:
        """
        VaR（Value at Risk）を計算
        
        Args:
            returns: リターンの配列
            confidence: 信頼区間
        
        Returns:
            VaR値
        """
        var = np.percentile(returns, (1 - confidence) * 100)
        return var
    
    def calculate_cvar(
        self,
        returns: np.ndarray,
        confidence: float = 0.95
    ) -> float:
        """
        CVaR（Conditional VaR）を計算
        
        Args:
            returns: リターンの配列
            confidence: 信頼区間
        
        Returns:
            CVaR値
        """
        var = self.calculate_var(returns, confidence)
        cvar = np.mean(returns[returns <= var])
        return cvar


---

## 16. テスト戦略

### 16.1 単体テスト（tests/unit/test_parsers.py）


"""
パーサの単体テスト
"""

import pytest
import pandas as pd
from pathlib import Path
from src.parsers.results_parser import parse_results_html
from src.parsers.shutuba_parser import parse_shutuba_html


class TestResultsParser:
    """
    レース結果パーサのテストクラス
    """
    
    def test_parse_results_basic(self):
        """
        基本的なパーステスト
        """
        # テスト用HTMLを読み込み
        test_file = Path('tests/fixtures/race_samples/202306010101.bin')
        
        if not test_file.exists():
            pytest.skip('テストフィクスチャが見つかりません')
        
        # パース実行
        df = parse_results_html(str(test_file), race_id='202306010101')
        
        # アサーション
        assert len(df) > 0, 'パース結果が空です'
        assert 'race_id' in df.columns
        assert 'horse_id' in df.columns
        assert 'finish_position' in df.columns
        
        # データ型チェック
        assert df['finish_position'].dtype in [int, 'Int64']
        assert df['win_odds'].dtype == float
    
    def test_parse_results_finish_position_range(self):
        """
        着順の範囲チェック
        """
        test_file = Path('tests/fixtures/race_samples/202306010101.bin')
        
        if not test_file.exists():
            pytest.skip('テストフィクスチャが見つかりません')
        
        df = parse_results_html(str(test_file), race_id='202306010101')
        
        # 着順は1以上、頭数以下
        assert df['finish_position'].min() >= 1
        assert df['finish_position'].max() <= len(df)
    
    def test_parse_time_conversion(self):
        """
        タイム変換のテスト
        """
        from src.parsers.results_parser import parse_time_to_seconds
        
        assert parse_time_to_seconds('1:59.8') == pytest.approx(119.8, rel=0.01)
        assert parse_time_to_seconds('2:00.0') == pytest.approx(120.0, rel=0.01)
        assert parse_time_to_seconds('---') is None
    
    def test_parse_margin_conversion(self):
        """
        着差変換のテスト
        """
        from src.parsers.results_parser import parse_margin_to_seconds
        
        assert parse_margin_to_seconds('1.1/2') == pytest.approx(1.5, rel=0.01)
        assert parse_margin_to_seconds('3/4') == pytest.approx(0.75, rel=0.01)
        assert parse_margin_to_seconds('アタマ') == pytest.approx(0.1, rel=0.01)
        assert parse_margin_to_seconds('ハナ') == pytest.approx(0.05, rel=0.01)


class TestShutubaParser:
    """
    出馬表パーサのテストクラス
    """
    
    def test_parse_shutuba_basic(self):
        """
        基本的なパーステスト
        """
        test_file = Path('tests/fixtures/shutuba_samples/202306010101.bin')
        
        if not test_file.exists():
            pytest.skip('テストフィクスチャが見つかりません')
        
        df = parse_shutuba_html(str(test_file), race_id='202306010101')
        
        assert len(df) > 0
        assert 'race_id' in df.columns
        assert 'horse_number' in df.columns
        assert 'jockey_id' in df.columns


### 16.2 統合テスト（tests/integration/test_pipeline_e2e.py）


"""
エンドツーエンド統合テスト
"""

import pytest
from pathlib import Path
import pandas as pd
from src.features.feature_engine import FeatureEngine
from src.models.model_train import MuEstimator


class TestEndToEnd:
    """
    エンドツーエンドテストクラス
    """
    
    @pytest.fixture
    def sample_data(self):
        """
        テスト用データを準備
        """
        # 小規模なサンプルデータ
        shutuba_df = pd.DataFrame({
            'race_id': ['test_race_001'] * 5,
            'horse_id': [f'horse_{i}' for i in range(5)],
            'horse_number': range(1, 6),
            'age': [3, 4, 5, 3, 4],
            'sex': ['牡', '牝', '牡', '牡', '牝'],
            'horse_weight': [480, 460, 500, 470, 455],
            'basis_weight': [55, 54, 56, 55, 53],
            'jockey_id': [f'jockey_{i}' for i in range(5)],
            'trainer_id': [f'trainer_{i}' for i in range(5)]
        })
        
        results_df = pd.DataFrame({
            'race_id': ['test_race_001'] * 5,
            'horse_id': [f'horse_{i}' for i in range(5)],
            'finish_position': [1, 2, 3, 4, 5],
            'finish_time_seconds': [120.5, 120.8, 121.0, 121.5, 122.0]
        })
        
        return shutuba_df, results_df
    
    def test_feature_generation_pipeline(self, sample_data):
        """
        特徴量生成パイプラインのテスト
        """
        shutuba_df, results_df = sample_data
        
        # 特徴量エンジン初期化
        config = {
            'basic_features': {'enabled': True},
            'past_performance_aggregation': {'windows': [1, 3]},
            'pedigree_features': {'enabled': False}
        }
        
        engine = FeatureEngine(config)
        
        # 特徴量生成（簡易版）
        features_df = engine._add_basic_features(shutuba_df)
        
        assert len(features_df) == len(shutuba_df)
        assert 'sex_牡' in features_df.columns or 'sex_牝' in features_df.columns
    
    def test_model_training_pipeline(self, sample_data):
        """
        モデル学習パイプラインのテスト
        """
        shutuba_df, results_df = sample_data
        
        # 特徴量準備
        features_df = shutuba_df.merge(
            results_df[['race_id', 'horse_id', 'finish_position']],
            on=['race_id', 'horse_id'],
            how='inner'
        )
        
        # モデル学習（小規模データなので警告が出る可能性あり）
        config = {
            'regressor_params': {'num_leaves': 5, 'num_boost_round': 10},
            'ranker_params': {'num_leaves': 5, 'num_boost_round': 10}
        }
        
        estimator = MuEstimator(config)
        
        # 学習実行は時間がかかるためスキップ
        # result = estimator.train(features_df)
        
        # モデル構造のチェックのみ
        assert estimator.feature_names == []  # 未学習状態


### 16.3 回帰テスト（tests/regression/test_parser_regression.py）


"""
パーサの回帰テスト
過去に正しくパースできたHTMLが引き続きパースできることを確認
"""

import pytest
import pandas as pd
from pathlib import Path
import hashlib
from src.parsers.results_parser import parse_results_html


class TestParserRegression:
    """
    パーサ回帰テストクラス
    """
    
    @pytest.fixture
    def regression_samples(self):
        """
        回帰テスト用サンプルを取得
        """
        samples_dir = Path('tests/fixtures/regression_samples')
        
        if not samples_dir.exists():
            pytest.skip('回帰テストサンプルが見つかりません')
        
        samples = []
        
        for html_file in samples_dir.glob('*.bin'):
            # 期待結果ファイル
            expected_file = html_file.with_suffix('.expected.parquet')
            
            if expected_file.exists():
                samples.append({
                    'html_file': html_file,
                    'expected_file': expected_file
                })
        
        return samples
    
    def test_results_parser_regression(self, regression_samples):
        """
        レース結果パーサの回帰テスト
        """
        if len(regression_samples) == 0:
            pytest.skip('回帰テストサンプルがありません')
        
        for sample in regression_samples:
            race_id = sample['html_file'].stem
            
            # パース実行
            df = parse_results_html(str(sample['html_file']), race_id=race_id)
            
            # 期待結果をロード
            expected_df = pd.read_parquet(sample['expected_file'])
            
            # アサーション
            assert len(df) == len(expected_df), \
                f"行数が一致しません: {len(df)} != {len(expected_df)}"
            
            # 主要カラムの一致を確認
            for col in ['horse_id', 'finish_position', 'jockey_id']:
                if col in expected_df.columns and col in df.columns:
                    assert df[col].equals(expected_df[col]), \
                        f"カラム {col} が一致しません"


### 16.4 性能テスト（tests/performance/test_simulation_performance.py）


"""
シミュレーションの性能テスト
"""

import pytest
import numpy as np
import time
from src.sim.simulator import simulate_plackett_luce_numba


class TestSimulationPerformance:
    """
    シミュレーション性能テストクラス
    """
    
    def test_simulation_speed_small(self):
        """
        小規模シミュレーション（16頭、K=1000）の速度テスト
        """
        n_horses = 16
        K = 1000
        
        mu = np.random.randn(n_horses)
        sigma = np.random.rand(n_horses) * 0.5 + 0.5
        nu = 1.0
        seed = 42
        
        start_time = time.time()
        
        rankings = simulate_plackett_luce_numba(
            mu=mu,
            sigma=sigma,
            nu=nu,
            n_horses=n_horses,
            K=K,
            seed=seed
        )
        
        elapsed = time.time() - start_time
        
        # 1秒以内に完了すること
        assert elapsed < 1.0, f"シミュレーションが遅すぎます: {elapsed:.2f}秒"
        
        # 結果の検証
        assert rankings.shape == (K, n_horses)
    
    def test_simulation_speed_large(self):
        """
        大規模シミュレーション（18頭、K=10000）の速度テスト
        """
        n_horses = 18
        K = 10000
        
        mu = np.random.randn(n_horses)
        sigma = np.random.rand(n_horses) * 0.5 + 0.5
        nu = 1.0
        seed = 42
        
        start_time = time.time()
        
        rankings = simulate_plackett_luce_numba(
            mu=mu,
            sigma=sigma,
            nu=nu,
            n_horses=n_horses,
            K=K,
            seed=seed
        )
        
        elapsed = time.time() - start_time
        
        # 10秒以内に完了すること
        assert elapsed < 10.0, f"大規模シミュレーションが遅すぎます: {elapsed:.2f}秒"
        
        # 結果の検証
        assert rankings.shape == (K, n_horses)


---

## 17. 参考コードモジュール（グルーコード）

*仕様書（セクション 13.1）で定義されていたが、実装が欠落していた「グルーコード」を以下に全文実装します。*

#### 17.1 src/parsers/parse_all.py

*`data/raw` からHTMLを読み込み、パーサを実行して `data/parsed` にParquetで保存します。*

```python
#!/usr/bin/env python3
"""
データ整形パイプライン 実行スクリプト
指定された日付（または期間）のrawデータをパースし、
Parquet形式で data/parsed/ に保存する。

実行例 (日付指定):
python src/parsers/parse_all.py --date 2023-10-01

実行例 (期間指定):
python src/parsers/parse_all.py --start_date 2023-10-01 --end_date 2023-10-31
"""

import argparse
import logging
import sys
import traceback
from datetime import datetime, timedelta, timezone
from pathlib import Path
from typing import List, Dict, Any, Callable

import pandas as pd
import sqlite3
import json

# プロジェクトルートをパスに追加
sys.path.append(str(Path(__file__).resolve().parents[1]))

from src.pipeline_core import setup_logging, get_db_connection, load_config
from src.parsers.results_parser import parse_results_html
from src.parsers.shutuba_parser import parse_shutuba_html
from src.parsers.horse_info_parser import parse_horse_profile, parse_horse_performance
from src.parsers.pedigree_parser import parse_pedigree_html


# --- エラーハンドリング (仕様書 5.1.2 より) ---

def parse_with_error_handling(
    file_path: str,
    parser_name: str,
    parse_func: Callable,
    db_conn: sqlite3.Connection,
    **kwargs
) -> Any:
    """
    エラーハンドリング付きパーサ実行

    Args:
        file_path: 対象ファイルパス
        parser_name: パーサ名
        parse_func: パース関数
        db_conn: SQLite接続
        **kwargs: パース関数に渡す追加引数

    Returns:
        パース結果（成功時）またはNone（失敗時）
    """
    try:
        result = parse_func(file_path, **kwargs)
        return result
    except Exception as e:
        error_message = str(e)
        stack_trace = traceback.format_exc()

        logging.error(f"パースエラー ({parser_name}): {file_path} - {error_message}")

        # データベースに記録
        try:
            cursor = db_conn.cursor()
            cursor.execute('''
            INSERT INTO parse_failures (
                parser_name, source_file, error_type,
                error_message, stack_trace, failed_ts
            ) VALUES (?, ?, ?, ?, ?, ?)
            ''', (
                parser_name,
                file_path,
                type(e).__name__,
                error_message,
                stack_trace,
                datetime.now(timezone.utc).isoformat()
            ))
            db_conn.commit()
        except Exception as db_e:
            logging.error(f"パースエラーのDB記録に失敗: {db_e}")

        # エラー詳細をJSONに保存
        try:
            error_dir = Path(f'data/errors/parse_failures/{parser_name}')
            error_dir.mkdir(parents=True, exist_ok=True)
            error_file = error_dir / f"{Path(file_path).stem}_error.json"

            error_data = {
                'file_path': file_path,
                'parser_name': parser_name,
                'error_type': type(e).__name__,
                'error_message': error_message,
                'stack_trace': stack_trace,
                'failed_ts': datetime.now(timezone.utc).isoformat()
            }
            with open(error_file, 'w', encoding='utf-8') as f:
                json.dump(error_data, f, ensure_ascii=False, indent=2)
        except Exception as json_e:
            logging.error(f"パースエラーのJSON保存に失敗: {json_e}")

        return None

# --- パーサ実行 ---

def run_parser(
    db_conn: sqlite3.Connection,
    config: Dict[str, Any],
    file_paths: List[Path],
    parser_name: str,
    parse_func: Callable,
    output_dir: Path,
    partition_cols: List[str]
):
    """
    指定されたパーサを実行し、結果をParquetに保存する
    """
    logging.info(f"--- {parser_name} 実行開始 ({len(file_paths)}件) ---")
    
    results = []
    
    for i, file_path in enumerate(file_paths):
        if (i + 1) % 100 == 0:
            logging.info(f"進捗: {i + 1}/{len(file_paths)}")
            
        result = parse_with_error_handling(
            file_path=str(file_path),
            parser_name=parser_name,
            parse_func=parse_func,
            db_conn=db_conn
        )
        
        if result is not None:
            if isinstance(result, pd.DataFrame):
                results.append(result)
            elif isinstance(result, dict):
                results.append(pd.DataFrame([result]))
    
    if not results:
        logging.warning(f"{parser_name}: 有効なパース結果がありませんでした")
        return

    # 結合
    try:
        df = pd.concat(results, ignore_index=True)
    except Exception as e:
        logging.error(f"{parser_name}: DataFrameの結合に失敗: {e}")
        return

    # 日付カラムを追加（パーティション用）
    if 'race_id' in df.columns:
        df['race_date_str'] = df['race_id'].astype(str).str[:8]
        df['race_date'] = pd.to_datetime(df['race_date_str'], format='%Y%m%d')
        df['year'] = df['race_date'].dt.year
        df['month'] = df['race_date'].dt.month
        df['day'] = df['race_date'].dt.day
    
    # 保存
    try:
        output_dir.mkdir(parents=True, exist_ok=True)
        df.to_parquet(
            output_dir,
            engine='pyarrow',
            compression='snappy',
            partition_cols=partition_cols,
            existing_data_behavior='overwrite_or_ignore'
        )
        logging.info(f"{parser_name}: {len(df)}行を {output_dir} に保存しました")
    except Exception as e:
        logging.error(f"{parser_name}: Parquet保存に失敗: {e}")


def get_target_files(base_dir: Path, start_dt: datetime, end_dt: datetime) -> List[Path]:
    """
    指定期間のrawファイルを取得
    (ファイル名に日付が含まれていないため、更新日時でフィルタ)
    """
    target_files = []
    
    # タイムスタンプに変換
    start_ts = start_dt.timestamp()
    end_ts = end_dt.timestamp() + 86400 # 終了日の終わりまで

    if not base_dir.exists():
        logging.warning(f"ディレクトリが見つかりません: {base_dir}")
        return []

    for file_path in base_dir.rglob('*.bin'):
        try:
            mtime = file_path.stat().st_mtime
            if start_ts <= mtime <= end_ts:
                target_files.append(file_path)
        except OSError:
            continue
            
    return target_files


def main():
    """メイン実行関数"""
    parser = argparse.ArgumentParser(description='Keiba AI データ整形パイプライン')
    parser.add_argument(
        '--date',
        type=str,
        help='処理対象日 (YYYY-MM-DD)。指定しない場合は期間指定が必須。'
    )
    parser.add_argument(
        '--start_date',
        type=str,
        help='処理開始日 (YYYY-MM-DD)'
    )
    parser.add_argument(
        '--end_date',
        type=str,
        help='処理終了日 (YYYY-MM-DD)'
    )
    parser.add_argument(
        '--config',
        type=str,
        default='configs/default.yaml',
        help='設定ファイルパス'
    )
    parser.add_argument(
        '--log_level',
        type=str,
        default='INFO',
        choices=['DEBUG', 'INFO', 'WARNING', 'ERROR', 'CRITICAL'],
        help='ログレベル'
    )

    args = parser.parse_args()

    # ロギング設定
    setup_logging(level=args.log_level)

    logging.info("=" * 60)
    logging.info("Keiba AI データ整形パイプライン開始")
    logging.info("=" * 60)

    # 設定ロード
    config = load_config(args.config)
    paths = config.get('paths', {})
    
    # データベース接続
    db_conn = get_db_connection(config.get('db_path', 'data/metadata/db.sqlite3'))

    # 日付範囲の決定
    try:
        if args.date:
            start_dt = datetime.strptime(args.date, '%Y-%m-%d')
            end_dt = start_dt
        elif args.start_date and args.end_date:
            start_dt = datetime.strptime(args.start_date, '%Y-%m-%d')
            end_dt = datetime.strptime(args.end_date, '%Y-%m-%d')
        else:
            logging.error("日付 (--date) または期間 (--start_date, --end_date) を指定してください")
            sys.exit(1)
            
        logging.info(f"処理対象期間: {start_dt.strftime('%Y-%m-%d')} - {end_dt.strftime('%Y-%m-%d')}")
            
    except ValueError as e:
        logging.error(f"日付フォーマットエラー: {e}")
        sys.exit(1)

    # --- 1. レース結果 (results_parser) ---
    raw_race_dir = Path(paths.get('raw_html_race', 'data/raw/html/race'))
    parsed_results_dir = Path(paths.get('parsed_parquet_results', 'data/parsed/parquet/results'))
    race_files = get_target_files(raw_race_dir, start_dt, end_dt)
    
    run_parser(
        db_conn=db_conn,
        config=config,
        file_paths=race_files,
        parser_name='results_parser',
        parse_func=parse_results_html,
        output_dir=parsed_results_dir,
        partition_cols=['year', 'month', 'day']
    )

    # --- 2. 出馬表 (shutuba_parser) ---
    raw_shutuba_dir = Path(paths.get('raw_html_shutuba', 'data/raw/html/shutuba'))
    parsed_shutuba_dir = Path(paths.get('parsed_parquet_shutuba', 'data/parsed/parquet/shutuba'))
    shutuba_files = get_target_files(raw_shutuba_dir, start_dt, end_dt)

    run_parser(
        db_conn=db_conn,
        config=config,
        file_paths=shutuba_files,
        parser_name='shutuba_parser',
        parse_func=parse_shutuba_html,
        output_dir=parsed_shutuba_dir,
        partition_cols=['year', 'month', 'day']
    )

    # --- 3. 馬プロフィール (horse_info_parser - profile) ---
    raw_horse_profile_dir = Path(paths.get('raw_html_horse_profile', 'data/raw/html/horse'))
    parsed_horses_dir = Path(paths.get('parsed_parquet_horses', 'data/parsed/parquet/horses'))
    # 馬データは日付フィルタが難しいため、更新されたファイル全てを対象とする
    horse_profile_files = list(raw_horse_profile_dir.glob('*_profile_*.bin')) 
    
    run_parser(
        db_conn=db_conn,
        config=config,
        file_paths=horse_profile_files,
        parser_name='horse_profile_parser',
        parse_func=parse_horse_profile,
        output_dir=parsed_horses_dir,
        partition_cols=None # 馬データは日付パーティションなし (仕様書 3.2)
    )

    # --- 4. 馬過去成績 (horse_info_parser - performance) ---
    raw_horse_perf_dir = Path(paths.get('raw_html_horse_perf', 'data/raw/html/horse'))
    parsed_horse_perf_dir = Path(paths.get('parsed_parquet_horse_perf', 'data/parsed/parquet/horse_perf'))
    horse_perf_files = list(raw_horse_perf_dir.glob('*_perf_*.bin'))

    run_parser(
        db_conn=db_conn,
        config=config,
        file_paths=horse_perf_files,
        parser_name='horse_performance_parser',
        parse_func=parse_horse_performance,
        output_dir=parsed_horse_perf_dir,
        partition_cols=None
    )
    
    # --- 5. 血統 (pedigree_parser) ---
    raw_ped_dir = Path(paths.get('raw_html_ped', 'data/raw/html/ped'))
    parsed_ped_dir = Path(paths.get('parsed_parquet_pedigree', 'data/parsed/parquet/pedigree'))
    ped_files = list(raw_ped_dir.glob('*.bin'))
    
    run_parser(
        db_conn=db_conn,
        config=config,
        file_paths=ped_files,
        parser_name='pedigree_parser',
        parse_func=parse_pedigree_html,
        output_dir=parsed_ped_dir,
        partition_cols=None
    )
    
    # --- 6. JRAオッズ (jra_odds_parser) ---
    # _scrape_jra_odds.py がパースまで行うため、ここではJSON -> Parquet変換のみ
    # (注: 仕様書 4.4 はJSON保存まで。グルーコードでParquet変換を追加実装)
    
    logging.info("--- jra_odds_parser 実行開始 ---")
    raw_odds_dir = Path(paths.get('raw_json_jra_odds', 'data/raw/json/jra_odds'))
    parsed_odds_dir = Path(paths.get('parsed_parquet_odds', 'data/parsed/parquet/odds'))
    
    odds_files = get_target_files(raw_odds_dir, start_dt, end_dt)
    odds_data = []
    
    for file_path in odds_files:
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            race_id = data.get('race_id')
            snapshot_time = data.get('snapshot_time')
            
            for horse in data.get('horses', []):
                horse_data = horse.copy()
                horse_data['race_id'] = race_id
                horse_data['snapshot_time'] = snapshot_time
                odds_data.append(horse_data)
                
        except Exception as e:
            logging.warning(f"JRAオッズJSONの読み込み失敗 ({file_path}): {e}")
            continue

    if odds_data:
        odds_df = pd.DataFrame(odds_data)
        
        # パーティション用カラム
        odds_df['snapshot_dt'] = pd.to_datetime(odds_df['snapshot_time'])
        odds_df['year'] = odds_df['snapshot_dt'].dt.year
        odds_df['month'] = odds_df['snapshot_dt'].dt.month
        odds_df['day'] = odds_df['snapshot_dt'].dt.day
        
        try:
            parsed_odds_dir.mkdir(parents=True, exist_ok=True)
            odds_df.to_parquet(
                parsed_odds_dir,
                engine='pyarrow',
                compression='snappy',
                partition_cols=['year', 'month', 'day'],
                existing_data_behavior='overwrite_or_ignore'
            )
            logging.info(f"jra_odds_parser: {len(odds_df)}行を {parsed_odds_dir} に保存しました")
        except Exception as e:
            logging.error(f"JRAオッズ Parquet保存に失敗: {e}")
            
    else:
        logging.warning("JRAオッズデータがありませんでした")

    # 接続クローズ
    db_conn.close()
    
    logging.info("=" * 60)
    logging.info("Keiba AI データ整形パイプライン完了")
    logging.info("=" * 60)


if __name__ == '__main__':
    main()
```

#### 17.2 src/features/generate_features.py

*`data/parsed` からデータを読み込み、`FeatureEngine` を実行して `data/features` に保存します。*

```python
#!/usr/bin/env python3
"""
特徴量生成 実行スクリプト
指定された日付（または期間）のパース済みデータを読み込み、
特徴量エンジニアリングを実行し、data/features/ に保存する。

実行例:
python src/features/generate_features.py --date 2023-10-01
"""

import argparse
import logging
import sys
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, Any

import pandas as pd
import yaml

# プロジェクトルートをパスに追加
sys.path.append(str(Path(__file__).resolve().parents[1]))

from src.pipeline_core import setup_logging, load_config
from src.features.feature_engine import FeatureEngine
from src.utils.data_utils import load_parquet_data_by_date

# --- データロードヘルパー (data_utils.py にあると仮定) ---
# (仕様書 に data_utils.py が定義されているため)
# 簡易版をここに実装します。
def load_data_for_features(
    paths_config: Dict[str, Any],
    start_dt: datetime,
    end_dt: datetime
) -> Dict[str, pd.DataFrame]:
    """
    特徴量生成に必要なデータをロードする
    """
    logging.info("特徴量生成用のデータをロード中...")
    
    # 仕様書 6.2 に基づく
    
    # 1. 出馬表 (対象期間)
    shutuba_dir = Path(paths_config.get('parsed_parquet_shutuba', 'data/parsed/parquet/shutuba'))
    shutuba_df = load_parquet_data_by_date(shutuba_dir, start_dt, end_dt)
    
    if shutuba_df.empty:
        logging.error("出馬表データが見つかりません。処理を中止します。")
        return {}
        
    # 2. 過去成績 (全期間)
    results_history_dir = Path(paths_config.get('parsed_parquet_results', 'data/parsed/parquet/results'))
    # 過去走集約のため、全期間をロード
    results_history_df = load_parquet_data_by_date(results_history_dir, None, end_dt) 

    # 3. 馬プロフィール (全期間)
    horse_profiles_dir = Path(paths_config.get('parsed_parquet_horses', 'data/parsed/parquet/horses'))
    horse_profiles_df = pd.read_parquet(horse_profiles_dir)

    # 4. 血統 (全期間)
    pedigree_dir = Path(paths_config.get('parsed_parquet_pedigree', 'data/parsed/parquet/pedigree'))
    pedigree_df = pd.read_parquet(pedigree_dir)
    
    logging.info("データロード完了")
    
    return {
        "shutuba_df": shutuba_df,
        "results_history_df": results_history_df,
        "horse_profiles_df": horse_profiles_df,
        "pedigree_df": pedigree_df
    }


def main():
    """メイン実行関数"""
    parser = argparse.ArgumentParser(description='Keiba AI 特徴量生成パイプライン')
    parser.add_argument(
        '--date',
        type=str,
        help='処理対象日 (YYYY-MM-DD)。指定しない場合は期間指定が必須。'
    )
    parser.add_argument(
        '--start_date',
        type=str,
        help='処理開始日 (YYYY-MM-DD)'
    )
    parser.add_argument(
        '--end_date',
        type=str,
        help='処理終了日 (YYYY-MM-DD)'
    )
    parser.add_argument(
        '--config',
        type=str,
        default='configs/default.yaml',
        help='設定ファイルパス'
    )
    parser.add_argument(
        '--features_config',
        type=str,
        default='configs/features.yaml',
        help='特徴量設定ファイルパス'
    )
    parser.add_argument(
        '--log_level',
        type=str,
        default='INFO',
        choices=['DEBUG', 'INFO', 'WARNING', 'ERROR', 'CRITICAL'],
        help='ログレベル'
    )

    args = parser.parse_args()

    # ロギング設定
    setup_logging(level=args.log_level)

    logging.info("=" * 60)
    logging.info("Keiba AI 特徴量生成パイプライン開始")
    logging.info("=" * 60)

    # 設定ロード
    config = load_config(args.config)
    paths = config.get('paths', {})
    
    try:
        with open(args.features_config, 'r') as f:
            features_config = yaml.safe_load(f)
    except FileNotFoundError:
        logging.error(f"特徴量設定ファイルが見つかりません: {args.features_config}")
        sys.exit(1)

    # 日付範囲の決定
    try:
        if args.date:
            start_dt = datetime.strptime(args.date, '%Y-%m-%d')
            end_dt = start_dt
        elif args.start_date and args.end_date:
            start_dt = datetime.strptime(args.start_date, '%Y-%m-%d')
            end_dt = datetime.strptime(args.end_date, '%Y-%m-%d')
        else:
            logging.error("日付 (--date) または期間 (--start_date, --end_date) を指定してください")
            sys.exit(1)
            
        logging.info(f"処理対象期間: {start_dt.strftime('%Y-%m-%d')} - {end_dt.strftime('%Y-%m-%d')}")
            
    except ValueError as e:
        logging.error(f"日付フォーマットエラー: {e}")
        sys.exit(1)

    # 1. データロード
    data = load_data_for_features(paths, start_dt, end_dt)
    
    if not data:
        logging.error("データロードに失敗しました。処理を終了します。")
        sys.exit(1)

    # 2. 特徴量エンジン初期化
    engine = FeatureEngine(config=features_config)

    # 3. 特徴量生成
    features_df = engine.generate_features(
        shutuba_df=data["shutuba_df"],
        results_history_df=data["results_history_df"],
        horse_profiles_df=data["horse_profiles_df"],
        pedigree_df=data["pedigree_df"]
        # TODO: jockey_stats_df, trainer_stats_df もロード・渡す
    )
    
    if features_df.empty:
        logging.error("特徴量生成に失敗しました。")
        sys.exit(1)

    # 4. 特徴量保存
    output_dir = Path(paths.get('features_parquet', 'data/features/parquet'))
    partition_cols = features_config.get('output', {}).get('partition_by', ['year', 'month'])
    
    # 保存のために 'race_date' からパーティションカラムを生成
    if 'race_date' in features_df.columns:
        features_df['race_date'] = pd.to_datetime(features_df['race_date'])
        features_df['year'] = features_df['race_date'].dt.year
        features_df['month'] = features_df['race_date'].dt.month
        features_df['day'] = features_df['race_date'].dt.day
    else:
        logging.warning("race_date カラムが特徴量にありません。パーティション分割が不正確になる可能性があります。")
        # race_id からの復元を試みる
        if 'race_id' in features_df.columns:
            features_df['race_date_str'] = features_df['race_id'].astype(str).str[:8]
            features_df['race_date'] = pd.to_datetime(features_df['race_date_str'], format='%Y%m%d', errors='coerce')
            features_df['year'] = features_df['race_date'].dt.year
            features_df['month'] = features_df['race_date'].dt.month
            features_df['day'] = features_df['race_date'].dt.day
        
    engine.save_features(
        features_df=features_df,
        output_dir=str(output_dir),
        partition_cols=partition_cols
    )

    logging.info("=" * 60)
    logging.info("Keiba AI 特徴量生成パイプライン完了")
    logging.info("=" * 60)

if __name__ == '__main__':
    main()
```

#### 17.3 src/models/predict.py

*`data/features` から特徴量を読み込み、学習済みモデルで推論し、`data/predictions` に保存します。*

```python
#!/usr/bin/env python3
"""
モデル推論 実行スクリプト
指定された日付の特徴量を読み込み、
学習済みモデル（μ, σ, ν）で推論を実行し、
結果を data/predictions/ に保存する。

実行例:
python src/models/predict.py --date 2023-10-01 --model_dir data/models/latest
"""

import argparse
import logging
import sys
from datetime import datetime
from pathlib import Path
from typing import Dict, Any

import pandas as pd
import numpy as np
import yaml

# プロジェクトルートをパスに追加
sys.path.append(str(Path(__file__).resolve().parents[1]))

from src.pipeline_core import setup_logging, load_config
from src.utils.data_utils import load_parquet_data_by_date
from src.models.model_train import MuEstimator
from src.models.sigma_estimator import SigmaEstimator
from src.models.nu_estimator import NuEstimator


def main():
    """メイン実行関数"""
    parser = argparse.ArgumentParser(description='Keiba AI モデル推論パイプライン')
    parser.add_argument(
        '--date',
        type=str,
        required=True,
        help='推論対象日 (YYYY-MM-DD)'
    )
    parser.add_argument(
        '--model_dir',
        type=str,
        required=True,
        help='学習済みモデルが格納されているディレクトリ'
    )
    parser.add_argument(
        '--config',
        type=str,
        default='configs/default.yaml',
        help='設定ファイルパス'
    )
    parser.add_argument(
        '--models_config',
        type=str,
        default='configs/models.yaml',
        help='モデル設定ファイルパス'
    )
    parser.add_argument(
        '--log_level',
        type=str,
        default='INFO',
        choices=['DEBUG', 'INFO', 'WARNING', 'ERROR', 'CRITICAL'],
        help='ログレベル'
    )

    args = parser.parse_args()

    # ロギング設定
    setup_logging(level=args.log_level)

    logging.info("=" * 60)
    logging.info("Keiba AI モデル推論パイプライン開始")
    logging.info("=" * 60)
    logging.info(f"対象日: {args.date}, モデル: {args.model_dir}")

    # 設定ロード
    config = load_config(args.config)
    paths = config.get('paths', {})
    
    try:
        with open(args.models_config, 'r') as f:
            models_config = yaml.safe_load(f)
    except FileNotFoundError:
        logging.error(f"モデル設定ファイルが見つかりません: {args.models_config}")
        sys.exit(1)

    # 日付範囲の決定
    try:
        target_dt = datetime.strptime(args.date, '%Y-%m-%d')
    except ValueError as e:
        logging.error(f"日付フォーマットエラー: {e}")
        sys.exit(1)

    # 1. 特徴量データのロード
    features_dir = Path(paths.get('features_parquet', 'data/features/parquet'))
    features_df = load_parquet_data_by_date(features_dir, target_dt, target_dt)
    
    if features_df.empty:
        logging.warning(f"{args.date} の特徴量データが見つかりません。処理を終了します。")
        sys.exit(0)

    # 2. モデルのロード
    model_dir_path = Path(args.model_dir)
    
    # 2.1 μ (mu) モデル
    logging.info("μモデルをロード中...")
    mu_model_config = models_config.get('mu_estimator', {})
    mu_model = MuEstimator(mu_model_config)
    try:
        mu_model.load_model(str(model_dir_path / 'mu_model'))
    except Exception as e:
        logging.error(f"μモデルのロードに失敗: {e}")
        sys.exit(1)

    # 2.2 σ (sigma) モデル
    logging.info("σモデルをロード中...")
    sigma_model_config = models_config.get('sigma_estimator', {})
    sigma_model = SigmaEstimator(sigma_model_config)
    try:
        sigma_model.load_model(str(model_dir_path / 'sigma_model'))
    except Exception as e:
        logging.error(f"σモデルのロードに失敗: {e}")
        sys.exit(1)

    # 2.3 ν (nu) モデル
    logging.info("νモデルをロード中...")
    nu_model_config = models_config.get('nu_estimator', {})
    nu_model = NuEstimator(nu_model_config)
    try:
        nu_model.load_model(str(model_dir_path / 'nu_model.pkl'))
    except Exception as e:
        logging.error(f"νモデルのロードに失敗: {e}")
        sys.exit(1)
        
    logging.info("全モデルのロード完了")

    # 3. 推論実行
    logging.info("推論実行中...")
    
    predictions_list = []
    
    race_ids = features_df['race_id'].unique()
    
    for race_id in race_ids:
        race_features_df = features_df[features_df['race_id'] == race_id].copy()
        
        # 3.1 μ の予測
        mu_pred = mu_model.predict(race_features_df)
        
        # 3.2 σ の予測
        # (σモデルは馬の属性を必要とする。features_dfに 'age_mean' などが含まれていない場合、
        #  features_df から計算するか、馬のマスターデータを参照する必要がある)
        # ここでは features_df に必要なカラムが含まれていると仮定
        try:
            sigma_pred = sigma_model.predict(race_features_df)
        except Exception as e:
            logging.warning(f"レース {race_id} のσ予測に失敗: {e}。グローバル値で代替します。")
            sigma_pred = np.full(len(race_features_df), sigma_model.global_sigma or 1.0)
        
        # 3.3 ν の予測
        # (νモデルはレース単位の特徴量を必要とする)
        race_level_features = race_features_df.iloc[0:1]
        nu_pred = nu_model.predict(race_level_features)[0]
        
        # 結果を格納
        result_df = pd.DataFrame({
            'race_id': race_id,
            'horse_id': race_features_df['horse_id'],
            'horse_number': race_features_df['horse_number'],
            'mu': mu_pred,
            'sigma': sigma_pred,
            'nu': nu_pred
        })
        
        predictions_list.append(result_df)
        
    if not predictions_list:
        logging.error("推論結果がありません。")
        sys.exit(1)
        
    predictions_df = pd.concat(predictions_list, ignore_index=True)
    
    logging.info(f"{len(predictions_df)}件の推論結果を生成")

    # 4. 推論結果の保存
    output_dir = Path(paths.get('predictions_parquet', 'data/predictions/parquet'))
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # パーティション用カラム
    predictions_df['year'] = target_dt.year
    predictions_df['month'] = target_dt.month
    predictions_df['day'] = target_dt.day
    
    try:
        predictions_df.to_parquet(
            output_dir,
            engine='pyarrow',
            compression='snappy',
            partition_cols=['year', 'month', 'day'],
            existing_data_behavior='overwrite_or_ignore'
        )
        logging.info(f"推論結果を {output_dir} に保存しました")
    except Exception as e:
        logging.error(f"推論結果のParquet保存に失敗: {e}")

    logging.info("=" * 60)
    logging.info("Keiba AI モデル推論パイプライン完了")
    logging.info("=" * 60)

if __name__ == '__main__':
    main()
```

#### 17.4 src/sim/simulate_daily_races.py

*`data/predictions` から推論結果を読み込み、`RaceSimulator` を実行して `data/simulations` に保存します。*

```python
#!/usr/bin/env python3
"""
日次シミュレーション 実行スクリプト
指定された日付の推論結果（μ, σ, ν）を読み込み、
モンテカルロシミュレーションを実行し、data/simulations/ に保存する。

実行例:
python src/sim/simulate_daily_races.py --date 2023-10-01 --K 1000
"""

import argparse
import logging
import sys
from datetime import datetime
from pathlib import Path
from typing import Dict, Any

import pandas as pd
import numpy as np
import yaml

# プロジェクトルートをパスに追加
sys.path.append(str(Path(__file__).resolve().parents[1]))

from src.pipeline_core import setup_logging, load_config
from src.utils.data_utils import load_parquet_data_by_date
from src.sim.simulator import RaceSimulator


def main():
    """メイン実行関数"""
    parser = argparse.ArgumentParser(description='Keiba AI 日次シミュレーション')
    parser.add_argument(
        '--date',
        type=str,
        required=True,
        help='シミュレーション対象日 (YYYY-MM-DD)'
    )
    parser.add_argument(
        '--K',
        type=int,
        default=1000,
        help='シミュレーション回数 (K)'
    )
    parser.add_argument(
        '--model_id',
        type=str,
        default='latest',
        help='使用したモデルのID (保存用)'
    )
    parser.add_argument(
        '--config',
        type=str,
        default='configs/default.yaml',
        help='設定ファイルパス'
    )
    parser.add_argument(
        '--log_level',
        type=str,
        default='INFO',
        choices=['DEBUG', 'INFO', 'WARNING', 'ERROR', 'CRITICAL'],
        help='ログレベル'
    )

    args = parser.parse_args()

    # ロギング設定
    setup_logging(level=args.log_level)

    logging.info("=" * 60)
    logging.info("Keiba AI 日次シミュレーション開始")
    logging.info("=" * 60)
    logging.info(f"対象日: {args.date}, K={args.K}")

    # 設定ロード
    config = load_config(args.config)
    paths = config.get('paths', {})
    
    # シミュレーション設定
    sim_config = config.get('simulation', {})
    sim_config['K'] = args.K
    
    # 日付範囲の決定
    try:
        target_dt = datetime.strptime(args.date, '%Y-%m-%d')
    except ValueError as e:
        logging.error(f"日付フォーマットエラー: {e}")
        sys.exit(1)

    # 1. 推論データのロード
    predictions_dir = Path(paths.get('predictions_parquet', 'data/predictions/parquet'))
    predictions_df = load_parquet_data_by_date(predictions_dir, target_dt, target_dt)
    
    if predictions_df.empty:
        logging.warning(f"{args.date} の推論データが見つかりません。処理を終了します。")
        sys.exit(0)

    # 2. シミュレータ初期化
    simulator = RaceSimulator(config=sim_config)
    
    # 3. レースごとにシミュレーション実行
    race_ids = predictions_df['race_id'].unique()
    
    logging.info(f"{len(race_ids)} レースのシミュレーションを実行します")
    
    for i, race_id in enumerate(race_ids, 1):
        logging.info(f"--- レース {i}/{len(race_ids)} ({race_id}) ---")
        
        race_data = predictions_df[predictions_df['race_id'] == race_id].copy()
        
        if len(race_data) < 2:
            logging.warning(f"レース {race_id} の出走馬が少なすぎるためスキップします")
            continue
            
        # パラメータ準備
        mu = race_data['mu'].values
        sigma = race_data['sigma'].values
        nu = race_data['nu'].iloc[0] # レース単位で共通
        horse_numbers = race_data['horse_number'].values
        
        seed = sim_config.get('seed', 42) + i # レースごとにシードを変更
        
        try:
            # 実行
            sim_results = simulator.simulate_race(
                mu=mu,
                sigma=sigma,
                nu=nu,
                horse_numbers=horse_numbers,
                K=args.K,
                seed=seed
            )
            
            # 保存
            output_dir = Path(paths.get('simulations', 'data/simulations'))
            simulator.save_simulation(
                race_id=race_id,
                model_id=args.model_id,
                simulation_results=sim_results,
                output_dir=str(output_dir)
            )
            
        except Exception as e:
            logging.error(f"レース {race_id} のシミュレーションに失敗: {e}")
            traceback.print_exc()

    logging.info("=" * 60)
    logging.info("Keiba AI 日次シミュレーション完了")
    logging.info("=" * 60)


if __name__ == '__main__':
    main()
```

#### 17.5 src/optimizer/optimize_daily_races.py

*`data/simulations` と `data/raw/json/jra_odds` からデータを読み込み、`PortfolioOptimizer` を実行して `data/orders` に保存します。*

```python
#!/usr/bin/env python3
"""
日次ポートフォリオ最適化 実行スクリプト
指定された日付のシミュレーション結果とオッズデータを読み込み、
ポートフォリオ最適化を実行し、data/orders/ に保存する。

実行例:
python src/optimizer/optimize_daily_races.py --date 2023-10-01 --W_0 100000
"""

import argparse
import logging
import sys
from datetime import datetime
from pathlib import Path
from typing import Dict, Any
import json
import yaml

# プロジェクトルートをパスに追加
sys.path.append(str(Path(__file__).resolve().parents[1]))

from src.pipeline_core import setup_logging, load_config
from src.optimizer.optimizer import PortfolioOptimizer


def load_simulation_data(sim_dir: Path, target_date_str: str) -> List[Dict]:
    """
    指定日のシミュレーションJSONファイルをロード
    """
    results = []
    # ファイル名形式: YYYYMMDD_HHMMSS_{model_id}_{race_id}.json
    pattern = f"*{target_date_str.replace('-', '')}*.json"
    
    # 注: ファイル名に日付がないため、シミュレーション保存時に日付フォルダに分けるか、
    # race_id から日付を逆引きする必要がある。
    # ここでは簡易的に、sim_dir 直下の全JSONをスキャンし、race_id から日付を判断
    
    all_files = list(sim_dir.glob('*.json'))
    
    for file_path in all_files:
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            race_id = data.get('race_id')
            if not race_id:
                continue
                
            # race_id (YYYYPPVVRRNN) から日付を抽出
            race_date_str = race_id[:8]
            
            if race_date_str == target_date_str.replace('-', ''):
                results.append(data)
                
        except Exception as e:
            logging.warning(f"シミュレーションファイルのロード失敗 ({file_path}): {e}")
            
    return results


def load_odds_data(odds_dir: Path, race_id: str) -> Dict[str, Any]:
    """
    指定レースIDの最新JRAオッズJSONをロード
    """
    # ファイル名形式: {race_id}_{data_version}.json
    odds_files = sorted(
        list(odds_dir.glob(f"{race_id}_*.json")),
        key=lambda p: p.stat().st_mtime,
        reverse=True
    )
    
    if not odds_files:
        logging.warning(f"レース {race_id} のオッズファイルが見つかりません")
        return {}
        
    try:
        with open(odds_files[0], 'r', encoding='utf-8') as f:
            data = json.load(f)
            
        # オプティマイザが期待する形式に変換
        # { 'win': {1: 5.5, 2: 12.3}, 'place': {1: 1.8, 2: 3.2} }
        odds_map = {'win': {}, 'place': {}}
        
        for horse in data.get('horses', []):
            umaban = horse.get('umaban')
            if not umaban:
                continue
                
            if horse.get('odds_tansho'):
                odds_map['win'][umaban] = horse['odds_tansho']
            
            # 複勝オッズは下限値を使用
            if horse.get('odds_fukusho_min'):
                odds_map['place'][umaban] = horse['odds_fukusho_min']
                
        return odds_map
        
    except Exception as e:
        logging.error(f"オッズファイルのロード失敗 ({odds_files[0]}): {e}")
        return {}


def main():
    """メイン実行関数"""
    parser = argparse.ArgumentParser(description='Keiba AI 日次ポートフォリオ最適化')
    parser.add_argument(
        '--date',
        type=str,
        required=True,
        help='最適化対象日 (YYYY-MM-DD)'
    )
    parser.add_argument(
        '--W_0',
        type=float,
        default=100000.0,
        help='初期（当日）資金'
    )
    parser.add_argument(
        '--config',
        type=str,
        default='configs/default.yaml',
        help='設定ファイルパス'
    )
    parser.add_argument(
        '--optimization_config',
        type=str,
        default='configs/optimization.yaml',
        help='最適化設定ファイルパス'
    )
    parser.add_argument(
        '--log_level',
        type=str,
        default='INFO',
        choices=['DEBUG', 'INFO', 'WARNING', 'ERROR', 'CRITICAL'],
        help='ログレベル'
    )

    args = parser.parse_args()

    # ロギング設定
    setup_logging(level=args.log_level)

    logging.info("=" * 60)
    logging.info("Keiba AI 日次ポートフォリオ最適化開始")
    logging.info("=" * 60)
    logging.info(f"対象日: {args.date}, 当日資金: ¥{args.W_0:,.0f}")

    # 設定ロード
    config = load_config(args.config)
    paths = config.get('paths', {})
    
    try:
        with open(args.optimization_config, 'r') as f:
            optimization_config = yaml.safe_load(f)
    except FileNotFoundError:
        logging.error(f"最適化設定ファイルが見つかりません: {args.optimization_config}")
        sys.exit(1)

    # 1. シミュレーションデータのロード
    sim_dir = Path(paths.get('simulations', 'data/simulations'))
    simulations = load_simulation_data(sim_dir, args.date)
    
    if not simulations:
        logging.warning(f"{args.date} のシミュレーションデータが見つかりません。処理を終了します。")
        sys.exit(0)

    # 2. オプティマイザ初期化
    optimizer = PortfolioOptimizer(config=optimization_config)

    # 3. レースごとに最適化実行
    odds_dir = Path(paths.get('raw_json_jra_odds', 'data/raw/json/jra_odds'))
    
    logging.info(f"{len(simulations)} レースの最適化を実行します")
    
    for i, sim_result in enumerate(simulations, 1):
        race_id = sim_result['race_id']
        logging.info(f"--- レース {i}/{len(simulations)} ({race_id}) ---")

        # 3.1 オッズデータのロード
        odds_data = load_odds_data(odds_dir, race_id)
        
        if not odds_data.get('win'):
            logging.warning(f"レース {race_id} のオッズデータ（単勝）が不十分なためスキップします")
            continue
            
        # 3.2 最適化実行
        opt_params = optimization_config.get('optimization', {})
        c_max = opt_params.get('c_max', 0.1)
        fraction = opt_params.get('fraction', 0.5)
        
        try:
            allocation_result = optimizer.optimize(
                simulation_results=sim_result,
                odds_data=odds_data,
                W_0=args.W_0,
                c_max=c_max,
                fraction=fraction
            )
            
            # 3.3 発注（ログ）保存
            if allocation_result['total_investment'] > 0:
                output_dir = Path(paths.get('orders', 'data/orders'))
                optimizer.save_allocation(
                    race_id=race_id,
                    allocation_result=allocation_result,
                    output_dir=str(output_dir)
                )
            else:
                logging.info(f"レース {race_id}: 投資対象なし")
                
        except Exception as e:
            logging.error(f"レース {race_id} の最適化に失敗: {e}")
            traceback.print_exc()

    logging.info("=" * 60)
    logging.info("Keiba AI 日次ポートフォリオ最適化完了")
    logging.info("=" * 60)


if __name__ == '__main__':
    main()
```

---

## 17. 導入ロードマップ

### 17.1 フェーズ1: 基盤構築（0-2ヶ月）

#### Week 1-2: 環境構築
- [ ] Python 3.10+ 環境セットアップ
- [ ] 依存パッケージインストール（requirements.txt）
- [ ] ディレクトリ構造作成
- [ ] 設定ファイル作成（configs/*.yaml）
- [ ] SQLiteデータベース初期化

#### Week 3-4: データ取得基盤
- [ ] スクレイピングモジュール実装
  - [ ] `_requests_utils.py`
  - [ ] `_prepare_chrome_driver.py`
  - [ ] `run_scraping_pipeline_local.py`
- [ ] robots.txt チェック実装
- [ ] 1ヶ月分のテストデータ取得
- [ ] メタデータ保存（fetch_log）

#### Week 5-6: パーサ実装
- [ ] `results_parser.py` 実装
- [ ] `shutuba_parser.py` 実装
- [ ] `horse_info_parser.py` 実装
- [ ] パーサのユニットテスト作成
- [ ] バリデーション機能実装

#### Week 7-8: 特徴量エンジン基礎
- [ ] `feature_engine.py` 実装
- [ ] 基本特徴量生成
- [ ] 過去走集約（N=1, 3, 5）
- [ ] 特徴量のParquet保存

### 17.2 フェーズ2: モデル構築（2-4ヶ月）

#### Week 9-10: μモデル（基礎）
- [ ] LightGBM Regressor 実装
- [ ] LightGBM Ranker 実装
- [ ] アンサンブル機能実装
- [ ] モデル評価指標実装（RMSE, NDCG）

#### Week 11-12: σ・νモデル
- [ ] σ推定（Empirical Bayes）実装
- [ ] ν推定（LightGBM）実装
- [ ] モデル保存・ロード機能実装

#### Week 13-14: キャリブレーション
- [ ] Temperature Scaling 実装
- [ ] Brier Score / ECE 計算実装
- [ ] Reliability Diagram 実装
- [ ] キャリブレーション自動更新

#### Week 15-16: バックテスト基盤
- [ ] バックテスターク ラス実装
- [ ] ウォークフォワード検証実装
- [ ] 資金曲線プロット実装
- [ ] バックテスト結果保存

### 17.3 フェーズ3: シミュレーションと最適化（4-6ヶ月）

#### Week 17-18: シミュレーション
- [ ] Plackett-Luce 実装（Python版）
- [ ] Numba最適化版実装
- [ ] 複合券確率計算実装
- [ ] シミュレーション結果保存

#### Week 19-20: ポートフォリオ最適化
- [ ] Fractional Kelly 実装
- [ ] scipy.optimize 統合
- [ ] 制約条件実装
- [ ] リスク管理機能実装

#### Week 21-22: 発注管理
- [ ] order_executor 実装（自動送信無効）
- [ ] 発注JSON生成
- [ ] 手動発注ワークフロー文書化
- [ ] 発注ログ管理

#### Week 23-24: 統合テスト
- [ ] エンドツーエンドテスト
- [ ] 性能テスト
- [ ] 回帰テスト
- [ ] バグ修正

### 17.4 フェーズ4: 運用化（6ヶ月〜）

#### Week 25-26: モニタリング
- [ ] Streamlitダッシュボード実装
- [ ] メトリクス収集実装
- [ ] 異常検知実装
- [ ] アラート機能実装

#### Week 27-28: 自動化
- [ ] 日次運用スクリプト作成
- [ ] 週次運用スクリプト作成
- [ ] 月次運用スクリプト作成
- [ ] cronジョブ設定

#### Week 29-30: ドキュメント整備
- [ ] 運用マニュアル作成
- [ ] トラブルシューティングガイド作成
- [ ] API仕様書作成
- [ ] README.md 充実化

#### Week 31-32: 本番運用開始
- [ ] 小額での実運用開始（¥1,000/日）
- [ ] 2週間のモニタリング
- [ ] パフォーマンス評価
- [ ] 必要に応じてパラメータ調整

---

## 18. 付録：用語集と算出式

### 18.1 用語集

| 用語 | 説明 |
|------|------|
| μ (mu) | 馬の基礎能力（平均性能スコア） |
| σ (sigma) | 馬固有の残差分散（成績のばらつき） |
| ν (nu) | レース荒れ度（レース全体の不確実性） |
| EV (Expected Value) | 期待値 = 的中確率 × オッズ |
| Brier Score | 確率予測の精度指標（小さいほど良い） |
| ECE | Expected Calibration Error（キャリブレーション誤差） |
| NDCG | Normalized Discounted Cumulative Gain（ランキング精度指標） |
| ROI | Return on Investment（投資収益率） |
| Sharpe Ratio | リスク調整後リターン |
| Maximum Drawdown | 最大ドローダウン（資産の最大下落率） |
| Kelly Criterion | 最適賭け金比率を求める公式 |
| Plackett-Luce | 順位データの確率モデル |
| Temperature Scaling | 確率キャリブレーション手法 |

### 18.2 主要算出式

#### 18.2.1 期待値（EV）


EV_j = P_j × Odds_j - 1

where:
  P_j: 券種jの的中確率（モデル予測）
  Odds_j: 券種jのオッズ（確定オッズ）
  
例: P = 0.20, Odds = 6.0 → EV = 0.20 × 6.0 - 1 = 0.20 (20%の期待利益)


#### 18.2.2 Brier Score


Brier = (1/N) Σ(p_i - y_i)²

where:
  p_i: 予測確率
  y_i: 実際の結果（0 or 1）
  N: サンプル数
  
理想値: 0（完全予測）
許容値: < 0.20


#### 18.2.3 ECE (Expected Calibration Error)


ECE = Σ (|B_b|/N) |acc(B_b) - conf(B_b)|

where:
  B_b: ビンb内のサンプル
  acc(B_b): ビンb内の実際の正解率
  conf(B_b): ビンb内の平均予測確率
  
理想値: 0（完全キャリブレーション）
許容値: < 0.05


#### 18.2.4 Kelly Criterion


f* = (p × odds - 1) / (odds - 1)

where:
  f*: 最適投資比率（資金の何%を投資すべきか）
  p: 的中確率
  odds: オッズ倍率
  
Fractional Kelly: f = α × f* (α = 0.5推奨)


#### 18.2.5 Sharpe Ratio


Sharpe = (E[R] - R_f) / σ_R

where:
  E[R]: 期待リターン
  R_f: 無リスク金利（≈ 0）
  σ_R: リターンの標準偏差
  
年率換算: Sharpe_annual = Sharpe_daily × √252


#### 18.2.6 Maximum Drawdown


MDD = max_t ((Peak_t - Trough_t) / Peak_t)

where:
  Peak_t: 時刻tまでの最高資産額
  Trough_t: Peak_t以降の最低資産額
  
例: Peak = ¥110,000, Trough = ¥90,000
   → MDD = (110,000 - 90,000) / 110,000 = 18.2%


---

## 19. 付録：データテーブル仕様

### 19.1 races.parquet（レース結果データ）

**主キー**: `race_id` + `horse_id`

**カラム定義**:

| カラム名 | データ型 | Nullable | 説明 | 実装状況 |
|---------|---------|----------|------|----------|
| race_id | string | No | レースID (YYYYPPNNDDRR形式) | ✅ 実装済み |
| race_date | date | No | レース開催日 (YYYY-MM-DD) | ✅ 実装済み |
| distance_m | Int64 | Yes | 距離(メートル) | ✅ 実装済み |
| track_surface | string | Yes | 馬場種別（芝/ダート） | ✅ 実装済み |
| weather | string | Yes | 天候（晴/曇/雨/雪） | ✅ 実装済み |
| track_condition | string | Yes | 馬場状態（良/稍重/重/不良） | ✅ 実装済み |
| post_time | string | Yes | 発走時刻 (HH:MM) | ✅ 実装済み |
| race_name | string | Yes | レース名 | ✅ 実装済み |
| venue | string | Yes | 競馬場名（東京/中山など） | ✅ 実装済み |
| day_of_meeting | Int64 | Yes | 開催日目 | ✅ 実装済み |
| round_of_year | Int64 | Yes | 開催回数 | ✅ 実装済み |
| race_class | string | Yes | レースクラス（G1/G2/G3/OP/1600/1000/500/未勝利/新馬） | ✅ 実装済み |
| head_count | Int64 | Yes | 出走頭数 | ✅ 実装済み |
| horse_id | string | No | 馬ID (YYYYNNNNNN形式) | ✅ 実装済み |
| horse_name | string | Yes | 馬名 | ✅ 実装済み |
| finish_position | Int64 | Yes | 着順 | ✅ 実装済み |
| bracket_number | Int64 | Yes | 枠番 | ✅ 実装済み |
| horse_number | Int64 | Yes | 馬番 | ✅ 実装済み |
| sex_age | string | Yes | 性齢（牡3など） | ✅ 実装済み |
| sex | string | Yes | 性別（牡/牝/セ） | ✅ 実装済み |
| age | Int64 | Yes | 年齢 | ✅ 実装済み |
| basis_weight | float64 | Yes | 斤量(kg) | ✅ 実装済み |
| jockey_id | string | Yes | 騎手ID | ✅ 実装済み |
| jockey_name | string | Yes | 騎手名 | ✅ 実装済み |
| finish_time_str | string | Yes | タイム（文字列） | ✅ 実装済み |
| finish_time_sec | float64 | Yes | タイム（秒数） | ✅ 実装済み |
| margin_str | string | Yes | 着差（文字列） | ✅ 実装済み |
| margin_seconds | float64 | Yes | 着差（秒数換算） | ✅ 実装済み |
| passing_order_1 | Int64 | Yes | 1コーナー通過順位 | ✅ 実装済み |
| passing_order_2 | Int64 | Yes | 2コーナー通過順位 | ✅ 実装済み |
| passing_order_3 | Int64 | Yes | 3コーナー通過順位 | ✅ 実装済み |
| passing_order_4 | Int64 | Yes | 4コーナー通過順位 | ✅ 実装済み |
| last_3f_time | float64 | Yes | 上がり3ハロン(秒) | ✅ 実装済み |
| win_odds | float64 | Yes | 単勝オッズ | ✅ 実装済み |
| popularity | Int64 | Yes | 人気 | ✅ 実装済み |
| horse_weight | Int64 | Yes | 馬体重(kg) | ✅ 実装済み |
| horse_weight_change | Int64 | Yes | 馬体重増減(kg) | ✅ 実装済み |
| trainer_id | string | Yes | 調教師ID | ✅ 実装済み |
| trainer_name | string | Yes | 調教師名 | ✅ 実装済み |
| owner_name | string | Yes | 馬主名 | ✅ 実装済み |
| prize_money | Int64 | Yes | 獲得賞金（1着のみ、万円） | ✅ 実装済み |
| prize_1st | Int64 | Yes | 1着賞金（万円） | ❌ 未実装 |
| prize_2nd | Int64 | Yes | 2着賞金（万円） | ❌ 未実装 |
| prize_3rd | Int64 | Yes | 3着賞金（万円） | ❌ 未実装 |
| prize_4th | Int64 | Yes | 4着賞金（万円） | ❌ 未実装 |
| prize_5th | Int64 | Yes | 5着賞金（万円） | ❌ 未実装 |
| time_before_last_3f | float64 | Yes | 上がり3F前のタイム（派生） | ✅ 実装済み |
| popularity_finish_diff | Int64 | Yes | 人気-着順（派生） | ✅ 実装済み |
| odds_rank | Int64 | Yes | オッズ順位（派生） | ✅ 実装済み |
| cumulative_margin | float64 | Yes | 累積着差（派生） | ✅ 実装済み |
| race_avg_time | float64 | Yes | レース平均タイム（派生） | ✅ 実装済み |
| time_deviation | float64 | Yes | タイム偏差（派生） | ✅ 実装済み |
| position_change | Int64 | Yes | 位置取り変化（派生） | ✅ 実装済み |

**データ量**: 約278,000レコード（2023年10月時点）

**パーティション**: なし（単一ファイル）

### 19.2 shutuba.parquet（出馬表データ）

**主キー**: `race_id` + `horse_id`

**カラム定義**: races.parquetのサブセット + 以下の追加カラム

| カラム名 | データ型 | Nullable | 説明 | 実装状況 |
|---------|---------|----------|------|----------|
| morning_odds | float64 | Yes | 前日/当日朝オッズ | ❌ 未実装 |
| morning_popularity | Int64 | Yes | 前日/当日朝人気 | ❌ 未実装 |
| blinkers | boolean | Yes | ブリンカー装着フラグ | ❌ 未実装 |
| prediction_mark | string | Yes | 予想印（◎○▲△など） | ❌ 未実装 |

**データ量**: 約278,000レコード

### 19.3 horses.parquet（馬プロフィールデータ）

**主キー**: `horse_id`

**カラム定義**:

| カラム名 | データ型 | Nullable | 説明 | 実装状況 |
|---------|---------|----------|------|----------|
| horse_id | string | No | 馬ID | ✅ 実装済み |
| horse_name | string | Yes | 馬名 | ✅ 実装済み |
| birth_date | date | Yes | 生年月日 | ✅ 実装済み |
| sex | string | Yes | 性別 | ✅ 実装済み |
| coat_color | string | Yes | 毛色 | ✅ 実装済み |
| trainer_id | string | Yes | 調教師ID | ✅ 実装済み |
| trainer_name | string | Yes | 調教師名 | ✅ 実装済み |
| owner_name | string | Yes | 馬主名 | ✅ 実装済み |
| breeder_name | string | Yes | 生産者名 | ✅ 実装済み |
| producing_area | string | Yes | 産地 | ✅ 実装済み |
| sire_id | string | Yes | 父馬ID | ✅ 実装済み |
| sire_name | string | Yes | 父馬名 | ✅ 実装済み |
| dam_id | string | Yes | 母馬ID | ✅ 実装済み |
| dam_name | string | Yes | 母馬名 | ✅ 実装済み |
| damsire_id | string | Yes | 母父ID | ✅ 実装済み |
| damsire_name | string | Yes | 母父名 | ✅ 実装済み |

### 19.4 pedigrees.parquet（血統データ）

**主キー**: `horse_id` + `generation` + `ancestor_id`

**カラム定義**:

| カラム名 | データ型 | Nullable | 説明 | 実装状況 |
|---------|---------|----------|------|----------|
| horse_id | string | No | 馬ID | ✅ 実装済み |
| ancestor_id | string | No | 祖先馬ID | ✅ 実装済み |
| ancestor_name | string | Yes | 祖先馬名 | ✅ 実装済み |
| generation | Int64 | No | 世代（1=父母, 2=祖父母, ... 5=5代前） | ✅ 実装済み |

**データ量**: 約1,377,000レコード（5世代分）

### 19.5 features.parquet（特徴量データ）

**主キー**: `race_id` + `horse_id`

**パーティション**: `year=YYYY/month=MM/`

**カラム定義**: races.parquet + 以下の集約特徴量

| カテゴリ | カラム例 | 実装状況 |
|---------|---------|----------|
| 過去成績集約 | avg_finish_last3, avg_finish_last5, win_rate_last10 | ❌ 未実装 |
| コース適性 | venue_avg_finish, distance_cat_avg_finish | ❌ 未実装 |
| 騎手・調教師 | jockey_win_rate, trainer_win_rate, combo_win_rate | ❌ 未実装 |
| 血統特徴量 | sire_avg_finish, sire_distance_affinity | ❌ 未実装 |

---

## 20. 付記：次に行うべき実行タスク

### 20.1 即座に実行すべきタスク（優先度：高）

1. **環境セットアップ**
   bash
   # リポジトリ作成
   mkdir keibaai
   cd keibaai
   
   # 仮想環境作成
   python -m venv venv
   source venv/bin/activate  # Windows: venv\Scripts\activate
   
   # 依存パッケージインストール
   pip install -r requirements.txt
   

2. **ディレクトリ構造作成**
   bash
   # 必要なディレクトリを一括作成
   mkdir -p data/{raw/{html/{race,shutuba,horse,ped},json/jra_odds},parsed/parquet,features/parquet,models,simulations,orders,metrics,logs,metadata,master,errors}
   mkdir -p src/{modules/{preparing,parsers,features,models,sim,optimizer,executor,monitoring},utils}
   mkdir -p tests/{unit,integration,regression,performance,fixtures}
   mkdir -p configs
   mkdir -p notebooks
   mkdir -p infra/{docker,sched/cron_examples}
   mkdir -p scripts
   mkdir -p docs
   

3. **SQLiteデータベース初期化**
   
   # scripts/init_database.py
   import sqlite3
   
   conn = sqlite3.connect('data/metadata/db.sqlite3')
   cursor = conn.cursor()
   
   # テーブル作成（本仕様書のSQLスキーマを実行）
   # fetch_log, model_metadata, data_versions, parse_failures, metrics
   
   conn.commit()
   conn.close()
   

4. **設定ファイル作成**
   - `configs/scraping.yaml`
   - `configs/features.yaml`
   - `configs/models.yaml`
   - `configs/optimization.yaml`

### 20.2 短期タスク（1-2週間）

1.  **スクレイピング基盤実装**
    ```bash
    # (仕様書 19.2.1 と同様)
    # 実装順序
    1. src/pipeline_core.py
    2. src/modules/preparing/_requests_utils.py
    3. src/modules/preparing/_prepare_chrome_driver.py
    4. src/run_scraping_pipeline_local.py
    ```
2.  **パーサ実装とテスト**
    ```bash
    # (仕様書 19.2.2 と同様)
    1. src/parsers/results_parser.py
    2. src/parsers/shutuba_parser.py
    3. src/parsers/horse_info_parser.py
    4. src/parsers/pedigree_parser.py
    5. pytest tests/unit/test_parsers.py -v
    ```
3.  **1ヶ月分のデータ取得とパース [修正]**
    ```bash
    # 2023年1月のデータ取得
    python src/run_scraping_pipeline_local.py \
      --from_date 2023-01-01 \
      --to_date 2023-01-31

    # [修正] 取得したデータをパース
    python src/parsers/parse_all.py \
      --start_date 2023-01-01 \
      --end_date 2023-01-31
    ```

### 20.3 中期タスク（2-4週間）

1.  **特徴量エンジン実装 [修正]**
    ```bash
    # (仕様書 19.3.1 と同様)
    1. src/features/feature_engine.py
    2. pytest tests/unit/test_features.py -v

    # [修正] 1ヶ月分の特徴量を生成
    python src/features/generate_features.py \
      --start_date 2023-01-01 \
      --end_date 2023-01-31
    ```
2.  **モデル学習（LightGBM）**
    ```bash
    # (仕様書 19.3.2 と同様)
    # μモデル学習
    python src/models/model_train.py \
      --start_date 2023-01-01 \
      --end_date 2023-06-30 \
      --output_dir data/models/202307_mu

    # σ, νモデル学習 (※別途学習スクリプトが必要だが、ここでは省略)
    ```
3.  **推論・シミュレーション・最適化のテスト [修正]**
    ```bash
    # [修正] 1日分で推論を実行
    python src/models/predict.py \
      --date 2023-07-01 \
      --model_dir data/models/202307_mu

    # [修正] 1日分でシミュレーションを実行
    python src/sim/simulate_daily_races.py \
      --date 2023-07-01 \
      --K 1000

    # [修正] 1日分で最適化を実行 (JRAオッズの事前取得が必要)
    # python src/modules/preparing/_scrape_jra_odds.py 202306050101 (対象レースID)

    python src/optimizer/optimize_daily_races.py \
      --date 2023-07-01 \
      --W_0 100000
    ```

### 20.4 長期タスク（1-2ヶ月）

1. **シミュレーションと最適化**
   ```python
   # Numba版シミュレータ実装
   src/sim/simulator.py
   
   # テスト
   python -m pytest tests/performance/test_simulation_performance.py
   
   # 最適化実装
   src/optimizer/optimizer.py
   
   # 統合テスト
   python scripts/test_simulation_optimization.py
   ```

2. **モニタリング基盤**
   ```bash
   # Streamlitダッシュボード実装
   src/monitoring/dashboard.py
   
   # ローカルで起動
   streamlit run src/monitoring/dashboard.py
   
   # ブラウザで http://localhost:8501 にアクセス
   ```

3. **運用自動化**
   ```bash
   # スクリプト作成
   scripts/daily_operation.sh
   scripts/weekly_operation.sh
   scripts/monthly_operation.sh
   
   # cronジョブ設定
   crontab -e
   
   # 追加:
   # 0 3 * * * cd /path/to/keibaai && bash scripts/daily_operation.sh
   ```

### 20.5 重要な注意事項

#### 19.5.1 法務・規約確認（最優先）

```markdown
⚠️ **必ず実行してください**

1. netkeibaの利用規約確認
   - URL: https://www.netkeiba.com/static/pc/policy/
   - スクレイピングの可否を確認
   - 個人利用の範囲を確認

2. JRAの利用規約確認
   - URL: https://www.jra.go.jp/
   - オッズデータの取得可否を確認
   - 商用利用の制限を確認

3. 必要に応じて問い合わせ
   - サイト運営者への事前確認
   - 自動アクセスの許可取得

⚠️ 規約違反が判明した場合は直ちにスクレイピングを中止してください
```

#### 19.5.2 小額テストの重要性

```markdown
💡 **実運用開始前に必ず実施**

1. ペーパートレード（1ヶ月）
   - 実際の資金は使わない
   - 発注案のみ記録
   - システムの動作確認

2. 超小額テスト（¥1,000/日、2週間）
   - 実際の発注を手動で実行
   - 最小単位（100円）での投資
   - オッズ取得タイミングの確認

3. 小額運用（¥5,000/日、1ヶ月）
   - システムに慣れる
   - 実際の収益性を確認
   - リスク管理の検証

4. 通常運用開始（¥10,000/日〜）
   - パフォーマンスが安定してから
   - 徐々に投資額を増やす
```

#### 19.5.3 データバックアップ

```bash
#!/bin/bash
# scripts/backup_data.sh

# 週次バックアップ（毎週日曜実行）
BACKUP_DATE=$(date +%Y%m%d)
BACKUP_DIR="/mnt/external_hdd/keibaai_backup_${BACKUP_DATE}"

# 重要データをバックアップ
rsync -avz --progress \
    --include='data/metadata/' \
    --include='data/parsed/' \
    --include='data/models/' \
    --include='data/features/' \
    --exclude='data/raw/' \
    data/ "${BACKUP_DIR}/"

# 古いバックアップ削除（60日以上前）
find /mnt/external_hdd/ -name "keibaai_backup_*" -mtime +60 -exec rm -rf {} \;

echo "バックアップ完了: ${BACKUP_DIR}"
```

### 20.6 トラブルシューティングガイド

#### 問題1: スクレイピングがBAN される

```markdown
症状: HTTP 403, 429, または接続タイムアウト

対処法:
1. 遅延時間を増やす（MIN_SLEEP_SECONDS を 10秒に）
2. User-Agentをローテーション
3. 取得頻度を減らす（1日1回に制限）
4. 手動でデータ取得を検討
```

#### 問題2: パースエラーが多発

```markdown
症状: parse_failures テーブルにエラーが蓄積

対処法:
1. エラーログを確認（data/errors/parse_failures/）
2. HTML構造が変更されていないか確認
3. サンプルHTMLで手動デバッグ
4. パーサのバージョンアップ
```

#### 問題3: モデル精度が低い

```markdown
症状: NDCG < 0.5, Top-1 Accuracy < 10%

対処法:
1. 特徴量の見直し（feature_importance確認）
2. データ期間の拡張（最低1年分）
3. ハイパーパラメータチューニング
4. キャリブレーションの再実行
```

#### 問題4: 最適化が収束しない

```markdown
症状: scipy.optimize.minimize が失敗

対処法:
1. 初期値の見直し（x0）
2. 制約条件の緩和
3. 最適化手法の変更（SLSQP → Trust-constr）
4. 投資候補の削減（EV閾値を上げる）
```

#### 問題5: ディスク容量不足

```markdown
症状: OSError: No space left on device

対処法:
1. 古いraw HTMLを削除または圧縮
2. ログファイルのローテーション
3. 外付けHDDの追加
4. 不要なシミュレーション結果を削除
```

### 20.7 推奨開発フロー

```mermaid
graph TD
    A[1. 環境構築] --> B[2. 小規模データ取得<br/>1週間分]
    B --> C[3. パーサ実装とテスト]
    C --> D[4. 特徴量生成<br/>基本のみ]
    D --> E[5. 簡易モデル学習<br/>LightGBM Regressor]
    E --> F[6. バックテスト<br/>1ヶ月分]
    F --> G{精度OK?}
    G -->|No| H[特徴量追加<br/>モデル改善]
    H --> E
    G -->|Yes| I[7. シミュレーション実装]
    I --> J[8. 最適化実装]
    J --> K[9. 小額ペーパートレード<br/>1ヶ月]
    K --> L{収益性OK?}
    L -->|No| M[パラメータ調整]
    M --> K
    L -->|Yes| N[10. 実運用開始<br/>超小額から]
```

---

## 20. 最終チェックリスト

### 20.1 実装前の確認事項

- [ ] Python 3.10+ がインストールされている
- [ ] 外付けHDD/NAS が準備されている（推奨容量: 500GB以上）
- [ ] Google Chrome がインストールされている
- [ ] インターネット接続が安定している
- [ ] 利用規約を確認済み（netkeiba, JRA）

### 20.2 基盤実装の確認事項

- [ ] ディレクトリ構造が作成されている
- [ ] SQLiteデータベースが初期化されている
- [ ] 設定ファイル（configs/*.yaml）が作成されている
- [ ] requirements.txt の全パッケージがインストールされている
- [ ] スクレイピングパイプラインが動作する
- [ ] パーサが正しくデータを抽出できる
- [ ] ユニットテストがパスする

### 20.3 モデル実装の確認事項

- [ ] 特徴量が正しく生成される
- [ ] LightGBMモデルが学習できる
- [ ] モデルの保存・ロードができる
- [ ] バックテストが実行できる
- [ ] キャリブレーションが機能する
- [ ] シミュレーションが高速に動作する（1レース1秒以内）
- [ ] 最適化が収束する

### 20.4 運用開始前の確認事項

- [ ] Streamlitダッシュボードが動作する
- [ ] メトリクス収集が機能する
- [ ] アラート機能が動作する
- [ ] 日次運用スクリプトが動作する
- [ ] バックアップスクリプトが動作する
- [ ] cronジョブが設定されている
- [ ] 運用マニュアルが整備されている

### 20.5 実運用の確認事項

- [ ] ペーパートレードで1ヶ月間の動作確認完了
- [ ] 超小額テスト（¥1,000/日）で2週間の動作確認完了
- [ ] パフォーマンスが期待値を満たしている
- [ ] リスク管理が機能している
- [ ] 手動発注のワークフローが確立している

---

## まとめ

本仕様書は、**継続的な金銭支出をゼロに抑える**ことを最優先としたAI競馬予測システムの完全な設計書です。

### 主要な特徴

1. **ゼロコスト運用**: クラウドサービスや有料APIを一切使用せず、ローカル環境のみで完結
2. **法務遵守**: スクレイピングは慎重に行い、robots.txt を尊重
3. **段階的実装**: 小規模から始めて徐々に拡張可能
4. **再現性**: 全てのデータとモデルをバージョン管理
5. **安全性**: 自動発注はデフォルト無効、手動確認を必須化

### 重要な注意事項

⚠️ **法務リスク**: スクレイピングは必ず利用規約を確認してから実行してください。規約違反が判明した場合は直ちに中止してください。

⚠️ **投資リスク**: 本システムは研究・教育目的です。実際の運用は自己責任で行い、小額から始めることを強く推奨します。

⚠️ **技術的制約**: 有料サービスを使わないため、以下の制約があります：
- スクレイピングBAN のリスク
- 計算速度の制限
- リアルタイム性の制約
- インフラの可用性

### 成功のための鍵

1. **段階的な実装**: 一度に全てを実装せず、フェーズごとに確実に進める
2. **小額でのテスト**: 実運用前に必ずペーパートレードと小額テストを実施
3. **継続的な改善**: モデルとパラメータを定期的に見直す
4. **リスク管理**: 投資額を常に管理し、損失を限定する
5. **法務遵守**: 常に規約とルールを守る

本仕様書に従って実装することで、**費用をかけずに機械学習ベースの競馬予測システムを構築**できます。ただし、実際の収益性は保証されません。あくまで研究・学習目的として活用してください。