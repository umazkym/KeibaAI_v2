#スクレイピングデータの実装方法の変更
ディレクトリに HTML データを収集するためのスクレイピング手法について、詳細かつ具体的に解説します。
このプロジェクトにおけるスクレイピングは、主に run_scraping_pipeline.py というパイプラインスクリプトによって実行されます。このスクリプトは、必要なデータを体系的に収集するため、複数のフェーズ（ステップ）に分かれています。
使用する主なライブラリは以下の通りです。

- requests: 静的な HTML ページの取得（高速）
- Selenium: JavaScript によって動的にレンダリングされるページの取得（低速だが高機能）
- BeautifulSoup: 取得した HTML の解析
- pandas: 取得した ID やキャッシュログの管理
  スクレイピングの核心は、\*\*「いかにして対象サイト（netkeiba.com 等）のアクセス制限（BAN）を回避するか」\*\*にあります。そのための戦略がコードの随所に組み込まれています。

---

## スクレイピングの全体像と BAN 回避戦略

data/html 以下のディレクトリ構造と、そこに対応するデータの収集方法は以下のようになっています。
| 保存先ディレクトリ | 収集するデータ | 使用技術 | 該当ステップ |
| :--- | :--- | :--- | :--- |
| data/html/race/ | レース結果ページ | requests (静的) | ステップ 2 |
| data/html/shutuba/ | 出馬表ページ | requests (静的) | ステップ 2 |
| data/html/horse/ | 馬のプロフィール \_profile.bin) | requests (静的) | ステップ 4 |
| data/html/horse/ | 馬の過去成績 \_perf.bin) | requests (AJAX 模倣) | ステップ 4 |
| data/html/ped/ | 馬の血統ページ | Selenium (JS 待機) | ステップ 4 |
| data/jra_odds_snapshots/ | (補足) JRA 公式オッズ | Selenium (画面遷移) | (補足) |

### BAN 回避のための共通戦略

実装するすべてのスクレイピング関数において、以下の対策を講じることが不可欠です。

#### 1\. 堅牢な Requests セッションの確立

modules/preparing/\_requests_utils.py や run_scraping_pipeline.py に見られるように、リトライ機能とランダムな User-Agent を持つ requests.Session を作成します。
python
import requests
import random
from urllib3.util.retry import Retry
from requests.adapters import HTTPAdapter

# 偽装する User-Agent のリスト

USER_AGENTS = [
"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36",
"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36",
"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36",
]

# リクエスト間の待機時間 (秒)

MIN_SLEEP_SECONDS = 2.5
MAX_SLEEP_SECONDS = 5.0

# リトライ設定

REQUESTS_RETRIES = 5
REQUESTS_BACKOFF_FACTOR = 0.5
REQUESTS_STATUS_FORCELIST = (500, 502, 503, 504, 429) # サーバーエラーやレート制限時にリトライ
def get_robust_session() -> requests.Session:
"""堅牢な Session を返す"""
session = requests.Session()
retries = Retry(
total=REQUESTS_RETRIES,
backoff_factor=REQUESTS_BACKOFF_FACTOR,
status_forcelist=REQUESTS_STATUS_FORCELIST,
allowed_methods={"GET", "POST"},
)
adapter = HTTPAdapter(max_retries=retries)
session.mount("http://", adapter)
session.mount("https://", adapter)
return session

#### 2\. 待機時間（スリープ）の挿入

fetch_html_robust_get 関数のように、リクエストの直前に必ずランダムなスリープを入れます。

```python
import time
def fetch_html_robust_get(url: str, session: requests.Session, ...) -> Optional[bytes]:
    try:
        # ランダムな待機
        sleep_time = random.uniform(MIN_SLEEP_SECONDS, MAX_SLEEP_SECONDS)
        time.sleep(sleep_time)
        # ランダムなUser-Agentを設定
        request_headers = {"User-Agent": random.choice(USER_AGENTS)}

        response = session.get(url, headers=request_headers, ...)

        # (中略) IP BANの可能性があるHTTP 400エラーの特別処理
        if response.status_code == 400:
            logger.critical(f"(GET) HTTP 400 Error. IP BANの可能性: {url}")
            time.sleep(HTTP_400_SLEEP_SECONDS) # 60秒など長めに待機
            return None

        response.raise_for_status()
        return response.content # HTMLはbytes形式で返す
    except requests.exceptions.RequestException as e:
        logger.warning(f"(GET) HTML取得失敗 ({url}): {e}")
        return None

#### 3\. 自動化検出を回避するSelenium WebDriver
`modules/preparing/_prepare_chrome_driver.py` にあるように、Selenium使用時は自動化フラグを隠蔽します。
python
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
def prepare_chrome_driver(headless=True):
    options = Options()
    if headless:
        options.add_argument('--headless=new')
    options.add_argument('--no-sandbox')
    options.add_argument('--disable-dev-shm-usage')
    options.add_argument(f"user-agent={random.choice(USER_AGENTS)}") # User-Agent偽装

    # --- 自動化検出の回避 ---
    options.add_argument("--disable-blink-features=AutomationControlled")
    options.add_experimental_option("excludeSwitches", ["enable-automation"])
    options.add_experimental_option('useAutomationExtension', False)
    # -------------------------

    driver = webdriver.Chrome(options=options)

    # --- WebDriverフラグの隠蔽 ---
    driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")

    return driver
``
-----
## ステップ・バイ・ステップによるHTML収集詳解
run_scraping_pipeline.py に基づき、データの流れを追って解説します。
### ステップ1: スクレイピング対象のIDを特定する (フェーズ0a)
まず、どのレースのデータを取得するかを特定します。
#### 1-a. 開催日 (kaisai\_date) の取得
  * **関数:** scrape_kaisai_date
  * **技術:** requests
  * **対象URL:** UrlPaths.CALENDAR_URL https://race.netkeiba.com/top/calendar.html`)
  * **処理:**
    1.  指定された期間（例: 2020-01-01〜2020-10-31）の月ごとにカレンダーURLにアクセスします (例: ?year=2020&month=1)。
    2.  get_robust_session と fetch_html_robust_get を使い、HTML (bytes) を取得します。
    3.  BeautifulSoup(html_content, "lxml", from_encoding='euc-jp') でHTMLを解析します。
    4.  soup.find('table', class_='Calendar_Table') でカレンダーテーブルを特定します。
    5.  テーブル内の全 <a> タグを検索しhref 属性に kaisai_date=(\d+) のパターンを持つリンクを探します。
    6.  正規表現で YYYYMMDD 形式の開催日文字列を抽出し、リストにまとめます。
#### 1-b. レースID (race\_id) の取得
  * **関数:** scrape_race_id_list
  * **技術:** Selenium (ページがJavaScriptで描画される可能性があるため)
  * **対象URL:** UrlPaths.RACE_LIST_URL https://race.netkeiba.com/top/race_list.html)
  * **処理:**
    1.  prepare_chrome_driver でWebDriverを起動します。
    2.  ステップ1-aで取得した kaisai_date ごとに、レース一覧ページにアクセスします (例: ?kaisai_date=20200105)。
    3.  WebDriverWait を使い、レース一覧のコンテナ（例: EC.presence_of_element_located((By.CLASS_NAME, 'RaceList_Box'))）が表示されるまで明示的に待機します。
    4.  コンテナ内の全 <a> タグを取得します。
    5.  href 属性に result.html?race_id= または shutuba.html?race_id= が含まれるリンクを探します。
    6.  正規表現 r'(?:shutuba|result)\.html\?race_id=(\d+)' を使い、12桁の race_id (例: 202006010101) を抽出します。
    7.  WebDriverがタイムアウトなどで応答しなくなった場合に備えtry-except ブロック内でWebDriverを再起動するロジックを組み込みます。
    8.  最後に driver.quit() でブラウザを閉じます。
-----
### ステップ2: レース結果と出馬表のHTMLを保存する (フェーズ0b)
race_id が特定できたら、それに対応する静的HTMLページを requests で高速に取得します。
#### 2-a. レース結果HTMLの保存 data/html/race/)
  * **関数:** scrape_html_race
  * **技術:** requests
  * **対象URL:** UrlPaths.RACE_URL https://db.netkeiba.com/race/)
  * **処理:**
    1.  race_id (例: 202006010101) ごとにループします。
    2.  保存先ファイルパス (例: data/html/race/202006010101.bin) が既に存在する場合、処理をスキップします skip=True の場合)。
    3.  fetch_html_robust_get を使い、対象URL (例: https://db.netkeiba.com/race/202006010101) にアクセスし、HTML (bytes) を取得します。
    4.  （省略可能だが推奨BeautifulSoup でHTMLを軽くチェックし、有効なページか（例: div.data_intro が存在するか）を確認します。
    5.  取得した html_content (bytes) を、バイナリ書き込みモード 'wb') で .bin ファイルとして保存します。
#### 2-b. 出馬表HTMLの保存 data/html/shutuba/)
  * **関数:** scrape_html_shutuba
  * **技術:** requests
  * **対象URL:** UrlPaths.SHUTUBA_TABLE https://race.netkeiba.com/race/shutuba.html)
  * **処理:**
    1.  レース結果と同様に race_id ごとにループします。
    2.  保存先ファイルパス (例: data/html/shutuba/202006010101.bin) の存在をチェックしスキップします。
    3.  fetch_html_robust_get を使い、対象URL (例: https://race.netkeiba.com/race/shutuba.html?race_id=202006010101) にアクセスします。
    4.  有効なページか（例: table.Shutuba_Table が存在するか）をチェックします。
    5.  取得した html_content (bytes) をバイナリで保存します。
-----
### ステップ3: 出走した全馬のIDを集約する (フェーズ1)
次に、取得したHTML（ローカルファイル）をスキャンして、馬のデータ（ステップ4）を取得するための horse_id を集めます。
  * **関数:** phase_1_get_all_horse_ids
  * **技術:** BeautifulSoup (ローカルファイル読み込み)
  * **対象:** data/html/race/*.bin
  * **処理:**
    1.  このフェーズでは**ネットワークアクセスは一切行いません**。
    2.  ステップ2で取得した data/html/race/ ディレクトリ内の .bin ファイルを読み込みます。
    3.  BeautifulSoup(html, "lxml", from_encoding='euc-jp') で解析します。
    4.  soup_race.find("table", attrs={"summary": "レース結果"}) でレース結果テーブルを特定します。
    5.  テーブル内の href が /horse/ で始まる全 <a> タグを検索します。
    6.  正規表現 r"/horse/(\d+)" で horse_id (例: 2017103291) を抽出します。
    7.  set() を使い、重複のない horse_id の集合を作成します。
-----
### ステップ4: 馬データをインテリジェントに取得する (フェーズ2)
最も複雑なステップです。ステップ3で集めた全 horse_id を対象に、馬の「プロフィール」「過去成績」「血統」のデータを取得します。
#### 4-a. キャッシュ戦略 (差分取得)
馬のデータは頻繁には更新されないため、毎回全件取得するのは非効率でBANリスクも高まります。
  * **関数:** phase_2_intelligent_fetch
  * **技術:** pandas
  * **対象:** data/master/horse_cache_log.csv
  * **処理:**
    1.  load_cache_log で horse_cache_log.csv を pandas.DataFrame として読み込みます。
    2.  CSVには horse_id と last_updated (最終更新日時) が記録されています。
    3.  ステップ3で集めた horse_id とCSVの last_updated を比較します。
    4.  CACHE_TTL_DAYS (例: 7日) を超えて古い馬、またはCSVに存在しない馬（新規の馬）のみを「取得対象 stale_horses)」とします。
#### 4-b. プロフィールと過去成績の取得 data/html/horse/)
  * **関数:** fetch_and_cache_horse_data
  * **技術:** requests (静的) および requests (AJAX模倣)
  * **処理 (取得対象の馬のみ):**
    1.  **プロフィール (静的):**
          * fetch_html_robust_get を使いUrlPaths.HORSE_URL + horse_id (例: https://db.netkeiba.com/horse/2017103291) にアクセスします。
          * 取得したHTML (bytes) を data/html/horse/{horse_id}_profile.bin に保存します。
    2.  **過去成績 (AJAX):**
          * これは通常のページではなくX-Requested-With: XMLHttpRequest ヘッダを要求するAPI（実質的にはHTML断片を返す）です。
          * fetch_dynamic_html_from_api (または fetch_html_robust_get に特殊なヘッダを追加して) を使用します。
          * 対象API URL: https://db.netkeiba.com/horse/ajax_horse_results.html
          * **重要:** ヘッダに {"Referer": (プロフィールのURL), "X-Requested-With": "XMLHttpRequest"} を追加しparams={'id': horse_id} を設定します。
          * 返り値はJSON形式 (例: {"status": "OK", "data": "<html>...</html>"}) です。
          * JSONをパースし"data" キーの値（HTML文字列）を取り出します。
          * perf_html_fragment_str.encode('euc-jp', errors='replace') のようにエンコードしdata/html/horse/{horse_id}_perf.bin に保存します。
#### 4-c. 血統ページの取得 data/html/ped/)
  * **関数:** fetch_and_cache_pedigree_data
  * **技術:** Selenium (JSレンダリング待機)
  * **処理 (取得対象の馬のみ):**
    1.  血統表 blood_table) はJavaScriptによってページ読み込み後に描画されます。そのため requests では取得できません。
    2.  prepare_chrome_driver で起動したWebDriverを使います。
    3.  driver.get(UrlPaths.PED_URL + horse_id) (例: https://db.netkeiba.com/horse/ped/2017103291) にアクセスします。
    4.  WebDriverWait(driver, SELENIUM_WAIT_TIMEOUT) を使い、血統表（例: EC.presence_of_element_located((By.CLASS_NAME, 'blood_table'))）が描画されるまで最大30秒待ちます。
    5.  待機後driver.page_source (JSによってレンダリングされた**後**の完全なHTML) を取得します。
    6.  html_content_str.encode('euc-jp', errors='replace') のようにエンコードしdata/html/ped/{horse_id}.bin に保存します。
#### 4-d. キャッシュログの更新
  * **関数:** save_cache_log
  * **処理:** 4-b, 4-c が両方成功したらpandas.DataFrame 上の該当 horse_id の last_updated を現在日時に更新しhorse_cache_log.csv に即時上書き保存します。
-----
### (補足) JRAオッズの収集
run_scraping_pipeline.py には含まれていませんがmodules/preparing/_scrape_jra_odds.py はJRA公式サイトからオッズを取得するために、さらに高度な Selenium の技術を使用しています。
  * **関数:** get_jra_odds_html_by_selenium
  * **技術:** Selenium (連続的な画面遷移)
  * **処理:**
    1.  JRAトップページ https://www.jra.go.jp) にアクセスします。
    2.  race_id (例: 202510250501) から競馬場コード(05=東京)とレース番号(01=1R)を特定します。
    3.  WebDriverWait と EC.element_to_be_clickable を駆使し、
        1.  「オッズ」リンクをクリック
        2.  「東京」リンクをクリック
        3.  「1R」リンクをクリック
        4.  「単勝・複勝」リンクをクリック
            という連続操作をエミュレートします。
    4.  最終的に到達したオッズページの driver.page_source を取得します。
    5.  parse_jra_odds_html でこのHTMLを解析しsave_jra_odds_snapshot で data/jra_odds_snapshots/{race_id}_jra_odds.json として保存します。
以上のステップによりdata/html 以下の各ディレクトリに必要なHTMLデータが体系的に収集・保存されます。
#主な参考コードパス
スクレイピング全体の流れ（パイプライン）
run_scraping_pipeline.py
解説: このファイルが全体の司令塔です。フェーズ0（レースID取得、HTML保存）、フェーズ1（馬IDスキャン）、フェーズ2（馬データの差分取得）のすべてを含んでいます。
BAN回避（requests）
modules/preparing/_requests_utils.py
解説: requests 使用時のリトライ、スリープ、User-Agentランダム化、HTTP 400エラー対応などのBAN回避策が実装されています。run_scraping_pipeline.py 内の get_robust_session や fetch_html_robust_get も同様の役割を担っています。
BAN回避（Selenium）
modules/preparing/_prepare_chrome_driver.py
解説: Selenium のWebDriver起動時に、自動化検出を回避するためのオプション（AutomationControlledフラグの無効化など）を設定しています。
IDリストの取得（Selenium / requests）
modules/preparing/_scrape_race_id_list.py
解説: scrape_kaisai_date（開催日取得、requests使用）と scrape_race_id_list（レースID取得、Selenium使用）の実装が含まれます。run_scraping_pipeline.py 内にも同名・同機能の関数が定義されています。
JRAオッズの取得（Selenium画面遷移）
modules/preparing/_scrape_jra_odds.py
解説: JRA公式サイトのトップページから複数のリンクをクリックし、目的のオッズページに到達する複雑なSelenium操作が実装されています。
URLと保存先の定義
modules/constants/_url_paths.py
modules/constants/_local_paths.py
解説: スクレイピング対象のURLやHTMLの保存先ディレクトリパスが定数としてまとめられています。
```
