#!/usr/bin/env python3
"""
ãƒ‡ãƒãƒƒã‚°ã‚¹ã‚¯ãƒªãƒ—ãƒˆ03: ãƒ‘ãƒ¼ã‚¹å¤±æ•—åŸå› ã®å¾¹åº•èª¿æŸ»

ãªãœ20,157ä»¶ä¸­1,000ä»¶ã—ã‹ãƒ‘ãƒ¼ã‚¹ã•ã‚Œã¦ã„ãªã„ã®ã‹ã‚’ç‰¹å®šã—ã¾ã™
"""

import os
import re
from pathlib import Path
from datetime import datetime
from collections import defaultdict, Counter
import pandas as pd

def analyze_html_files():
    """RAW HTMLãƒ•ã‚¡ã‚¤ãƒ«ã®è©³ç´°åˆ†æ"""
    print("=" * 80)
    print("ğŸ“ RAW HTMLãƒ•ã‚¡ã‚¤ãƒ«ã®è©³ç´°åˆ†æ")
    print("=" * 80)

    html_base = Path("keibaai/data/raw/html")

    # ãƒ¬ãƒ¼ã‚¹çµæœHTMLã®åˆ†æ
    race_dir = html_base / "race"
    if race_dir.exists():
        race_files = list(race_dir.glob("*.bin"))
        race_ids = [f.stem for f in race_files]

        # å¹´æœˆåˆ¥ã®é›†è¨ˆ
        year_month_count = defaultdict(int)
        year_count = defaultdict(int)

        for race_id in race_ids:
            # race_id = YYYYPPNNDDRR
            if len(race_id) == 12 and race_id.isdigit():
                year = race_id[:4]
                month = race_id[4:6]
                year_count[year] += 1
                year_month_count[f"{year}-{month}"] += 1

        print(f"\nğŸ“Š ãƒ¬ãƒ¼ã‚¹çµæœHTMLãƒ•ã‚¡ã‚¤ãƒ«: åˆè¨ˆ {len(race_files)} ä»¶")
        print("\nå¹´åˆ¥ãƒ•ã‚¡ã‚¤ãƒ«æ•°:")
        for year in sorted(year_count.keys()):
            print(f"  {year}å¹´: {year_count[year]:>5} ä»¶")

        print("\nå¹´æœˆåˆ¥ãƒ•ã‚¡ã‚¤ãƒ«æ•°ï¼ˆä¸Šä½20ä»¶ï¼‰:")
        sorted_ym = sorted(year_month_count.items(), key=lambda x: x[0])
        for ym, count in sorted_ym[:20]:
            print(f"  {ym}: {count:>4} ä»¶")

    # é¦¬ãƒ‡ãƒ¼ã‚¿HTMLã®åˆ†æ
    horse_dir = html_base / "horse"
    if horse_dir.exists():
        horse_files = list(horse_dir.glob("*.bin"))
        profile_files = [f for f in horse_files if "_profile" in f.name]
        perf_files = [f for f in horse_files if "_perf" in f.name]

        print(f"\nğŸ“Š é¦¬ãƒ‡ãƒ¼ã‚¿HTMLãƒ•ã‚¡ã‚¤ãƒ«: åˆè¨ˆ {len(horse_files)} ä»¶")
        print(f"  - ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«: {len(profile_files)} ä»¶")
        print(f"  - éå»æˆç¸¾: {len(perf_files)} ä»¶")


def analyze_parsed_parquet():
    """ãƒ‘ãƒ¼ã‚¹æ¸ˆã¿Parquetã®è©³ç´°åˆ†æ"""
    print("\n" + "=" * 80)
    print("ğŸ“Š ãƒ‘ãƒ¼ã‚¹æ¸ˆã¿Parquetã®è©³ç´°åˆ†æ")
    print("=" * 80)

    parquet_base = Path("keibaai/data/parsed/parquet")

    # races.parquet ã®åˆ†æ
    races_file = parquet_base / "races" / "races.parquet"
    if races_file.exists():
        df = pd.read_parquet(races_file)
        print(f"\nğŸ“„ races.parquet:")
        print(f"  ç·è¡Œæ•°: {len(df):,}")

        # race_idã‹ã‚‰å¹´æœˆã‚’æŠ½å‡º
        df['year'] = df['race_id'].astype(str).str[:4]
        df['year_month'] = df['race_id'].astype(str).str[:6]

        print("\n  å¹´åˆ¥ãƒ¬ãƒ¼ã‚¹æ•°:")
        for year, count in df['year'].value_counts().sort_index().items():
            print(f"    {year}å¹´: {count:>5} ãƒ¬ãƒ¼ã‚¹")

        print("\n  å¹´æœˆåˆ¥ãƒ¬ãƒ¼ã‚¹æ•°ï¼ˆä¸Šä½20ä»¶ï¼‰:")
        for ym, count in df['year_month'].value_counts().sort_index().head(20).items():
            year = ym[:4]
            month = ym[4:6]
            print(f"    {year}-{month}: {count:>4} ãƒ¬ãƒ¼ã‚¹")

        # æ—¥ä»˜ç¯„å›²ã®ç¢ºèª
        print(f"\n  æ—¥ä»˜ç¯„å›²:")
        print(f"    æœ€å°: {df['race_date'].min()}")
        print(f"    æœ€å¤§: {df['race_date'].max()}")

        # ãƒ¦ãƒ‹ãƒ¼ã‚¯ãƒ¬ãƒ¼ã‚¹æ•°
        unique_races = df['race_id'].nunique()
        print(f"\n  ãƒ¦ãƒ‹ãƒ¼ã‚¯ãƒ¬ãƒ¼ã‚¹æ•°: {unique_races:,}")
        print(f"  ãƒ¬ãƒ¼ã‚¹ã‚ãŸã‚Šå¹³å‡é ­æ•°: {len(df) / unique_races:.2f}")


def check_parse_logs():
    """ãƒ‘ãƒ¼ã‚¹å‡¦ç†ã®ãƒ­ã‚°ã‚’ç¢ºèª"""
    print("\n" + "=" * 80)
    print("ğŸ“‹ ãƒ‘ãƒ¼ã‚¹ãƒ­ã‚°ã®ç¢ºèª")
    print("=" * 80)

    log_dir = Path("keibaai/data/logs")
    if log_dir.exists():
        log_files = sorted(log_dir.glob("*.log"))
        if log_files:
            print(f"\nè¦‹ã¤ã‹ã£ãŸãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«: {len(log_files)} ä»¶")
            for log_file in log_files[-5:]:  # æœ€æ–°5ä»¶
                print(f"  - {log_file.name} ({log_file.stat().st_size} bytes)")

            # æœ€æ–°ã®ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã‚€
            latest_log = log_files[-1]
            print(f"\nğŸ“„ æœ€æ–°ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã®å†…å®¹ï¼ˆ{latest_log.name}ï¼‰:")
            print("-" * 80)
            with open(latest_log, 'r', encoding='utf-8', errors='ignore') as f:
                lines = f.readlines()
                # ã‚¨ãƒ©ãƒ¼è¡Œã‚’æŠ½å‡º
                error_lines = [line for line in lines if 'ERROR' in line or 'Exception' in line]
                if error_lines:
                    print("âš ï¸ ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸:")
                    for line in error_lines[:20]:  # æœ€åˆã®20ä»¶
                        print(f"  {line.strip()}")
                else:
                    print("âœ… ã‚¨ãƒ©ãƒ¼ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")

                # æœ€å¾Œã®20è¡Œã‚’è¡¨ç¤º
                print("\næœ€å¾Œã®20è¡Œ:")
                for line in lines[-20:]:
                    print(f"  {line.strip()}")
        else:
            print("âŒ ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
    else:
        print("âŒ ãƒ­ã‚°ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã—ã¾ã›ã‚“")


def check_parse_script_config():
    """ãƒ‘ãƒ¼ã‚¹å‡¦ç†ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®è¨­å®šç¢ºèª"""
    print("\n" + "=" * 80)
    print("âš™ï¸ ãƒ‘ãƒ¼ã‚¹å‡¦ç†ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®è¨­å®šç¢ºèª")
    print("=" * 80)

    script_path = Path("keibaai/src/run_parsing_pipeline_local.py")
    if script_path.exists():
        print(f"\nğŸ“„ {script_path}")
        with open(script_path, 'r', encoding='utf-8') as f:
            content = f.read()

            # æ—¥ä»˜ç¯„å›²ã®è¨­å®šã‚’æ¢ã™
            date_patterns = [
                r'start_date\s*=\s*["\'](\d{4}-\d{2}-\d{2})["\']',
                r'end_date\s*=\s*["\'](\d{4}-\d{2}-\d{2})["\']',
                r'date_range\s*=\s*\(',
                r'limit\s*=\s*(\d+)',
            ]

            print("\nğŸ” è¨­å®šå€¤ã®æ¤œç´¢:")
            for pattern in date_patterns:
                matches = re.findall(pattern, content)
                if matches:
                    print(f"  {pattern}: {matches}")

            # ãƒ•ã‚¡ã‚¤ãƒ«å†…ã« "1000" ã¨ã„ã†æ•°å­—ãŒã‚ã‚‹ã‹ç¢ºèª
            if "1000" in content:
                print("\nâš ï¸ ã‚¹ã‚¯ãƒªãƒ—ãƒˆå†…ã« '1000' ã¨ã„ã†æ•°å€¤ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸ")
                # è©²å½“è¡Œã‚’è¡¨ç¤º
                lines = content.split('\n')
                for i, line in enumerate(lines, 1):
                    if "1000" in line:
                        print(f"  è¡Œ{i}: {line.strip()}")
    else:
        print("âŒ ãƒ‘ãƒ¼ã‚¹å‡¦ç†ã‚¹ã‚¯ãƒªãƒ—ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")


def check_config_files():
    """è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®ç¢ºèª"""
    print("\n" + "=" * 80)
    print("ğŸ“ è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®ç¢ºèª")
    print("=" * 80)

    config_dir = Path("keibaai/configs")
    if config_dir.exists():
        config_files = list(config_dir.glob("*.yaml"))
        print(f"\nè¦‹ã¤ã‹ã£ãŸè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«: {len(config_files)} ä»¶")
        for config_file in config_files:
            print(f"  - {config_file.name}")

            # scraping.yaml ã¨ default.yaml ã‚’è©³ã—ãè¦‹ã‚‹
            if config_file.name in ['scraping.yaml', 'default.yaml']:
                with open(config_file, 'r', encoding='utf-8') as f:
                    print(f"\nğŸ“„ {config_file.name} ã®å†…å®¹:")
                    print("-" * 80)
                    content = f.read()
                    print(content[:500])  # æœ€åˆã®500æ–‡å­—
                    if len(content) > 500:
                        print("... (çœç•¥)")
    else:
        print("âŒ è¨­å®šãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")


def compare_raw_vs_parsed():
    """RAWã¨ãƒ‘ãƒ¼ã‚¹æ¸ˆã¿ã®è©³ç´°æ¯”è¼ƒ"""
    print("\n" + "=" * 80)
    print("ğŸ” RAW vs ãƒ‘ãƒ¼ã‚¹æ¸ˆã¿è©³ç´°æ¯”è¼ƒ")
    print("=" * 80)

    # RAWã®race_idãƒªã‚¹ãƒˆã‚’å–å¾—
    race_dir = Path("keibaai/data/raw/html/race")
    if race_dir.exists():
        raw_race_ids = set(f.stem for f in race_dir.glob("*.bin"))
        print(f"\nRAW HTMLã®ãƒ¬ãƒ¼ã‚¹IDæ•°: {len(raw_race_ids):,}")
    else:
        raw_race_ids = set()

    # ãƒ‘ãƒ¼ã‚¹æ¸ˆã¿ã®race_idãƒªã‚¹ãƒˆã‚’å–å¾—
    races_file = Path("keibaai/data/parsed/parquet/races/races.parquet")
    if races_file.exists():
        df = pd.read_parquet(races_file)
        parsed_race_ids = set(df['race_id'].unique())
        print(f"ãƒ‘ãƒ¼ã‚¹æ¸ˆã¿ã®ãƒ¬ãƒ¼ã‚¹IDæ•°: {len(parsed_race_ids):,}")
    else:
        parsed_race_ids = set()

    # å·®åˆ†ã‚’åˆ†æ
    if raw_race_ids and parsed_race_ids:
        missing_race_ids = raw_race_ids - parsed_race_ids
        print(f"\nâš ï¸ æœªãƒ‘ãƒ¼ã‚¹ã®ãƒ¬ãƒ¼ã‚¹IDæ•°: {len(missing_race_ids):,}")

        if missing_race_ids:
            # æœªãƒ‘ãƒ¼ã‚¹ãƒ¬ãƒ¼ã‚¹ã®å¹´åˆ¥åˆ†å¸ƒ
            year_count = defaultdict(int)
            for race_id in missing_race_ids:
                if len(race_id) == 12 and race_id.isdigit():
                    year = race_id[:4]
                    year_count[year] += 1

            print("\næœªãƒ‘ãƒ¼ã‚¹ãƒ¬ãƒ¼ã‚¹ã®å¹´åˆ¥åˆ†å¸ƒ:")
            for year in sorted(year_count.keys()):
                print(f"  {year}å¹´: {year_count[year]:>5} ä»¶")

            # ã‚µãƒ³ãƒ—ãƒ«ã¨ã—ã¦æœ€åˆã®10ä»¶ã‚’è¡¨ç¤º
            print("\næœªãƒ‘ãƒ¼ã‚¹ãƒ¬ãƒ¼ã‚¹IDã®ã‚µãƒ³ãƒ—ãƒ«ï¼ˆæœ€åˆã®10ä»¶ï¼‰:")
            for race_id in sorted(missing_race_ids)[:10]:
                print(f"  {race_id}")


def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    print("=" * 80)
    print("ğŸ” KeibaAI_v2 ãƒ‘ãƒ¼ã‚¹å¤±æ•—åŸå› ã®å¾¹åº•èª¿æŸ»")
    print(f"â° å®Ÿè¡Œæ™‚åˆ»: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 80)

    analyze_html_files()
    analyze_parsed_parquet()
    check_parse_logs()
    check_parse_script_config()
    check_config_files()
    compare_raw_vs_parsed()

    print("\n" + "=" * 80)
    print("ğŸ“‹ èª¿æŸ»çµæœã‚µãƒãƒªãƒ¼")
    print("=" * 80)
    print("""
ğŸ’¡ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—:
  1. ãƒ‘ãƒ¼ã‚¹å‡¦ç†ã‚¹ã‚¯ãƒªãƒ—ãƒˆã«æ—¥ä»˜åˆ¶é™ã‚„limitæŒ‡å®šãŒãªã„ã‹ç¢ºèª
  2. ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°ã§ç•°å¸¸çµ‚äº†ã®åŸå› ã‚’ç‰¹å®š
  3. æœªãƒ‘ãƒ¼ã‚¹ã®ãƒ¬ãƒ¼ã‚¹IDãƒªã‚¹ãƒˆã‚’ç”Ÿæˆã—ã€å·®åˆ†ãƒ‘ãƒ¼ã‚¹ã‚’å®Ÿè¡Œ
  4. ãƒ‘ãƒ¼ã‚µãƒ¼ã‚³ãƒ¼ãƒ‰ï¼ˆresults_parser.pyç­‰ï¼‰ã®å•é¡Œç‚¹ã‚’ç¢ºèª
    """)

    print("=" * 80)
    print("âœ… ãƒ‡ãƒãƒƒã‚°ã‚¹ã‚¯ãƒªãƒ—ãƒˆ03å®Œäº†")
    print("=" * 80)


if __name__ == "__main__":
    main()
