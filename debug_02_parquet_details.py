#!/usr/bin/env python3
"""
ãƒ‡ãƒãƒƒã‚°ã‚¹ã‚¯ãƒªãƒ—ãƒˆ02: Parquetãƒ•ã‚¡ã‚¤ãƒ«ã®è©³ç´°åˆ†æ
ãƒ‘ãƒ¼ã‚¹æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã®å†…å®¹ã€ã‚«ãƒ©ãƒ ã€ãƒ‡ãƒ¼ã‚¿å‹ã€æ¬ æçŠ¶æ³ã‚’è©³ã—ãèª¿æŸ»ã—ã¾ã™
"""

import pandas as pd
from pathlib import Path
from datetime import datetime
import numpy as np

def analyze_parquet_file(file_path: Path, file_name: str):
    """Parquetãƒ•ã‚¡ã‚¤ãƒ«ã‚’è©³ç´°åˆ†æ"""
    print("\n" + "=" * 80)
    print(f"ğŸ“„ {file_name} ã®è©³ç´°åˆ†æ")
    print("=" * 80)

    if not file_path.exists():
        print(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ã¾ã›ã‚“: {file_path}")
        return None

    try:
        df = pd.read_parquet(file_path)

        # åŸºæœ¬æƒ…å ±
        print(f"\nğŸ“Š åŸºæœ¬æƒ…å ±:")
        print(f"  è¡Œæ•°: {len(df):,} è¡Œ")
        print(f"  åˆ—æ•°: {len(df.columns)} åˆ—")
        print(f"  ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {df.memory_usage(deep=True).sum() / 1024 / 1024:.2f} MB")

        # ã‚«ãƒ©ãƒ ä¸€è¦§ã¨å‹
        print(f"\nğŸ“‹ ã‚«ãƒ©ãƒ ä¸€è¦§ ({len(df.columns)}å€‹):")
        print(f"\n{'ã‚«ãƒ©ãƒ å':<35s} | {'ãƒ‡ãƒ¼ã‚¿å‹':<15s} | {'æ¬ ææ•°':<10s} | {'æ¬ æç‡':<8s}")
        print("-" * 80)

        for col in df.columns:
            dtype = str(df[col].dtype)
            null_count = df[col].isna().sum()
            null_rate = (null_count / len(df)) * 100 if len(df) > 0 else 0
            print(f"{col:<35s} | {dtype:<15s} | {null_count:<10,} | {null_rate:>6.2f}%")

        # ãƒ‡ãƒ¼ã‚¿ã‚µãƒ³ãƒ—ãƒ«ï¼ˆæœ€åˆã®3è¡Œï¼‰
        print(f"\nğŸ” ãƒ‡ãƒ¼ã‚¿ã‚µãƒ³ãƒ—ãƒ«ï¼ˆæœ€åˆã®3è¡Œï¼‰:")
        print("-" * 80)
        if len(df) > 0:
            # pandasã®è¡¨ç¤ºã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’ä¸€æ™‚çš„ã«å¤‰æ›´
            with pd.option_context('display.max_columns', None,
                                   'display.width', None,
                                   'display.max_colwidth', 30):
                print(df.head(3).to_string())
        else:
            print("  ãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™")

        # æ—¥ä»˜ç¯„å›²ã®ç¢ºèªï¼ˆrace_dateã‚«ãƒ©ãƒ ãŒã‚ã‚Œã°ï¼‰
        if 'race_date' in df.columns:
            print(f"\nğŸ“… æ—¥ä»˜ç¯„å›²:")
            try:
                df['race_date'] = pd.to_datetime(df['race_date'])
                min_date = df['race_date'].min()
                max_date = df['race_date'].max()
                unique_dates = df['race_date'].nunique()
                print(f"  æœ€å°æ—¥ä»˜: {min_date.strftime('%Y-%m-%d') if pd.notna(min_date) else 'N/A'}")
                print(f"  æœ€å¤§æ—¥ä»˜: {max_date.strftime('%Y-%m-%d') if pd.notna(max_date) else 'N/A'}")
                print(f"  ãƒ¦ãƒ‹ãƒ¼ã‚¯æ—¥æ•°: {unique_dates:,} æ—¥")
            except Exception as e:
                print(f"  âš ï¸ æ—¥ä»˜è§£æã‚¨ãƒ©ãƒ¼: {e}")

        # race_idã®ç¯„å›²ç¢ºèª
        if 'race_id' in df.columns:
            print(f"\nğŸ‡ ãƒ¬ãƒ¼ã‚¹IDæƒ…å ±:")
            unique_races = df['race_id'].nunique()
            print(f"  ãƒ¦ãƒ‹ãƒ¼ã‚¯ãƒ¬ãƒ¼ã‚¹æ•°: {unique_races:,}")

            # ãƒ¬ãƒ¼ã‚¹IDã®å¹´åº¦åˆ¥é›†è¨ˆ
            try:
                df['year'] = df['race_id'].astype(str).str[:4]
                year_counts = df.groupby('year')['race_id'].nunique().sort_index()
                print(f"\n  å¹´åº¦åˆ¥ãƒ¬ãƒ¼ã‚¹æ•°:")
                for year, count in year_counts.items():
                    print(f"    {year}: {count:>6,} ãƒ¬ãƒ¼ã‚¹")
                df.drop('year', axis=1, inplace=True)
            except Exception as e:
                print(f"  âš ï¸ å¹´åº¦é›†è¨ˆã‚¨ãƒ©ãƒ¼: {e}")

        # horse_idã®ç¯„å›²ç¢ºèª
        if 'horse_id' in df.columns:
            print(f"\nğŸ´ é¦¬IDæƒ…å ±:")
            unique_horses = df['horse_id'].nunique()
            print(f"  ãƒ¦ãƒ‹ãƒ¼ã‚¯é¦¬æ•°: {unique_horses:,}")

            # é¦¬IDã®ç”Ÿå¹´åˆ¥é›†è¨ˆ
            try:
                df['birth_year'] = df['horse_id'].astype(str).str[:4]
                birth_year_counts = df.groupby('birth_year')['horse_id'].nunique().sort_index()
                print(f"\n  ç”Ÿå¹´åˆ¥é¦¬æ•°ï¼ˆä¸Šä½10å¹´ï¼‰:")
                for birth_year, count in birth_year_counts.tail(10).items():
                    print(f"    {birth_year}: {count:>6,} é ­")
                df.drop('birth_year', axis=1, inplace=True)
            except Exception as e:
                print(f"  âš ï¸ ç”Ÿå¹´é›†è¨ˆã‚¨ãƒ©ãƒ¼: {e}")

        # çµ±è¨ˆæƒ…å ±ï¼ˆæ•°å€¤ã‚«ãƒ©ãƒ ã®ã¿ï¼‰
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        if len(numeric_cols) > 0:
            print(f"\nğŸ“ˆ æ•°å€¤ã‚«ãƒ©ãƒ ã®çµ±è¨ˆæƒ…å ±:")
            print("-" * 80)
            stats = df[numeric_cols].describe().T
            print(stats.to_string())

        return df

    except Exception as e:
        print(f"\nâŒ ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()
        return None

def check_horses_performance():
    """é¦¬éå»æˆç¸¾ãƒ•ã‚¡ã‚¤ãƒ«ã®ç¢ºèª"""
    print("\n" + "=" * 80)
    print("ğŸ” é¦¬éå»æˆç¸¾ãƒ•ã‚¡ã‚¤ãƒ«ã®ç¢ºèª")
    print("=" * 80)

    perf_path = Path("keibaai/data/parsed/parquet/horses/horses_performance.parquet")

    if perf_path.exists():
        print(f"\nâœ… horses_performance.parquet ãŒå­˜åœ¨ã—ã¾ã™")
        df = analyze_parquet_file(perf_path, "horses_performance")
    else:
        print(f"\nâŒ horses_performance.parquet ãŒå­˜åœ¨ã—ã¾ã›ã‚“")
        print(f"   æœŸå¾…ã•ã‚Œã‚‹ãƒ‘ã‚¹: {perf_path}")
        print(f"\nğŸ’¡ REVIEW.mdã§ã¯å®Ÿè£…æ¸ˆã¿ã¨ãªã£ã¦ã„ã¾ã™ãŒã€ãƒ•ã‚¡ã‚¤ãƒ«ãŒç”Ÿæˆã•ã‚Œã¦ã„ã¾ã›ã‚“")
        print(f"   â†’ ãƒ‘ãƒ¼ã‚¹å‡¦ç†ã‚’å†å®Ÿè¡Œã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™")

def compare_raw_vs_parsed():
    """RAWãƒ‡ãƒ¼ã‚¿ã¨ãƒ‘ãƒ¼ã‚¹æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã®æ¯”è¼ƒ"""
    print("\n" + "=" * 80)
    print("ğŸ“Š RAWãƒ‡ãƒ¼ã‚¿ vs ãƒ‘ãƒ¼ã‚¹æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã®æ¯”è¼ƒ")
    print("=" * 80)

    # RAWãƒ¬ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«æ•°
    raw_race_dir = Path("keibaai/data/raw/html/race")
    raw_race_count = len(list(raw_race_dir.glob("*.bin"))) if raw_race_dir.exists() else 0

    # ãƒ‘ãƒ¼ã‚¹æ¸ˆã¿ãƒ¬ãƒ¼ã‚¹æ•°
    parsed_race_path = Path("keibaai/data/parsed/parquet/races/races.parquet")
    if parsed_race_path.exists():
        df_races = pd.read_parquet(parsed_race_path)
        parsed_race_count = df_races['race_id'].nunique() if 'race_id' in df_races.columns else len(df_races)
    else:
        parsed_race_count = 0

    # RAWé¦¬ãƒ•ã‚¡ã‚¤ãƒ«æ•°ï¼ˆprofileã¨perfã®åŠåˆ†ï¼‰
    raw_horse_dir = Path("keibaai/data/raw/html/horse")
    raw_horse_files = list(raw_horse_dir.glob("*.bin")) if raw_horse_dir.exists() else []
    raw_horse_count = len([f for f in raw_horse_files if '_profile.bin' in f.name])

    # ãƒ‘ãƒ¼ã‚¹æ¸ˆã¿é¦¬æ•°
    parsed_horse_path = Path("keibaai/data/parsed/parquet/horses/horses.parquet")
    if parsed_horse_path.exists():
        df_horses = pd.read_parquet(parsed_horse_path)
        parsed_horse_count = df_horses['horse_id'].nunique() if 'horse_id' in df_horses.columns else len(df_horses)
    else:
        parsed_horse_count = 0

    print(f"\nğŸ“ˆ æ¯”è¼ƒçµæœ:")
    print(f"\n  ãƒ¬ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿:")
    print(f"    RAW HTML:     {raw_race_count:>8,} ä»¶")
    print(f"    ãƒ‘ãƒ¼ã‚¹æ¸ˆã¿:   {parsed_race_count:>8,} ãƒ¬ãƒ¼ã‚¹")
    print(f"    ãƒ‘ãƒ¼ã‚¹ç‡:     {(parsed_race_count / raw_race_count * 100) if raw_race_count > 0 else 0:>7.2f} %")
    print(f"    æœªãƒ‘ãƒ¼ã‚¹:     {raw_race_count - parsed_race_count:>8,} ä»¶")

    print(f"\n  é¦¬ãƒ‡ãƒ¼ã‚¿:")
    print(f"    RAW HTML:     {raw_horse_count:>8,} é ­")
    print(f"    ãƒ‘ãƒ¼ã‚¹æ¸ˆã¿:   {parsed_horse_count:>8,} é ­")
    print(f"    ãƒ‘ãƒ¼ã‚¹ç‡:     {(parsed_horse_count / raw_horse_count * 100) if raw_horse_count > 0 else 0:>7.2f} %")
    print(f"    æœªãƒ‘ãƒ¼ã‚¹:     {raw_horse_count - parsed_horse_count:>8,} é ­")

    if parsed_race_count < raw_race_count:
        print(f"\nâš ï¸ ç´„{raw_race_count - parsed_race_count:,}ä»¶ã®ãƒ¬ãƒ¼ã‚¹ãŒãƒ‘ãƒ¼ã‚¹ã•ã‚Œã¦ã„ã¾ã›ã‚“")

    if parsed_horse_count < raw_horse_count:
        print(f"\nğŸš¨ ç´„{raw_horse_count - parsed_horse_count:,}é ­ã®é¦¬ãƒ‡ãƒ¼ã‚¿ãŒãƒ‘ãƒ¼ã‚¹ã•ã‚Œã¦ã„ã¾ã›ã‚“")
        print(f"   ã“ã‚Œã¯ãƒ¢ãƒ‡ãƒ«ç²¾åº¦ã«å¤§ããå½±éŸ¿ã—ã¾ã™ï¼")

def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    print("\n")
    print("ğŸ” KeibaAI_v2 Parquetãƒ•ã‚¡ã‚¤ãƒ«è©³ç´°åˆ†æ")
    print(f"â° å®Ÿè¡Œæ™‚åˆ»: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("\n")

    # 1. races.parquet åˆ†æ
    analyze_parquet_file(
        Path("keibaai/data/parsed/parquet/races/races.parquet"),
        "races.parquet"
    )

    # 2. shutuba.parquet åˆ†æ
    analyze_parquet_file(
        Path("keibaai/data/parsed/parquet/shutuba/shutuba.parquet"),
        "shutuba.parquet"
    )

    # 3. horses.parquet åˆ†æ
    analyze_parquet_file(
        Path("keibaai/data/parsed/parquet/horses/horses.parquet"),
        "horses.parquet"
    )

    # 4. pedigrees.parquet åˆ†æ
    analyze_parquet_file(
        Path("keibaai/data/parsed/parquet/pedigrees/pedigrees.parquet"),
        "pedigrees.parquet"
    )

    # 5. é¦¬éå»æˆç¸¾ãƒ•ã‚¡ã‚¤ãƒ«ã®ç¢ºèª
    check_horses_performance()

    # 6. RAW vs ãƒ‘ãƒ¼ã‚¹æ¸ˆã¿æ¯”è¼ƒ
    compare_raw_vs_parsed()

    # æœ€çµ‚ã‚µãƒãƒªãƒ¼
    print("\n" + "=" * 80)
    print("ğŸ“‹ åˆ†æå®Œäº†")
    print("=" * 80)
    print("\næ¬¡ã®ãƒ‡ãƒãƒƒã‚°ã‚¹ãƒ†ãƒƒãƒ—:")
    print("  1. ãƒ‘ãƒ¼ã‚¹å‡¦ç†ã‚’å®Œå…¨ã«å†å®Ÿè¡Œï¼ˆæœªãƒ‘ãƒ¼ã‚¹åˆ†ã‚’å‡¦ç†ï¼‰")
    print("  2. é¦¬éå»æˆç¸¾ãƒ‡ãƒ¼ã‚¿ã®ç”Ÿæˆç¢ºèª")
    print("  3. æ¬ æãƒ‡ãƒ¼ã‚¿ã®åŸå› ç‰¹å®š")
    print("\n" + "=" * 80)
    print("\n")

if __name__ == "__main__":
    main()
