#!/usr/bin/env python3
"""
horses.parquetã«è¡€çµ±æƒ…å ±ã‚’è¿½åŠ ã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
pedigrees.parquetã®generation=1ï¼ˆä¸¡è¦ªï¼‰ãƒ‡ãƒ¼ã‚¿ã‚’çµ±åˆ
"""

import pandas as pd
from pathlib import Path
from datetime import datetime

def add_pedigree_to_horses():
    """horses.parquetã«è¡€çµ±æƒ…å ±ã‚’è¿½åŠ """

    print("\n" + "=" * 80)
    print("ğŸ”§ horses.parquetã¸ã®è¡€çµ±æƒ…å ±è¿½åŠ å‡¦ç†")
    print("=" * 80)

    # 1. ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã®è¨­å®š
    horses_path = Path("keibaai/data/parsed/parquet/horses/horses.parquet")
    pedigrees_path = Path("keibaai/data/parsed/parquet/pedigrees/pedigrees.parquet")
    output_path = horses_path  # ä¸Šæ›¸ãä¿å­˜

    # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆ
    backup_path = horses_path.parent / f"horses_backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}.parquet"

    # 2. ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    print("\nğŸ“‚ ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ä¸­...")

    if not horses_path.exists():
        print(f"âŒ horses.parquetãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {horses_path}")
        return False

    if not pedigrees_path.exists():
        print(f"âŒ pedigrees.parquetãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {pedigrees_path}")
        return False

    horses_df = pd.read_parquet(horses_path)
    pedigrees_df = pd.read_parquet(pedigrees_path)

    print(f"  âœ… horses.parquet: {len(horses_df):,} è¡Œ")
    print(f"  âœ… pedigrees.parquet: {len(pedigrees_df):,} è¡Œ")

    # 3. ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆ
    print(f"\nğŸ’¾ ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆä¸­...")
    horses_df.to_parquet(backup_path, index=False)
    print(f"  âœ… ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä¿å­˜: {backup_path}")

    # 4. generation=1ã®ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
    print(f"\nğŸ” generation=1ï¼ˆä¸¡è¦ªï¼‰ã®ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º...")
    gen1_df = pedigrees_df[pedigrees_df['generation'] == 1].copy()
    print(f"  âœ… generation=1: {len(gen1_df):,} è¡Œ")

    # 5. å„é¦¬ã”ã¨ã«ä¸¡è¦ªæƒ…å ±ã‚’æ•´å½¢
    print(f"\nğŸ”§ ä¸¡è¦ªæƒ…å ±ã‚’æ•´å½¢ä¸­...")

    # horse_idã§ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ã—ã¦ã€1é ­ç›®ã¨2é ­ç›®ã‚’å–å¾—
    parent_data = []

    for horse_id, group in gen1_df.groupby('horse_id'):
        # ã‚½ãƒ¼ãƒˆï¼ˆå¿µã®ãŸã‚ã€ancestor_idã§ã‚½ãƒ¼ãƒˆï¼‰
        group = group.sort_values('ancestor_id')

        row = {'horse_id': horse_id}

        if len(group) >= 1:
            # 1é ­ç›®ã‚’ä¾¿å®œçš„ã« sireï¼ˆçˆ¶ï¼‰ã¨ã™ã‚‹
            first = group.iloc[0]
            row['sire_id'] = first['ancestor_id']
            row['sire_name'] = first['ancestor_name']

        if len(group) >= 2:
            # 2é ­ç›®ã‚’ä¾¿å®œçš„ã« damï¼ˆæ¯ï¼‰ã¨ã™ã‚‹
            second = group.iloc[1]
            row['dam_id'] = second['ancestor_id']
            row['dam_name'] = second['ancestor_name']

        parent_data.append(row)

    parent_df = pd.DataFrame(parent_data)
    print(f"  âœ… ä¸¡è¦ªæƒ…å ±ã‚’æ•´å½¢: {len(parent_df):,} é ­")

    # 6. horses_dfã«çµåˆ
    print(f"\nğŸ”— horses.parquetã«è¡€çµ±æƒ…å ±ã‚’çµåˆä¸­...")

    # çµåˆå‰ã®ã‚«ãƒ©ãƒ æ•°
    before_cols = len(horses_df.columns)

    # å·¦çµåˆï¼ˆhorses_dfã‚’ãƒ™ãƒ¼ã‚¹ã«ã€parent_dfã‚’çµåˆï¼‰
    merged_df = horses_df.merge(
        parent_df,
        on='horse_id',
        how='left'
    )

    # çµåˆå¾Œã®ã‚«ãƒ©ãƒ æ•°
    after_cols = len(merged_df.columns)

    print(f"  âœ… çµåˆå®Œäº†")
    print(f"    - çµåˆå‰: {before_cols} ã‚«ãƒ©ãƒ ")
    print(f"    - çµåˆå¾Œ: {after_cols} ã‚«ãƒ©ãƒ ")
    print(f"    - è¿½åŠ ã‚«ãƒ©ãƒ : {after_cols - before_cols} å€‹")

    # 7. çµ±è¨ˆæƒ…å ±ã®è¡¨ç¤º
    print(f"\nğŸ“Š è¡€çµ±æƒ…å ±ã®çµ±è¨ˆ:")

    if 'sire_id' in merged_df.columns:
        sire_filled = merged_df['sire_id'].notna().sum()
        sire_rate = sire_filled / len(merged_df) * 100
        print(f"  sire_id:   {sire_filled:>6,} / {len(merged_df):,} é ­ ({sire_rate:>5.1f}%)")

    if 'dam_id' in merged_df.columns:
        dam_filled = merged_df['dam_id'].notna().sum()
        dam_rate = dam_filled / len(merged_df) * 100
        print(f"  dam_id:    {dam_filled:>6,} / {len(merged_df):,} é ­ ({dam_rate:>5.1f}%)")

    # 8. ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã®è¡¨ç¤º
    print(f"\nğŸ” ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ï¼ˆæœ€åˆã®3é ­ï¼‰:")
    print("-" * 80)

    # è¡¨ç¤ºç”¨ã«ä¸€éƒ¨ã‚«ãƒ©ãƒ ã‚’é¸æŠ
    display_cols = ['horse_id', 'horse_name', 'sire_name', 'dam_name']
    available_cols = [col for col in display_cols if col in merged_df.columns]

    if available_cols:
        print(merged_df[available_cols].head(3).to_string(index=False))

    # 9. ã‚«ãƒ©ãƒ ä¸€è¦§ã®è¡¨ç¤º
    print(f"\nğŸ“‹ æ›´æ–°å¾Œã®ã‚«ãƒ©ãƒ ä¸€è¦§:")
    for i, col in enumerate(merged_df.columns, 1):
        null_count = merged_df[col].isna().sum()
        null_rate = null_count / len(merged_df) * 100
        marker = "ğŸ†•" if col in ['sire_id', 'sire_name', 'dam_id', 'dam_name'] else "  "
        print(f"  {marker} {i:>2}. {col:25s} (æ¬ æ: {null_rate:>5.1f}%)")

    # 10. ä¿å­˜
    print(f"\nğŸ’¾ ä¿å­˜ä¸­...")
    merged_df.to_parquet(output_path, index=False)
    print(f"  âœ… ä¿å­˜å®Œäº†: {output_path}")

    # 11. æ¤œè¨¼
    print(f"\nâœ… æ¤œè¨¼:")
    verification_df = pd.read_parquet(output_path)
    print(f"  èª­ã¿è¾¼ã¿ç¢ºèª: {len(verification_df):,} è¡Œ Ã— {len(verification_df.columns)} åˆ—")

    # è¡€çµ±ã‚«ãƒ©ãƒ ã®å­˜åœ¨ç¢ºèª
    pedigree_cols = ['sire_id', 'sire_name', 'dam_id', 'dam_name']
    missing_cols = [col for col in pedigree_cols if col not in verification_df.columns]

    if missing_cols:
        print(f"  âš ï¸ æ¬ è½ã‚«ãƒ©ãƒ : {missing_cols}")
        return False
    else:
        print(f"  âœ… ã™ã¹ã¦ã®è¡€çµ±ã‚«ãƒ©ãƒ ãŒå­˜åœ¨ã—ã¾ã™")

    return True

def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    print("\n")
    print("ğŸ” KeibaAI_v2 è¡€çµ±æƒ…å ±çµ±åˆå‡¦ç†")
    print(f"â° å®Ÿè¡Œæ™‚åˆ»: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("\n")

    success = add_pedigree_to_horses()

    print("\n" + "=" * 80)
    if success:
        print("âœ… å‡¦ç†å®Œäº†")
        print("=" * 80)
        print("\nğŸ’¡ æ¬¡ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³:")
        print("  1. ç¢ºèª: python debug_02_parquet_details.py")
        print("  2. å…¨ä»¶ãƒ‘ãƒ¼ã‚¹å‡¦ç†ã‚’å®Ÿè¡Œ: python keibaai/src/run_parsing_pipeline_local.py")
    else:
        print("âŒ å‡¦ç†å¤±æ•—")
        print("=" * 80)

    print("\n")

if __name__ == "__main__":
    main()
