"""
æ¬ æå€¤å‡¦ç†ã®è¿½è·¡ãƒ‡ãƒãƒƒã‚°ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

ç›®çš„:
- feature_engine.py ã§ã©ã“ã§ fillna ãŒä½¿ã‚ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª
- features.yaml ã® imputation è¨­å®šã‚’ç¢ºèª
- å…ƒãƒ‡ãƒ¼ã‚¿ï¼ˆshutuba.parquet, races.parquetï¼‰ã«æ¬ æå€¤ãŒã‚ã‚‹ã‹ç¢ºèª
- ç‰¹å¾´é‡ç”Ÿæˆéç¨‹ã®ã©ã®æ®µéšã§æ¬ æå€¤ãŒå‡¦ç†ã•ã‚Œã¦ã„ã‚‹ã‹ã‚’ç‰¹å®š

ä½¿ç”¨æ–¹æ³•:
    python debug_nan_handling.py
"""

import pandas as pd
from pathlib import Path
import re
import sys

def check_fillna_in_feature_engine():
    """feature_engine.py ã§ fillna ãŒä½¿ã‚ã‚Œã¦ã„ã‚‹ç®‡æ‰€ã‚’æ¤œç´¢"""

    print("=" * 70)
    print("ğŸ” 1. feature_engine.py ã§ã® fillna ä½¿ç”¨ç®‡æ‰€")
    print("=" * 70)
    print()

    feature_engine_path = Path("keibaai/src/features/feature_engine.py")

    if not feature_engine_path.exists():
        print(f"âŒ {feature_engine_path} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return

    with open(feature_engine_path, 'r', encoding='utf-8') as f:
        lines = f.readlines()

    fillna_lines = []
    for i, line in enumerate(lines, 1):
        if 'fillna' in line.lower():
            fillna_lines.append((i, line.rstrip()))

    if fillna_lines:
        print(f"âœ… fillna ãŒä½¿ã‚ã‚Œã¦ã„ã‚‹ç®‡æ‰€: {len(fillna_lines)}ä»¶")
        print()
        for line_num, line_content in fillna_lines:
            print(f"  è¡Œ{line_num}: {line_content}")
        print()
    else:
        print("âœ… fillna ã¯ä½¿ã‚ã‚Œã¦ã„ã¾ã›ã‚“")
        print()

    # imputation é–¢é€£ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚‚æ¤œç´¢
    imputation_keywords = ['imputation', 'æ¬ æ', 'NaN', 'missing']
    imputation_lines = []

    for i, line in enumerate(lines, 1):
        for keyword in imputation_keywords:
            if keyword in line:
                imputation_lines.append((i, line.rstrip()))
                break

    if imputation_lines:
        print(f"ğŸ“‹ æ¬ æå€¤å‡¦ç†é–¢é€£ã®ã‚³ãƒ¼ãƒ‰: {len(imputation_lines)}ä»¶")
        print()
        for line_num, line_content in imputation_lines:
            print(f"  è¡Œ{line_num}: {line_content}")
        print()

def check_features_yaml_imputation():
    """features.yaml ã® imputation è¨­å®šã‚’ç¢ºèª"""

    print("=" * 70)
    print("ğŸ” 2. features.yaml ã®æ¬ æå€¤å‡¦ç†è¨­å®š")
    print("=" * 70)
    print()

    yaml_path = Path("keibaai/configs/features.yaml")

    if not yaml_path.exists():
        print(f"âŒ {yaml_path} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return

    with open(yaml_path, 'r', encoding='utf-8') as f:
        lines = f.readlines()

    # imputation ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’æ¢ã™
    in_imputation_section = False
    imputation_lines = []

    for i, line in enumerate(lines, 1):
        if 'imputation:' in line:
            in_imputation_section = True
            imputation_lines.append((i, line.rstrip()))
        elif in_imputation_section:
            # ã‚¤ãƒ³ãƒ‡ãƒ³ãƒˆãŒã‚ã‚‹è¡Œã¯ imputation ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®ä¸€éƒ¨
            if line.startswith('  ') or line.strip() == '':
                imputation_lines.append((i, line.rstrip()))
            else:
                # ã‚¤ãƒ³ãƒ‡ãƒ³ãƒˆãŒãªããªã£ãŸã‚‰ã‚»ã‚¯ã‚·ãƒ§ãƒ³çµ‚äº†
                break

    if imputation_lines:
        print("ğŸ“‹ imputation è¨­å®š:")
        print()
        for line_num, line_content in imputation_lines:
            print(f"  è¡Œ{line_num}: {line_content}")
        print()
    else:
        print("âš ï¸  imputation è¨­å®šãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        print()

def check_source_data_nan():
    """å…ƒãƒ‡ãƒ¼ã‚¿ï¼ˆshutuba.parquet, races.parquetï¼‰ã®æ¬ æå€¤ã‚’ç¢ºèª"""

    print("=" * 70)
    print("ğŸ” 3. å…ƒãƒ‡ãƒ¼ã‚¿ï¼ˆãƒ‘ãƒ¼ã‚¹æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ï¼‰ã®æ¬ æå€¤ç¢ºèª")
    print("=" * 70)
    print()

    # shutuba.parquet ã®ç¢ºèª
    shutuba_path = Path("keibaai/data/parsed/parquet/shutuba/shutuba.parquet")

    if shutuba_path.exists():
        print("ğŸ“‚ shutuba.parquet ã®æ¬ æå€¤åˆ†æ")
        print("-" * 70)

        try:
            df_shutuba = pd.read_parquet(shutuba_path)
            print(f"ç·è¡Œæ•°: {len(df_shutuba):,}è¡Œ")
            print(f"ç·ã‚«ãƒ©ãƒ æ•°: {len(df_shutuba.columns)}ã‚«ãƒ©ãƒ ")
            print()

            # æ¬ æå€¤ã®ã‚ã‚‹ã‚«ãƒ©ãƒ ã‚’é›†è¨ˆ
            nan_counts = df_shutuba.isna().sum()
            nan_cols = nan_counts[nan_counts > 0].sort_values(ascending=False)

            if len(nan_cols) > 0:
                print(f"âš ï¸  NaNã‚’æŒã¤ã‚«ãƒ©ãƒ : {len(nan_cols)}å€‹")
                print()
                for col, count in nan_cols.head(20).items():
                    nan_rate = (count / len(df_shutuba)) * 100
                    print(f"  {col}: {count:,}ä»¶ ({nan_rate:.2f}%)")
                print()
            else:
                print("âœ… shutuba.parquet ã«ã¯ NaNå€¤ãŒã‚ã‚Šã¾ã›ã‚“")
                print()

        except Exception as e:
            print(f"âŒ ã‚¨ãƒ©ãƒ¼: {e}")
            print()
    else:
        print(f"âŒ {shutuba_path} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        print()

    # races.parquet ã®ç¢ºèª
    races_path = Path("keibaai/data/parsed/parquet/races/races.parquet")

    if races_path.exists():
        print("ğŸ“‚ races.parquet ã®æ¬ æå€¤åˆ†æ")
        print("-" * 70)

        try:
            df_races = pd.read_parquet(races_path)
            print(f"ç·è¡Œæ•°: {len(df_races):,}è¡Œ")
            print(f"ç·ã‚«ãƒ©ãƒ æ•°: {len(df_races.columns)}ã‚«ãƒ©ãƒ ")
            print()

            # æ¬ æå€¤ã®ã‚ã‚‹ã‚«ãƒ©ãƒ ã‚’é›†è¨ˆ
            nan_counts = df_races.isna().sum()
            nan_cols = nan_counts[nan_counts > 0].sort_values(ascending=False)

            if len(nan_cols) > 0:
                print(f"âš ï¸  NaNã‚’æŒã¤ã‚«ãƒ©ãƒ : {len(nan_cols)}å€‹")
                print()
                for col, count in nan_cols.head(20).items():
                    nan_rate = (count / len(df_races)) * 100
                    print(f"  {col}: {count:,}ä»¶ ({nan_rate:.2f}%)")
                print()
            else:
                print("âœ… races.parquet ã«ã¯ NaNå€¤ãŒã‚ã‚Šã¾ã›ã‚“")
                print()

        except Exception as e:
            print(f"âŒ ã‚¨ãƒ©ãƒ¼: {e}")
            print()
    else:
        print(f"âŒ {races_path} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        print()

def check_generated_features_nan_sample():
    """ç”Ÿæˆã•ã‚ŒãŸç‰¹å¾´é‡ãƒ‡ãƒ¼ã‚¿ã®ä¸€éƒ¨ã‚’èª­ã¿è¾¼ã‚“ã§NaNç¢ºèªï¼ˆè©³ç´°ç‰ˆï¼‰"""

    print("=" * 70)
    print("ğŸ” 4. ç”Ÿæˆã•ã‚ŒãŸç‰¹å¾´é‡ãƒ‡ãƒ¼ã‚¿ã®è©³ç´°ç¢ºèª")
    print("=" * 70)
    print()

    features_base = Path("keibaai/data/features/parquet")

    # 2023å¹´1æœˆã®ãƒ‡ãƒ¼ã‚¿ã‚’1ãƒ•ã‚¡ã‚¤ãƒ«ã ã‘èª­ã¿è¾¼ã‚€
    sample_path = features_base / "year=2023" / "month=1"

    if not sample_path.exists():
        print(f"âŒ {sample_path} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return

    parquet_files = list(sample_path.glob("*.parquet"))

    if not parquet_files:
        print("âŒ Parquetãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return

    print(f"ğŸ“‚ ã‚µãƒ³ãƒ—ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«: {parquet_files[0].name}")
    print()

    try:
        df = pd.read_parquet(parquet_files[0])

        print(f"ç·è¡Œæ•°: {len(df):,}è¡Œ")
        print(f"ç·ã‚«ãƒ©ãƒ æ•°: {len(df.columns)}ã‚«ãƒ©ãƒ ")
        print()

        # å…¨ã‚«ãƒ©ãƒ ã®å‹ã‚’ç¢ºèª
        print("ğŸ“Š ãƒ‡ãƒ¼ã‚¿å‹åˆ¥ã®ã‚«ãƒ©ãƒ æ•°:")
        dtype_counts = df.dtypes.value_counts()
        for dtype, count in dtype_counts.items():
            print(f"  {dtype}: {count}å€‹")
        print()

        # NaNç¢ºèªï¼ˆå¿µã®ãŸã‚ï¼‰
        nan_counts = df.isna().sum()
        nan_cols = nan_counts[nan_counts > 0]

        if len(nan_cols) > 0:
            print(f"âš ï¸  ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã«ã¯ {len(nan_cols)}å€‹ã®ã‚«ãƒ©ãƒ ã«NaNãŒå­˜åœ¨ã—ã¾ã™:")
            print()
            for col, count in nan_cols.items():
                nan_rate = (count / len(df)) * 100
                print(f"  {col}: {count}ä»¶ ({nan_rate:.2f}%)")
            print()
        else:
            print("âœ… ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã«ã¯ NaNå€¤ãŒã‚ã‚Šã¾ã›ã‚“")
            print()

        # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’è¡¨ç¤ºï¼ˆæœ€åˆã®3è¡Œï¼‰
        print("ğŸ“‹ ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ï¼ˆæœ€åˆã®3è¡Œã€æœ€åˆã®10ã‚«ãƒ©ãƒ ï¼‰:")
        print()
        sample_cols = df.columns[:10].tolist()
        print(df[sample_cols].head(3).to_string())
        print()

    except Exception as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()
        print()

def main():
    print("=" * 70)
    print("ğŸ” æ¬ æå€¤å‡¦ç†ã®è¿½è·¡ãƒ‡ãƒãƒƒã‚°ã‚¹ã‚¯ãƒªãƒ—ãƒˆ")
    print("=" * 70)
    print()

    # 1. feature_engine.py ã® fillna ä½¿ç”¨ç®‡æ‰€
    check_fillna_in_feature_engine()

    # 2. features.yaml ã® imputation è¨­å®š
    check_features_yaml_imputation()

    # 3. å…ƒãƒ‡ãƒ¼ã‚¿ã®æ¬ æå€¤ç¢ºèª
    check_source_data_nan()

    # 4. ç”Ÿæˆã•ã‚ŒãŸç‰¹å¾´é‡ãƒ‡ãƒ¼ã‚¿ã®è©³ç´°ç¢ºèª
    check_generated_features_nan_sample()

    # ã¾ã¨ã‚
    print("=" * 70)
    print("ğŸ“‹ ã¾ã¨ã‚")
    print("=" * 70)
    print()
    print("ä¸Šè¨˜ã®èª¿æŸ»çµæœã‹ã‚‰ã€ä»¥ä¸‹ã‚’ç¢ºèªã—ã¦ãã ã•ã„:")
    print()
    print("1. feature_engine.py ã§ fillna ãŒä½¿ã‚ã‚Œã¦ã„ã‚‹å ´åˆ:")
    print("   â†’ ç‰¹å¾´é‡ç”Ÿæˆæ™‚ã«æ¬ æå€¤å‡¦ç†ãŒè¡Œã‚ã‚Œã¦ã„ã‚‹")
    print("   â†’ predict_bulk.py ã® fillna(0) ã¯äºŒé‡å‡¦ç†ã®å¯èƒ½æ€§")
    print()
    print("2. å…ƒãƒ‡ãƒ¼ã‚¿ï¼ˆshutuba.parquet, races.parquetï¼‰ã«æ¬ æå€¤ãŒã‚ã‚‹å ´åˆ:")
    print("   â†’ ãƒ‘ãƒ¼ã‚¹æ®µéšã§ã¯æ¬ æå€¤ãŒæ®‹ã£ã¦ã„ã‚‹")
    print("   â†’ ç‰¹å¾´é‡ç”Ÿæˆæ™‚ã«å‡¦ç†ã•ã‚Œã¦ã„ã‚‹")
    print()
    print("3. ç‰¹å¾´é‡ãƒ‡ãƒ¼ã‚¿ã« NaN ãŒãªã„å ´åˆ:")
    print("   â†’ ç‰¹å¾´é‡ç”Ÿæˆæ™‚ã«å®Œå…¨ã«å‡¦ç†æ¸ˆã¿")
    print("   â†’ predict_bulk.py ã® fillna(0) ã¯ä¸è¦ãªå¯èƒ½æ€§")
    print()
    print("=" * 70)

if __name__ == "__main__":
    try:
        main()
    except Exception as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
