#!/usr/bin/env python3
# src/features/feature_engine.py
"""
ç‰¹å¾´é‡ç”Ÿæˆã‚¨ãƒ³ã‚¸ãƒ³ (ãƒ¬ã‚·ãƒ”ãƒ™ãƒ¼ã‚¹)
"""
import logging
import re
from typing import Dict, List, Optional
import pandas as pd
import numpy as np
import yaml
from pathlib import Path

class FeatureEngine:
    """
    YAMLè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã«åŸºã¥ã„ã¦å‹•çš„ã«ç‰¹å¾´é‡ã‚’ç”Ÿæˆã™ã‚‹ã‚¨ãƒ³ã‚¸ãƒ³ã€‚
    """
    
    def __init__(self, config_or_path):
        """
        Args:
            config_or_path (str | Path | dict): features.yamlã¸ã®ãƒ‘ã‚¹ ã¾ãŸã¯ è¨­å®šè¾æ›¸
        """
        if isinstance(config_or_path, (str, Path)):
            with open(config_or_path, 'r', encoding='utf-8') as f:
                self.config = yaml.safe_load(f)
        elif isinstance(config_or_path, dict):
            self.config = config_or_path
        else:
            raise TypeError(f"config_or_path must be str, Path, or dict. Got {type(config_or_path)}")

        self.recipes = self.config.get('feature_recipes', {})
        self.feature_names_ = []
        logging.info(f"FeatureEngine (v{self.config.get('feature_engine', {}).get('version', 'N/A')}) ãŒåˆæœŸåŒ–ã•ã‚Œã¾ã—ãŸ")

    def generate_features(
        self,
        shutuba_df: pd.DataFrame,
        results_history_df: pd.DataFrame,
        horse_profiles_df: pd.DataFrame,
        pedigree_df: Optional[pd.DataFrame] = None,
    ) -> pd.DataFrame:
        """
        shutuba_df ã¨å„ç¨®å±¥æ­´ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ç‰¹å¾´é‡ã‚’ç”Ÿæˆã™ã‚‹
        """
        logging.info("ãƒ¬ã‚·ãƒ”ãƒ™ãƒ¼ã‚¹ã®ç‰¹å¾´é‡ç”Ÿæˆãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’é–‹å§‹ã—ã¾ã™...")
        df = shutuba_df.copy()

        # --- å‰å‡¦ç†: å¿…è¦ãªIDã‚’ãƒãƒ¼ã‚¸ ---
        if not horse_profiles_df.empty:
            ped_cols = ['horse_id', 'sire_id', 'damsire_id']
            ped_cols_to_merge = [col for col in ped_cols if col in horse_profiles_df.columns]
            if 'horse_id' in ped_cols_to_merge:
                df = df.merge(horse_profiles_df[ped_cols_to_merge], on='horse_id', how='left')

        # --- ãƒ¬ã‚·ãƒ”ã«åŸºã¥ã„ã¦ç‰¹å¾´é‡ç”Ÿæˆãƒ¡ã‚½ãƒƒãƒ‰ã‚’ãƒ‡ã‚£ã‚¹ãƒ‘ãƒƒãƒ ---
        if self.recipes.get('basic_features', {}).get('enabled', False):
            df = self._add_basic_features(df)
        
        # å…±é€šã®æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ä½œæˆ
        combined_df = self._create_combined_timeseries_df(df, results_history_df)

        if self.recipes.get('career_stats', {}).get('enabled', False):
            combined_df = self._add_career_stats(combined_df)

        if self.recipes.get('past_performance', {}).get('enabled', False):
            combined_df = self._add_past_performance_features(combined_df)

        if self.recipes.get('change_flags', {}).get('enabled', False):
            combined_df = self._add_change_flags(combined_df)
        
        # --- é›†è¨ˆãƒ™ãƒ¼ã‚¹ã®ç‰¹å¾´é‡ã‚’ãƒãƒ¼ã‚¸ ---
        # é¨æ‰‹ãƒ»èª¿æ•™å¸«ãƒ»è¡€çµ±ãªã©ã®é›†è¨ˆç‰¹å¾´é‡
        if self.recipes.get('entity_stats', {}).get('enabled', False):
            df = self._add_entity_stats(df, results_history_df)

        # --- äº¤äº’ä½œç”¨ç‰¹å¾´é‡ ---
        if self.recipes.get('interaction_features', {}).get('enabled', False):
            df = self._add_interaction_features(df, results_history_df)

        # --- ãƒ¬ãƒ¼ã‚¹å†…ç›¸å¯¾ç‰¹å¾´é‡ ---
        # career_stats ãªã©ã€ä»–ã®ç‰¹å¾´é‡ãŒå‡ºæƒã£ãŸå¾Œã§è¨ˆç®—
        # ã¾ãšã¯ç”Ÿæˆã•ã‚ŒãŸç‰¹å¾´é‡ã‚’ãƒãƒ¼ã‚¸
        if 'is_target_race' in combined_df.columns:
            generated_features = combined_df[combined_df['is_target_race'] == 1]
            # is_target_race ã¨ is_win ã¯ãƒãƒ¼ã‚¸å¯¾è±¡ã‹ã‚‰é™¤å¤–
            cols_to_merge = [c for c in generated_features.columns if c not in df.columns or c in ['race_id', 'horse_id']]
            df = df.merge(generated_features[cols_to_merge], on=['race_id', 'horse_id'], how='left')

        if self.recipes.get('relative_features', {}).get('enabled', False):
            df = self._add_relative_features(df)

        # --- é«˜åº¦ãªç‰¹å¾´é‡ (Advanced Features) ---
        # Phase D: ROIå‘ä¸Šã®ãŸã‚ã€æœªä½¿ç”¨ã®é«˜åº¦ãªç‰¹å¾´é‡ã‚’è¿½åŠ 
        try:
            # srcãŒãƒ‘ã‚¹ã«é€šã£ã¦ã„ã‚‹å‰æ
            from features.advanced_features import AdvancedFeatureEngine
            adv_engine = AdvancedFeatureEngine()

            logging.info("AdvancedFeatureEngine ã‚’ä½¿ç”¨ã—ã¦é«˜åº¦ãªç‰¹å¾´é‡ã‚’ç”Ÿæˆã—ã¾ã™...")

            # 1. ãƒšãƒ¼ã‚¹ãƒ»è„šè³ª
            df = adv_engine.generate_pace_features(df)

            # 2. è¡€çµ± (Deep)
            if pedigree_df is not None and not pedigree_df.empty:
                df = adv_engine.generate_deep_pedigree_features(df, pedigree_df, results_history_df)

            # 3. ã‚³ãƒ¼ã‚¹ãƒã‚¤ã‚¢ã‚¹
            df = adv_engine.generate_course_bias_features(df, results_history_df)

            # 4. ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒˆãƒ¬ãƒ³ãƒ‰
            df = adv_engine.generate_performance_trend_features(df, results_history_df)

            # 5. é¨æ‰‹Ã—èª¿æ•™å¸«ã®ç›¸æ€§ (Synergy)
            df = adv_engine.generate_jockey_trainer_synergy(df, results_history_df)

            # ğŸ†• Phase D: ä»¥ä¸‹3ã¤ã®ç‰¹å¾´é‡ã‚’æ–°è¦è¿½åŠ  (ROIå‘ä¸Šã®ãŸã‚)

            # 6. ã‚³ãƒ¼ã‚¹é©æ€§ç‰¹å¾´é‡ (ç«¶é¦¬å ´åˆ¥ãƒ»è·é›¢åˆ¥ãƒ»é¦¬å ´åˆ¥æˆç¸¾)
            logging.info("ã‚³ãƒ¼ã‚¹é©æ€§ç‰¹å¾´é‡ã‚’ç”Ÿæˆä¸­...")
            df = adv_engine.generate_course_affinity_features(df, results_history_df)

            # 7. ãƒ¬ãƒ¼ã‚¹æ¡ä»¶ç‰¹å¾´é‡ (ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚µã‚¤ã‚ºãƒ»å­£ç¯€æ€§ãƒ»ãƒ¬ãƒ¼ã‚¹é‡è¦åº¦)
            logging.info("ãƒ¬ãƒ¼ã‚¹æ¡ä»¶ç‰¹å¾´é‡ã‚’ç”Ÿæˆä¸­...")
            df = adv_engine.generate_race_condition_features(df)

            # 8. ç›¸å¯¾æŒ‡æ¨™ (ã‚¿ã‚¤ãƒ åå·®å€¤ãƒ»ä¸ŠãŒã‚Š3Fç›¸å¯¾å€¤ãƒ»ã‚ªãƒƒã‚ºé †ä½)
            logging.info("ãƒ¬ãƒ¼ã‚¹å†…ç›¸å¯¾æŒ‡æ¨™ã‚’ç”Ÿæˆä¸­...")
            df = adv_engine.calculate_relative_metrics(df)

            logging.info("âœ“ Phase D: æ–°è¦ç‰¹å¾´é‡ 3ã‚«ãƒ†ã‚´ãƒªã‚’è¿½åŠ å®Œäº†")

        except ImportError as e:
            logging.warning(f"AdvancedFeatureEngineã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
            # ãƒ‘ã‚¹ãŒé€šã£ã¦ã„ãªã„å ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼ˆç›¸å¯¾ã‚¤ãƒ³ãƒãƒ¼ãƒˆï¼‰
            try:
                from .advanced_features import AdvancedFeatureEngine
                adv_engine = AdvancedFeatureEngine()
                # å†è©¦è¡Œ... (ã‚³ãƒ¼ãƒ‰é‡è¤‡ã‚’é¿ã‘ã‚‹ãŸã‚ã“ã“ã§ã¯ãƒ­ã‚°ã®ã¿)
                logging.info("ç›¸å¯¾ã‚¤ãƒ³ãƒãƒ¼ãƒˆã§æˆåŠŸã—ã¾ã—ãŸã€‚å†å®Ÿè¡Œã¯å®Ÿè£…ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
            except:
                pass
        except Exception as e:
            logging.error(f"é«˜åº¦ãªç‰¹å¾´é‡ã®ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)

        # --- å¾Œå‡¦ç† ---
        df = self._handle_missing_values(df)
        self.feature_names_ = self._select_features(df)
        
        # âš ï¸ ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯é˜²æ­¢: ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå¤‰æ•°ã‚’æ˜ç¤ºçš„ã«é™¤å¤–
        target_variables = [
            'finish_position', 'finish_time_seconds', 'is_win',
            'prize_money', 'odds', 'win_odds', 'popularity',
            'margin_seconds', 'finish_time_str', 'margin_str'
        ]
        
        # self.feature_names_ã‹ã‚‰ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå¤‰æ•°ã‚’é™¤å¤–
        self.feature_names_ = [col for col in self.feature_names_ if col not in target_variables]
        logging.info(f"ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå¤‰æ•°ã‚’é™¤å¤–ã—ã¾ã—ãŸ: {[v for v in target_variables if v in df.columns]}")
        
        # â˜… race_dateã‚’æ˜ç¤ºçš„ã«ä¿æŒï¼ˆãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³åŒ–ã«å¿…è¦ï¼‰â˜…
        # _select_features()ã¯æ•°å€¤å‹ã®ã¿ã‚’é¸æŠã™ã‚‹ãŸã‚ã€datetimeå‹ã®race_dateã¯é™¤å¤–ã•ã‚Œã‚‹
        # ã—ã‹ã—ã€race_dateã¯ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³åŒ–ï¼ˆyear, monthåˆ†å‰²ï¼‰ã«å¿…é ˆãªã®ã§æ˜ç¤ºçš„ã«è¿½åŠ 
        final_cols = ['race_id', 'horse_id']
        if 'race_date' in df.columns:
            final_cols.append('race_date')
        final_cols.extend(self.feature_names_)
        final_cols_exist = [col for col in final_cols if col in df.columns]

        # â˜… æœ€çµ‚çš„ãªé‡è¤‡æ’é™¤ï¼ˆå®‰å…¨å¯¾ç­–ï¼‰â˜…
        # ç‰¹å¾´é‡ç”Ÿæˆéç¨‹ã§ã®ãƒãƒ¼ã‚¸æ“ä½œã«ã‚ˆã‚Šæ„å›³ã—ãªã„é‡è¤‡ãŒç™ºç”Ÿã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ãŸã‚ã€
        # æœ€çµ‚æ®µéšã§(race_id, horse_id)ã®çµ„ã¿åˆã‚ã›ãŒä¸€æ„ã«ãªã‚‹ã‚ˆã†ä¿è¨¼ã™ã‚‹
        initial_rows = len(df)
        df = df.drop_duplicates(subset=['race_id', 'horse_id'], keep='first')
        final_rows = len(df)

        if initial_rows > final_rows:
            duplicates_removed = initial_rows - final_rows
            logging.warning(f"é‡è¤‡è¡Œã‚’æ¤œå‡ºã—å‰Šé™¤ã—ã¾ã—ãŸ: {duplicates_removed}è¡Œ ({duplicates_removed/initial_rows*100:.2f}%)")
            logging.warning(f"  é‡è¤‡å‰: {initial_rows:,}è¡Œ â†’ é‡è¤‡å¾Œ: {final_rows:,}è¡Œ")
        else:
            logging.debug(f"é‡è¤‡ãƒã‚§ãƒƒã‚¯å®Œäº†: é‡è¤‡ãªã—ï¼ˆ{final_rows:,}è¡Œï¼‰")

        logging.info(f"ç‰¹å¾´é‡ç”Ÿæˆå®Œäº†ã€‚{len(self.feature_names_)}å€‹ã®ç‰¹å¾´é‡ã‚’ç”Ÿæˆã—ã¾ã—ãŸã€‚")
        return df[final_cols_exist]

    def _add_basic_features(self, df: pd.DataFrame) -> pd.DataFrame:
        logging.debug("ãƒ¬ã‚·ãƒ”ã«åŸºã¥ã„ã¦åŸºæœ¬ç‰¹å¾´é‡ã‚’è¿½åŠ ä¸­...")
        recipe = self.recipes.get('basic_features', {})
        
        # One-Hot Encoding
        for ohe_recipe in recipe.get('one_hot_encoding', []):
            col = ohe_recipe.get('column')
            prefix = ohe_recipe.get('prefix')
            if col in df.columns:
                dummies = pd.get_dummies(df[col], prefix=prefix, dtype=int)
                df = pd.concat([df, dummies], axis=1)
        
        # Bracket Categorization
        bracket_recipe = recipe.get('bracket_categorization', {})
        if bracket_recipe.get('enabled') and 'bracket_number' in df.columns:
            df['bracket_is_inner'] = df['bracket_number'].isin(bracket_recipe.get('inner', [])).astype(int)
            df['bracket_is_middle'] = df['bracket_number'].isin(bracket_recipe.get('middle', [])).astype(int)
            df['bracket_is_outer'] = df['bracket_number'].isin(bracket_recipe.get('outer', [])).astype(int)
            
        return df

    def _create_combined_timeseries_df(self, df: pd.DataFrame, history_df: pd.DataFrame) -> pd.DataFrame:
        logging.debug("æ™‚ç³»åˆ—åˆ†æç”¨ã®çµ±åˆãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ä½œæˆä¸­...")
        history_df = history_df.copy()
        history_df['is_target_race'] = 0
        df = df.copy()
        df['is_target_race'] = 1
        
        combined = pd.concat([history_df, df], ignore_index=True, sort=False)
        combined = combined.drop_duplicates(subset=['race_id', 'horse_id'], keep='last')
        
        combined['race_date'] = pd.to_datetime(combined['race_date']).dt.tz_localize(None)
        combined = combined.sort_values(by=['horse_id', 'race_date'], ascending=[True, True])
        
        # is_win ã‚’äº‹å‰ã«è¨ˆç®—
        if 'finish_position' in combined.columns:
            combined['is_win'] = (combined['finish_position'] == 1).fillna(False).astype(int)
        else:
            combined['is_win'] = 0
            
        return combined

    def _add_career_stats(self, df: pd.DataFrame) -> pd.DataFrame:
        logging.debug("ãƒ¬ã‚·ãƒ”ã«åŸºã¥ã„ã¦é€šç®—æˆç¸¾ç‰¹å¾´é‡ã‚’è¿½åŠ ä¸­...")
        recipe = self.recipes.get('career_stats', {})
        stats_to_calc = recipe.get('stats', [])
        
        grouped = df.groupby('horse_id', sort=False)
        
        if 'career_starts' in stats_to_calc:
            df['career_starts'] = grouped.cumcount()
        
        if 'career_wins' in stats_to_calc:
            df['career_wins'] = grouped['is_win'].cumsum().shift(1).fillna(0).astype(int)
            
        if 'prize_total' in stats_to_calc:
            if 'prize_money' not in df.columns:
                df['prize_money'] = 0
            df['prize_money'] = df['prize_money'].fillna(0)
            df['prize_total'] = grouped['prize_money'].cumsum().shift(1).fillna(0)
            
        return df

    def _add_past_performance_features(self, df: pd.DataFrame) -> pd.DataFrame:
        logging.debug("ãƒ¬ã‚·ãƒ”ã«åŸºã¥ã„ã¦éå»èµ°é›†ç´„ç‰¹å¾´é‡ã‚’è¿½åŠ ä¸­...")
        recipe = self.recipes.get('past_performance', {})
        agg_cols = recipe.get('columns', [])
        windows = recipe.get('windows', [])
        aggregations = recipe.get('aggregations', [])
        
        grouped = df.groupby('horse_id', sort=False)
        
        for col in agg_cols:
            if col not in df.columns:
                logging.warning(f"éå»èµ°é›†ç´„: ã‚«ãƒ©ãƒ  '{col}' ãŒå­˜åœ¨ã—ã¾ã›ã‚“ã€‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
                continue
            
            shifted = grouped[col].shift(1)
            
            for w in windows:
                rolled = shifted.rolling(window=w, min_periods=1)
                for agg in aggregations:
                    feat_name = f'past_{w}_{col}_{agg}'
                    try:
                        agg_result = getattr(rolled, agg)()
                        df[feat_name] = agg_result.reset_index(level=0, drop=True)
                    except AttributeError:
                        logging.error(f"é›†è¨ˆé–¢æ•° '{agg}' ã¯ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
        
        # å‰èµ°ã‹ã‚‰ã®æ—¥æ•°
        df['days_since_last_race'] = grouped['race_date'].diff().dt.days
        
        return df

    def _add_change_flags(self, df: pd.DataFrame) -> pd.DataFrame:
        logging.debug("ãƒ¬ã‚·ãƒ”ã«åŸºã¥ã„ã¦ä¹—ã‚Šæ›¿ã‚ã‚Šãƒ•ãƒ©ã‚°ã‚’è¿½åŠ ä¸­...")
        recipe = self.recipes.get('change_flags', {})
        columns = recipe.get('columns', [])
        
        grouped = df.groupby('horse_id', sort=False)
        
        for col in columns:
            if col in df.columns:
                prev_col_name = f'prev_{col}'
                flag_col_name = f'is_{col}_changed'
                df[prev_col_name] = grouped[col].shift(1)
                is_changed = (df[col] != df[prev_col_name]) & (df[prev_col_name].notna())
                df[flag_col_name] = is_changed.fillna(False).astype(int)
        
        return df

    def _add_entity_stats(self, df: pd.DataFrame, history_df: pd.DataFrame) -> pd.DataFrame:
        logging.debug("ãƒ¬ã‚·ãƒ”ã«åŸºã¥ã„ã¦ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£é›†è¨ˆç‰¹å¾´é‡ã‚’è¿½åŠ ä¸­...")
        recipe = self.recipes.get('entity_stats', {})
        entities = recipe.get('entities', [])
        
        history_df = history_df.copy()
        if 'finish_position' in history_df.columns:
            history_df['is_win'] = (history_df['finish_position'] == 1).fillna(False).astype(int)
        
        for entity_recipe in entities:
            entity_name = entity_recipe.get('name')
            id_col = entity_recipe.get('id_column')
            target_col = entity_recipe.get('target')
            agg_func = entity_recipe.get('agg')
            feature_name = entity_recipe.get('feature_name')

            if id_col not in history_df.columns:
                logging.warning(f"ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£é›†è¨ˆ ({entity_name}): IDã‚«ãƒ©ãƒ  '{id_col}' ãŒå±¥æ­´ã«ã‚ã‚Šã¾ã›ã‚“ã€‚")
                continue
            if target_col not in history_df.columns:
                logging.warning(f"ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£é›†è¨ˆ ({entity_name}): ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã‚«ãƒ©ãƒ  '{target_col}' ãŒå±¥æ­´ã«ã‚ã‚Šã¾ã›ã‚“ã€‚")
                continue
            
            # æ¬ æã‚’é™¤å¤–ã—ã¦é›†è¨ˆ
            stats = history_df.dropna(subset=[id_col, target_col]).groupby(id_col)[target_col].agg(agg_func).reset_index()
            stats = stats.rename(columns={target_col: feature_name})
            
            if id_col in df.columns and not stats.empty:
                df = df.merge(stats, on=id_col, how='left')
        
        return df

    def _add_interaction_features(self, df: pd.DataFrame, history_df: pd.DataFrame) -> pd.DataFrame:
        """
        äº¤äº’ä½œç”¨ç‰¹å¾´é‡ã‚’è¿½åŠ ï¼ˆé¨æ‰‹Ã—ç«¶é¦¬å ´ã€ç¨®ç‰¡é¦¬Ã—é¦¬å ´ãªã©ï¼‰

        Args:
            df: äºˆæ¸¬å¯¾è±¡ãƒ‡ãƒ¼ã‚¿
            history_df: éå»ã®ãƒ¬ãƒ¼ã‚¹å±¥æ­´ãƒ‡ãƒ¼ã‚¿

        Returns:
            äº¤äº’ä½œç”¨ç‰¹å¾´é‡ãŒè¿½åŠ ã•ã‚ŒãŸDataFrame
        """
        logging.debug("ãƒ¬ã‚·ãƒ”ã«åŸºã¥ã„ã¦äº¤äº’ä½œç”¨ç‰¹å¾´é‡ã‚’è¿½åŠ ä¸­...")
        interaction_config = self.recipes.get('interaction_features', {})

        # YAMLæ§‹é€ : interaction_features.interactions ãŒãƒªã‚¹ãƒˆ
        if isinstance(interaction_config, dict):
            interaction_recipes = interaction_config.get('interactions', [])
        else:
            interaction_recipes = []

        if not interaction_recipes:
            logging.debug("äº¤äº’ä½œç”¨ç‰¹å¾´é‡ã®ãƒ¬ã‚·ãƒ”ãŒå®šç¾©ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
            return df

        # distance_categoryãŒå­˜åœ¨ã—ãªã„å ´åˆã¯ä½œæˆ
        if 'distance_m' in df.columns and 'distance_category' not in df.columns:
            df = self._add_distance_category(df)
        if 'distance_m' in history_df.columns and 'distance_category' not in history_df.columns:
            history_df = self._add_distance_category(history_df)

        # is_winã‚«ãƒ©ãƒ ã®ç¢ºä¿
        history_df = history_df.copy()
        if 'finish_position' in history_df.columns and 'is_win' not in history_df.columns:
            history_df['is_win'] = (history_df['finish_position'] == 1).fillna(False).astype(int)

        for interaction_recipe in interaction_recipes:
            if not isinstance(interaction_recipe, dict):
                continue

            interaction_name = interaction_recipe.get('name')
            id_col = interaction_recipe.get('id_column')
            context_col = interaction_recipe.get('context_column')
            target_col = interaction_recipe.get('target')
            agg_func = interaction_recipe.get('agg')
            feature_template = interaction_recipe.get('feature_template')

            # å¿…é ˆã‚«ãƒ©ãƒ ã®ãƒã‚§ãƒƒã‚¯
            if id_col not in history_df.columns:
                logging.warning(f"äº¤äº’ä½œç”¨ ({interaction_name}): IDã‚«ãƒ©ãƒ  '{id_col}' ãŒå±¥æ­´ã«ã‚ã‚Šã¾ã›ã‚“ã€‚")
                continue
            if context_col not in history_df.columns:
                logging.warning(f"äº¤äº’ä½œç”¨ ({interaction_name}): ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚«ãƒ©ãƒ  '{context_col}' ãŒå±¥æ­´ã«ã‚ã‚Šã¾ã›ã‚“ã€‚")
                continue
            if target_col not in history_df.columns:
                logging.warning(f"äº¤äº’ä½œç”¨ ({interaction_name}): ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã‚«ãƒ©ãƒ  '{target_col}' ãŒå±¥æ­´ã«ã‚ã‚Šã¾ã›ã‚“ã€‚")
                continue

            # æ¬ æã‚’é™¤å¤–ã—ã¦é›†è¨ˆ
            valid_history = history_df.dropna(subset=[id_col, context_col, target_col])

            if valid_history.empty:
                logging.warning(f"äº¤äº’ä½œç”¨ ({interaction_name}): æœ‰åŠ¹ãªå±¥æ­´ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
                continue

            # äº¤äº’ä½œç”¨ã”ã¨ã«é›†è¨ˆ
            grouped = valid_history.groupby([id_col, context_col])[target_col].agg(agg_func).reset_index()

            # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå€¤ã”ã¨ã«å€‹åˆ¥ã®ã‚«ãƒ©ãƒ ã‚’ä½œæˆï¼ˆãƒ”ãƒœãƒƒãƒˆï¼‰
            for context_value in grouped[context_col].unique():
                # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå€¤ã®ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«åã«ä½¿ãˆã‚‹å½¢å¼ã«ï¼‰
                context_value_clean = str(context_value).replace(' ', '_').replace('/', '_')
                feature_name = feature_template.format(context=context_value_clean)

                # ã“ã®ç‰¹å®šã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå€¤ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
                context_stats = grouped[grouped[context_col] == context_value][[id_col, target_col]].copy()
                context_stats = context_stats.rename(columns={target_col: feature_name})

                # ãƒãƒ¼ã‚¸
                if id_col in df.columns and not context_stats.empty:
                    df = df.merge(context_stats, on=id_col, how='left')
                    logging.debug(f"äº¤äº’ä½œç”¨ç‰¹å¾´é‡ '{feature_name}' ã‚’è¿½åŠ ã—ã¾ã—ãŸã€‚")

        return df

    def _add_distance_category(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        è·é›¢ã‚«ãƒ†ã‚´ãƒªã‚’è¿½åŠ ï¼ˆsprint, mile, intermediate, long, marathonï¼‰
        """
        if 'distance_m' not in df.columns:
            return df

        def categorize_distance(distance):
            if pd.isna(distance):
                return 'unknown'
            if distance < 1400:
                return 'sprint'
            elif distance < 1800:
                return 'mile'
            elif distance < 2200:
                return 'intermediate'
            elif distance < 2800:
                return 'long'
            else:
                return 'marathon'

        df['distance_category'] = df['distance_m'].apply(categorize_distance)
        return df

    def _add_relative_features(self, df: pd.DataFrame) -> pd.DataFrame:
        logging.debug("ãƒ¬ã‚·ãƒ”ã«åŸºã¥ã„ã¦ãƒ¬ãƒ¼ã‚¹å†…ç›¸å¯¾ç‰¹å¾´é‡ã‚’è¿½åŠ ä¸­...")
        recipe = self.recipes.get('relative_features', {})
        zscore_cols = recipe.get('z_score', {}).get('columns', [])
        
        if 'race_id' not in df.columns:
            return df
             
        grouped = df.groupby('race_id')
        
        for col in zscore_cols:
            if col in df.columns:
                feat_name = f'{col}_zscore'
                try:
                    df[feat_name] = grouped[col].transform(
                        lambda x: (x - x.mean()) / (x.std() + 1e-6)
                    )
                except Exception as e:
                     logging.warning(f"Z-score ({col}) ã®è¨ˆç®—ã«å¤±æ•—: {e}")
                     df[feat_name] = 0.0
        
        return df

    def _handle_missing_values(self, df: pd.DataFrame) -> pd.DataFrame:
        logging.debug("æ¬ æå€¤ã‚’å‡¦ç†ä¸­...")
        recipe = self.config.get('imputation', {})
        strategy = recipe.get('numeric_strategy', 'median')
        default_value = recipe.get('default_value', 0.0)
        
        num_cols = df.select_dtypes(include=np.number).columns
        
        # ç‰¹å¾´é‡ã¨ã—ã¦é¸æŠã•ã‚Œã‚‹å¯èƒ½æ€§ã®ã‚ã‚‹æ•°å€¤ã‚«ãƒ©ãƒ ã®ã¿ã‚’å¯¾è±¡
        # ã“ã®æ™‚ç‚¹ã§ã¯ã¾ã  self.feature_names_ ãŒç¢ºå®šã—ã¦ã„ãªã„ã®ã§ã€é™¤å¤–ãƒ‘ã‚¿ãƒ¼ãƒ³ã§åˆ¤å®š
        temp_feature_names = self._select_features(df.copy())
        cols_to_fill = [col for col in num_cols if col in temp_feature_names]
        
        for col in cols_to_fill:
            if df[col].isnull().any():
                if strategy == 'median':
                    fill_value = df[col].median()
                elif strategy == 'mean':
                    fill_value = df[col].mean()
                else: # zero
                    fill_value = 0
                
                if pd.isna(fill_value):
                    fill_value = default_value
                    
                df[col] = df[col].fillna(fill_value)
        
        return df

    def _select_features(self, df: pd.DataFrame) -> List[str]:
        logging.debug("ä½¿ç”¨ã™ã‚‹ç‰¹å¾´é‡ã‚’é¸æŠä¸­...")
        recipe = self.config.get('feature_selection', {})
        exclude_patterns = recipe.get('exclude_patterns', [])
        
        feature_cols = []
        all_cols = df.columns.tolist()

        for col in all_cols:
            is_excluded = False
            for pattern in exclude_patterns:
                try:
                    if re.match(pattern, col):
                        is_excluded = True
                        break
                except re.error as e:
                    logging.warning(f"ç‰¹å¾´é‡é¸æŠã®é™¤å¤–ãƒ‘ã‚¿ãƒ¼ãƒ³ã§æ­£è¦è¡¨ç¾ã‚¨ãƒ©ãƒ¼: '{pattern}' - {e}")
                    continue
            
            if not is_excluded:
                # ã•ã‚‰ã«æ•°å€¤å‹ã§ã‚ã‚‹ã“ã¨ã‚‚ç¢ºèª
                if pd.api.types.is_numeric_dtype(df[col]):
                    feature_cols.append(col)

        return sorted(list(dict.fromkeys(feature_cols)))

    def save_features(
        self,
        features_df: pd.DataFrame,
        output_dir: str,
        partition_cols: List[str]
    ):
        """
        ç”Ÿæˆã—ãŸç‰¹å¾´é‡ã‚’Parquetå½¢å¼ã§ä¿å­˜
        """
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)
        
        try:
            valid_partition_cols = [col for col in partition_cols if col in features_df.columns]
            
            if valid_partition_cols:
                 logging.info(f"ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ã‚«ãƒ©ãƒ  {valid_partition_cols} ã‚’ä½¿ç”¨ã—ã¦ä¿å­˜ã—ã¾ã™")
                 features_df.to_parquet(
                    output_path,
                    engine='pyarrow',
                    compression='snappy',
                    partition_cols=valid_partition_cols,
                    existing_data_behavior='delete_matching'
                )
            else:
                 logging.warning(f"ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ã‚«ãƒ©ãƒ  {partition_cols} ãŒDFã«ãªã„ãŸã‚ã€å˜ä¸€ãƒ•ã‚¡ã‚¤ãƒ«ã§ä¿å­˜ã—ã¾ã™")
                 features_df.to_parquet(
                    output_path / "features.parquet",
                    engine='pyarrow',
                    compression='snappy'
                )
            logging.info(f"ç‰¹å¾´é‡ã‚’ {output_path} ã«ä¿å­˜ã—ã¾ã—ãŸ")

            # ç‰¹å¾´é‡ãƒªã‚¹ãƒˆã‚’ä¿å­˜
            features_list_path = output_path / "feature_names.yaml"
            try:
                with open(features_list_path, 'w', encoding='utf-8') as f:
                    yaml.dump(self.feature_names_, f, allow_unicode=True)
                logging.info(f"ç‰¹å¾´é‡ãƒªã‚¹ãƒˆã‚’ {features_list_path} ã«ä¿å­˜ã—ã¾ã—ãŸ")
            except Exception as e:
                 logging.error(f"ç‰¹å¾´é‡ãƒªã‚¹ãƒˆ (feature_names.yaml) ã®ä¿å­˜ã«å¤±æ•—: {e}")

        except Exception as e:
            logging.error(f"ç‰¹å¾´é‡ã®Parquetä¿å­˜ã«å¤±æ•—: {e}", exc_info=True)
            if "partition_cols" in str(e):
                 logging.error("ãƒ’ãƒ³ãƒˆ: ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ã‚«ãƒ©ãƒ  (year, month) ãŒDataFrameã«å­˜åœ¨ã™ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
