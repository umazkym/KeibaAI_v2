#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ãƒ‘ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼ã®è©³ç´°åˆ†æã‚¹ã‚¯ãƒªãƒ—ãƒˆ

å®Ÿéš›ã«ãƒ‘ãƒ¼ã‚µãƒ¼ã‚’å®Ÿè¡Œã—ã¦ã€ã©ã“ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã™ã‚‹ã‹ã‚’ç‰¹å®šï¼š
1. ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒªã‚¹ãƒˆ
2. ã‚¨ãƒ©ãƒ¼ã®ç¨®é¡ã¨é »åº¦
3. ã‚¨ãƒ©ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ã®è©³ç´°å†…å®¹
4. æ­£å¸¸ãƒ•ã‚¡ã‚¤ãƒ«ã¨ã®æ¯”è¼ƒ
"""

import sys
from pathlib import Path
from collections import defaultdict, Counter
from datetime import datetime
import json
import traceback
from typing import Dict, List, Optional

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆ
project_root = Path(__file__).resolve().parent / "keibaai"
sys.path.append(str(project_root))

from src.modules.parsers import results_parser, shutuba_parser


class ParseErrorAnalyzer:
    """ãƒ‘ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼ã‚’åˆ†æã™ã‚‹ã‚¯ãƒ©ã‚¹"""

    def __init__(self, data_dir: Path):
        self.data_dir = data_dir
        self.html_dir = data_dir / "raw" / "html"
        self.error_details = []
        self.success_count = 0
        self.error_count = 0
        self.error_types = Counter()

    def analyze_race_files(self, year_filter: Optional[int] = None, max_files: int = 100):
        """ãƒ¬ãƒ¼ã‚¹çµæœãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ‘ãƒ¼ã‚¹åˆ†æ"""
        print(f"\nğŸ“„ ãƒ¬ãƒ¼ã‚¹çµæœãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ãƒ¼ã‚¹åˆ†æ")
        print("=" * 80)

        race_dir = self.html_dir / "race"
        if not race_dir.exists():
            print(f"âŒ ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {race_dir}")
            return

        files = list(race_dir.glob("*.bin"))

        # å¹´ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼
        if year_filter:
            files = [f for f in files if f.stem.startswith(str(year_filter))]
            print(f"ğŸ” {year_filter}å¹´ã®ãƒ•ã‚¡ã‚¤ãƒ«ã«çµã‚Šè¾¼ã¿: {len(files)}ä»¶")

        # æœ€å¤§ä»¶æ•°åˆ¶é™
        if len(files) > max_files:
            print(f"âš ï¸  ãƒ•ã‚¡ã‚¤ãƒ«æ•°ãŒå¤šã„ãŸã‚ã€æœ€åˆã®{max_files}ä»¶ã®ã¿åˆ†æã—ã¾ã™")
            files = files[:max_files]

        print(f"ğŸ“Š åˆ†æå¯¾è±¡: {len(files)}ä»¶\n")

        # ãƒ‘ãƒ¼ã‚¹å®Ÿè¡Œ
        for i, file_path in enumerate(files, 1):
            race_id = file_path.stem
            print(f"  [{i}/{len(files)}] {race_id}...", end=" ")

            try:
                # ãƒ‘ãƒ¼ã‚¹å®Ÿè¡Œ
                df = results_parser.parse_race_results_from_file(str(file_path))

                if df is None or df.empty:
                    print("âŒ ç©ºã®DataFrame")
                    self.error_count += 1
                    self.error_types['empty_dataframe'] += 1
                    self._record_error(file_path, 'empty_dataframe', 'ãƒ‘ãƒ¼ã‚¹çµæœãŒç©º')
                else:
                    print(f"âœ… {len(df)}è¡Œ")
                    self.success_count += 1

            except Exception as e:
                print(f"âŒ {type(e).__name__}")
                self.error_count += 1
                error_type = type(e).__name__
                self.error_types[error_type] += 1
                self._record_error(file_path, error_type, str(e), traceback.format_exc())

        print("\n" + "-" * 80)
        print(f"âœ… æˆåŠŸ: {self.success_count}ä»¶")
        print(f"âŒ å¤±æ•—: {self.error_count}ä»¶")

    def analyze_shutuba_files(self, year_filter: Optional[int] = None, max_files: int = 100):
        """å‡ºé¦¬è¡¨ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ‘ãƒ¼ã‚¹åˆ†æ"""
        print(f"\nğŸ“„ å‡ºé¦¬è¡¨ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ãƒ¼ã‚¹åˆ†æ")
        print("=" * 80)

        shutuba_dir = self.html_dir / "shutuba"
        if not shutuba_dir.exists():
            print(f"âŒ ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {shutuba_dir}")
            return

        files = list(shutuba_dir.glob("*.bin"))

        # å¹´ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼
        if year_filter:
            files = [f for f in files if f.stem.startswith(str(year_filter))]
            print(f"ğŸ” {year_filter}å¹´ã®ãƒ•ã‚¡ã‚¤ãƒ«ã«çµã‚Šè¾¼ã¿: {len(files)}ä»¶")

        # æœ€å¤§ä»¶æ•°åˆ¶é™
        if len(files) > max_files:
            print(f"âš ï¸  ãƒ•ã‚¡ã‚¤ãƒ«æ•°ãŒå¤šã„ãŸã‚ã€æœ€åˆã®{max_files}ä»¶ã®ã¿åˆ†æã—ã¾ã™")
            files = files[:max_files]

        print(f"ğŸ“Š åˆ†æå¯¾è±¡: {len(files)}ä»¶\n")

        # ãƒ‘ãƒ¼ã‚¹å®Ÿè¡Œ
        success = 0
        error = 0

        for i, file_path in enumerate(files, 1):
            race_id = file_path.stem
            print(f"  [{i}/{len(files)}] {race_id}...", end=" ")

            try:
                # ãƒ‘ãƒ¼ã‚¹å®Ÿè¡Œ
                df = shutuba_parser.parse_shutuba_from_file(str(file_path))

                if df is None or df.empty:
                    print("âŒ ç©ºã®DataFrame")
                    error += 1
                    self.error_types['shutuba_empty_dataframe'] += 1
                    self._record_error(file_path, 'shutuba_empty_dataframe', 'ãƒ‘ãƒ¼ã‚¹çµæœãŒç©º')
                else:
                    print(f"âœ… {len(df)}è¡Œ")
                    success += 1

            except Exception as e:
                print(f"âŒ {type(e).__name__}")
                error += 1
                error_type = f"shutuba_{type(e).__name__}"
                self.error_types[error_type] += 1
                self._record_error(file_path, error_type, str(e), traceback.format_exc())

        print("\n" + "-" * 80)
        print(f"âœ… æˆåŠŸ: {success}ä»¶")
        print(f"âŒ å¤±æ•—: {error}ä»¶")

        self.success_count += success
        self.error_count += error

    def _record_error(self, file_path: Path, error_type: str, error_message: str, traceback_str: str = ""):
        """ã‚¨ãƒ©ãƒ¼è©³ç´°ã‚’è¨˜éŒ²"""
        error_info = {
            'file_path': str(file_path),
            'file_name': file_path.name,
            'file_id': file_path.stem,
            'file_size': file_path.stat().st_size,
            'error_type': error_type,
            'error_message': error_message,
            'traceback': traceback_str,
        }

        # ãƒ•ã‚¡ã‚¤ãƒ«ã®æœ€åˆã®500æ–‡å­—ã‚’è¨˜éŒ²
        try:
            with open(file_path, 'rb') as f:
                content_bytes = f.read(1000)
            try:
                content = content_bytes.decode('euc_jp', errors='replace')
            except:
                content = content_bytes.decode('utf-8', errors='replace')
            error_info['file_preview'] = content[:500]
        except Exception as e:
            error_info['file_preview'] = f"[ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}]"

        self.error_details.append(error_info)

    def print_error_summary(self):
        """ã‚¨ãƒ©ãƒ¼ã‚µãƒãƒªãƒ¼ã‚’å‡ºåŠ›"""
        print("\n" + "=" * 80)
        print("ğŸ“Š ã‚¨ãƒ©ãƒ¼ã‚µãƒãƒªãƒ¼")
        print("=" * 80)

        print(f"\nâ–  ã‚¨ãƒ©ãƒ¼çµ±è¨ˆ:")
        print(f"  ç·ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {self.success_count + self.error_count:,}")
        print(f"  æˆåŠŸ: {self.success_count:,} ({self.success_count/(self.success_count+self.error_count)*100:.1f}%)")
        print(f"  å¤±æ•—: {self.error_count:,} ({self.error_count/(self.success_count+self.error_count)*100:.1f}%)")

        if self.error_types:
            print(f"\nâ–  ã‚¨ãƒ©ãƒ¼ã‚¿ã‚¤ãƒ—åˆ¥é›†è¨ˆ:")
            print(f"  {'ã‚¨ãƒ©ãƒ¼ã‚¿ã‚¤ãƒ—':<30} {'ä»¶æ•°':>10}")
            print("  " + "-" * 42)
            for error_type, count in self.error_types.most_common():
                print(f"  {error_type:<30} {count:>10,}")

        if self.error_details:
            print(f"\nâ–  ã‚¨ãƒ©ãƒ¼è©³ç´°ï¼ˆæœ€åˆã®10ä»¶ï¼‰:")
            for i, error in enumerate(self.error_details[:10], 1):
                print(f"\n  {i}. {error['file_name']} ({error['file_size']} bytes)")
                print(f"     ã‚¨ãƒ©ãƒ¼: {error['error_type']}")
                print(f"     ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸: {error['error_message'][:100]}")
                if error['file_preview']:
                    preview_lines = error['file_preview'].split('\n')[:3]
                    print(f"     ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹: {preview_lines[0][:60]}...")

    def save_report(self, output_path: Path):
        """è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆã‚’JSONã§ä¿å­˜"""
        report = {
            'metadata': {
                'analysis_datetime': datetime.now().isoformat(),
                'data_directory': str(self.data_dir),
            },
            'summary': {
                'total_files': self.success_count + self.error_count,
                'success_count': self.success_count,
                'error_count': self.error_count,
                'success_rate': self.success_count / (self.success_count + self.error_count) * 100
                               if (self.success_count + self.error_count) > 0 else 0,
            },
            'error_types': dict(self.error_types),
            'error_details': self.error_details,
        }

        output_path.parent.mkdir(parents=True, exist_ok=True)
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)

        print(f"\nğŸ’¾ è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆã‚’ä¿å­˜: {output_path}")


def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    import argparse

    parser = argparse.ArgumentParser(
        description='ãƒ‘ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼ã®è©³ç´°åˆ†æ',
        formatter_class=argparse.RawDescriptionHelpFormatter
    )
    parser.add_argument('--year', type=int, help='åˆ†æå¯¾è±¡ã®å¹´ï¼ˆä¾‹: 2025ï¼‰')
    parser.add_argument('--max-files', type=int, default=100, help='æœ€å¤§åˆ†æãƒ•ã‚¡ã‚¤ãƒ«æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 100ï¼‰')
    parser.add_argument('--race', action='store_true', help='ãƒ¬ãƒ¼ã‚¹çµæœã‚’åˆ†æ')
    parser.add_argument('--shutuba', action='store_true', help='å‡ºé¦¬è¡¨ã‚’åˆ†æ')

    args = parser.parse_args()

    print("ğŸ” KeibaAI_v2 ãƒ‘ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼è©³ç´°åˆ†æ")
    print("=" * 80)

    data_dir = Path("keibaai/data")

    if not data_dir.exists():
        print(f"âŒ ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {data_dir}")
        return

    analyzer = ParseErrorAnalyzer(data_dir)

    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ã¯ä¸¡æ–¹åˆ†æ
    analyze_race = args.race or not (args.race or args.shutuba)
    analyze_shutuba = args.shutuba or not (args.race or args.shutuba)

    if analyze_race:
        analyzer.analyze_race_files(year_filter=args.year, max_files=args.max_files)

    if analyze_shutuba:
        analyzer.analyze_shutuba_files(year_filter=args.year, max_files=args.max_files)

    # ã‚µãƒãƒªãƒ¼å‡ºåŠ›
    analyzer.print_error_summary()

    # ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    year_suffix = f"_{args.year}" if args.year else ""
    output_path = data_dir / "logs" / "analysis" / f"parse_errors{year_suffix}_{timestamp}.json"
    analyzer.save_report(output_path)

    print("\n" + "=" * 80)
    print("âœ… åˆ†æå®Œäº†!")
    print("=" * 80)


if __name__ == "__main__":
    main()
