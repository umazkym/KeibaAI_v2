#!/usr/bin/env python3
"""
KeibaAI_v2 Advanced Log Analyzer
ã‚ˆã‚Šè©³ç´°ãªåˆ†æã¨ã‚°ãƒ©ãƒ•ç”Ÿæˆæ©Ÿèƒ½ã‚’å«ã‚€æ‹¡å¼µç‰ˆãƒ­ã‚°ã‚¢ãƒŠãƒ©ã‚¤ã‚¶ãƒ¼
"""

import re
from pathlib import Path
from datetime import datetime, timedelta
from collections import defaultdict, Counter
from typing import Dict, List, Optional
import json


class AdvancedKeibaLogAnalyzer:
    """æ‹¡å¼µç‰ˆãƒ­ã‚°ã‚¢ãƒŠãƒ©ã‚¤ã‚¶ãƒ¼"""

    def __init__(self, log_dir: Path):
        self.log_dir = Path(log_dir)
        self.log_entries = []
        self.timeline = []  # æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿
        self.stats = {
            'total_entries': 0,
            'by_level': Counter(),
            'errors': [],
            'warnings': [],
            'scraping_stats': {
                'races': {'scraped': 0, 'failed': 0, 'skipped': 0},
                'shutuba': {'scraped': 0, 'failed': 0, 'skipped': 0},
                'horses': {'scraped': 0, 'failed': 0, 'skipped': 0},
                'pedigrees': {'scraped': 0, 'failed': 0, 'skipped': 0},
                'http_errors': Counter(),
                'retry_count': 0,
            },
            'parsing_stats': {
                'races': {'parsed': 0, 'failed': 0},
                'shutuba': {'parsed': 0, 'failed': 0},
                'horses': {'parsed': 0, 'failed': 0},
                'pedigrees': {'parsed': 0, 'failed': 0},
                'missing_fields': Counter(),
                'data_quality': defaultdict(list),
            },
            'performance': {
                'scraping_rate': [],  # (timestamp, count) pairs
                'parsing_rate': [],
                'execution_stages': {},
            },
        }

    def find_log_files(self) -> List[Path]:
        """ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢"""
        if not self.log_dir.exists():
            print(f"âš ï¸  ãƒ­ã‚°ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã—ã¾ã›ã‚“: {self.log_dir}")
            return []

        log_files = sorted(self.log_dir.glob("*.log"))
        return log_files

    def parse_log_line(self, line: str) -> Optional[Dict]:
        """ãƒ­ã‚°è¡Œã‚’ãƒ‘ãƒ¼ã‚¹"""
        pattern = r'(\d{4}-\d{2}-\d{2}\s+\d{2}:\d{2}:\d{2},\d{3})\s+-\s+(\w+)\s+-\s+(.+)'
        match = re.match(pattern, line)

        if match:
            timestamp_str, level, message = match.groups()
            try:
                timestamp = datetime.strptime(timestamp_str, '%Y-%m-%d %H:%M:%S,%f')
            except:
                timestamp = None

            return {
                'timestamp': timestamp,
                'level': level,
                'message': message.strip(),
                'raw': line
            }
        return None

    def analyze_log_file(self, log_file: Path):
        """ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’åˆ†æ"""
        print(f"ğŸ“„ åˆ†æä¸­: {log_file.name}")

        try:
            with open(log_file, 'r', encoding='utf-8') as f:
                for line in f:
                    line = line.strip()
                    if not line:
                        continue

                    entry = self.parse_log_line(line)
                    if entry:
                        self.log_entries.append(entry)
                        self.stats['total_entries'] += 1
                        self.stats['by_level'][entry['level']] += 1

                        if entry['level'] == 'ERROR':
                            self.stats['errors'].append(entry)
                        elif entry['level'] == 'WARNING':
                            self.stats['warnings'].append(entry)

                        self._analyze_scraping(entry)
                        self._analyze_parsing(entry)
                        self._analyze_performance(entry)

        except Exception as e:
            print(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {log_file.name} - {e}")

    def _analyze_scraping(self, entry: Dict):
        """ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã®è©³ç´°åˆ†æ"""
        msg = entry['message']
        msg_lower = msg.lower()

        # ãƒ¬ãƒ¼ã‚¹çµæœ
        if 'race' in msg_lower and 'scraping' in msg_lower:
            if 'success' in msg_lower or 'completed' in msg_lower:
                self.stats['scraping_stats']['races']['scraped'] += 1
            elif 'failed' in msg_lower or 'error' in msg_lower:
                self.stats['scraping_stats']['races']['failed'] += 1
            elif 'skip' in msg_lower:
                self.stats['scraping_stats']['races']['skipped'] += 1

        # å‡ºé¦¬è¡¨
        if 'shutuba' in msg_lower and 'scraping' in msg_lower:
            if 'success' in msg_lower or 'completed' in msg_lower:
                self.stats['scraping_stats']['shutuba']['scraped'] += 1
            elif 'failed' in msg_lower or 'error' in msg_lower:
                self.stats['scraping_stats']['shutuba']['failed'] += 1
            elif 'skip' in msg_lower:
                self.stats['scraping_stats']['shutuba']['skipped'] += 1

        # é¦¬æƒ…å ±
        if 'horse' in msg_lower and 'scraping' in msg_lower and 'pedigree' not in msg_lower:
            if 'success' in msg_lower or 'completed' in msg_lower:
                self.stats['scraping_stats']['horses']['scraped'] += 1
            elif 'failed' in msg_lower or 'error' in msg_lower:
                self.stats['scraping_stats']['horses']['failed'] += 1
            elif 'skip' in msg_lower:
                self.stats['scraping_stats']['horses']['skipped'] += 1

        # è¡€çµ±æƒ…å ±
        if 'pedigree' in msg_lower and 'scraping' in msg_lower:
            if 'success' in msg_lower or 'completed' in msg_lower:
                self.stats['scraping_stats']['pedigrees']['scraped'] += 1
            elif 'failed' in msg_lower or 'error' in msg_lower:
                self.stats['scraping_stats']['pedigrees']['failed'] += 1
            elif 'skip' in msg_lower:
                self.stats['scraping_stats']['pedigrees']['skipped'] += 1

        # HTTPã‚¨ãƒ©ãƒ¼
        http_match = re.search(r'http[s]?\s+(\d{3})', msg_lower)
        if http_match:
            status_code = http_match.group(1)
            self.stats['scraping_stats']['http_errors'][status_code] += 1

        # ãƒªãƒˆãƒ©ã‚¤
        if 'retry' in msg_lower or 'retrying' in msg_lower:
            self.stats['scraping_stats']['retry_count'] += 1

    def _analyze_parsing(self, entry: Dict):
        """ãƒ‘ãƒ¼ã‚¹ã®è©³ç´°åˆ†æ"""
        msg = entry['message']
        msg_lower = msg.lower()

        # ãƒ¬ãƒ¼ã‚¹çµæœãƒ‘ãƒ¼ã‚¹
        if 'race' in msg_lower and 'pars' in msg_lower:
            if 'success' in msg_lower or 'completed' in msg_lower:
                self.stats['parsing_stats']['races']['parsed'] += 1
            elif 'failed' in msg_lower or 'error' in msg_lower:
                self.stats['parsing_stats']['races']['failed'] += 1

        # å‡ºé¦¬è¡¨ãƒ‘ãƒ¼ã‚¹
        if 'shutuba' in msg_lower and 'pars' in msg_lower:
            if 'success' in msg_lower or 'completed' in msg_lower:
                self.stats['parsing_stats']['shutuba']['parsed'] += 1
            elif 'failed' in msg_lower or 'error' in msg_lower:
                self.stats['parsing_stats']['shutuba']['failed'] += 1

        # é¦¬æƒ…å ±ãƒ‘ãƒ¼ã‚¹
        if 'horse' in msg_lower and 'pars' in msg_lower and 'pedigree' not in msg_lower:
            if 'success' in msg_lower or 'completed' in msg_lower:
                self.stats['parsing_stats']['horses']['parsed'] += 1
            elif 'failed' in msg_lower or 'error' in msg_lower:
                self.stats['parsing_stats']['horses']['failed'] += 1

        # è¡€çµ±ãƒ‘ãƒ¼ã‚¹
        if 'pedigree' in msg_lower and 'pars' in msg_lower:
            if 'success' in msg_lower or 'completed' in msg_lower:
                self.stats['parsing_stats']['pedigrees']['parsed'] += 1
            elif 'failed' in msg_lower or 'error' in msg_lower:
                self.stats['parsing_stats']['pedigrees']['failed'] += 1

        # æ¬ æãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰
        missing_match = re.search(r'missing\s+field[s]?:\s*(\w+)', msg_lower)
        if missing_match:
            field = missing_match.group(1)
            self.stats['parsing_stats']['missing_fields'][field] += 1

        # ãƒ‡ãƒ¼ã‚¿å“è³ª
        quality_match = re.search(r'(distance|surface|weather|venue|class).*not found', msg_lower)
        if quality_match:
            field = quality_match.group(1)
            self.stats['parsing_stats']['data_quality'][field].append(entry)

    def _analyze_performance(self, entry: Dict):
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æ"""
        msg = entry['message']

        # å‡¦ç†å®Œäº†æ™‚é–“
        time_match = re.search(r'completed in\s+([\d.]+)\s+seconds?', msg, re.IGNORECASE)
        if time_match:
            seconds = float(time_match.group(1))

            if 'scraping' in msg.lower():
                stage = 'scraping'
            elif 'parsing' in msg.lower():
                stage = 'parsing'
            elif 'pipeline' in msg.lower():
                stage = 'total_pipeline'
            else:
                stage = 'other'

            self.stats['performance']['execution_stages'][stage] = seconds

        # ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆï¼ˆæ™‚é–“ã‚ãŸã‚Šã®å‡¦ç†æ•°ï¼‰
        throughput_match = re.search(r'(\d+)\s+items?\s+in\s+([\d.]+)\s+seconds?', msg, re.IGNORECASE)
        if throughput_match:
            items = int(throughput_match.group(1))
            seconds = float(throughput_match.group(2))
            rate = items / seconds if seconds > 0 else 0

            if entry['timestamp']:
                if 'scraping' in msg.lower():
                    self.stats['performance']['scraping_rate'].append((entry['timestamp'], rate))
                elif 'parsing' in msg.lower():
                    self.stats['performance']['parsing_rate'].append((entry['timestamp'], rate))

    def generate_detailed_report(self) -> str:
        """è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        report = []
        report.append("=" * 100)
        report.append("ğŸ“Š KeibaAI_v2 è©³ç´°ãƒ­ã‚°åˆ†æãƒ¬ãƒãƒ¼ãƒˆ")
        report.append("=" * 100)
        report.append(f"ğŸ“ ãƒ­ã‚°ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {self.log_dir}")
        report.append(f"ğŸ“… åˆ†ææ—¥æ™‚: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report.append(f"ğŸ“ ç·ãƒ­ã‚°ã‚¨ãƒ³ãƒˆãƒªæ•°: {self.stats['total_entries']:,}")
        report.append("")

        # ãƒ­ã‚°ãƒ¬ãƒ™ãƒ«åˆ¥çµ±è¨ˆ
        report.append("â–  ãƒ­ã‚°ãƒ¬ãƒ™ãƒ«åˆ¥çµ±è¨ˆ")
        report.append("â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”")
        report.append("â”‚ ãƒ¬ãƒ™ãƒ«      â”‚ ä»¶æ•°     â”‚ å‰²åˆ    â”‚")
        report.append("â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤")
        for level in ['DEBUG', 'INFO', 'WARNING', 'ERROR', 'CRITICAL']:
            count = self.stats['by_level'].get(level, 0)
            if self.stats['total_entries'] > 0:
                percentage = (count / self.stats['total_entries'] * 100)
                report.append(f"â”‚ {level:11s} â”‚ {count:8,} â”‚ {percentage:6.2f}% â”‚")
        report.append("â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜")
        report.append("")

        # ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°çµ±è¨ˆ
        report.append("â–  ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°è©³ç´°çµ±è¨ˆ")
        report.append("â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”")
        report.append("â”‚ ãƒ‡ãƒ¼ã‚¿ç¨®åˆ¥   â”‚ æˆåŠŸ     â”‚ å¤±æ•—     â”‚ ã‚¹ã‚­ãƒƒãƒ— â”‚ åˆè¨ˆ     â”‚")
        report.append("â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤")

        for data_type in ['races', 'shutuba', 'horses', 'pedigrees']:
            stats = self.stats['scraping_stats'][data_type]
            total = stats['scraped'] + stats['failed'] + stats['skipped']
            report.append(
                f"â”‚ {data_type:12s} â”‚ {stats['scraped']:8,} â”‚ "
                f"{stats['failed']:8,} â”‚ {stats['skipped']:8,} â”‚ {total:8,} â”‚"
            )

        report.append("â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜")

        # HTTPã‚¨ãƒ©ãƒ¼
        if self.stats['scraping_stats']['http_errors']:
            report.append("")
            report.append("  HTTPã‚¨ãƒ©ãƒ¼çµ±è¨ˆ:")
            for status, count in sorted(self.stats['scraping_stats']['http_errors'].items()):
                report.append(f"    â€¢ HTTP {status}: {count:,} ä»¶")

        report.append(f"\n  ãƒªãƒˆãƒ©ã‚¤å›æ•°: {self.stats['scraping_stats']['retry_count']:,} å›")
        report.append("")

        # ãƒ‘ãƒ¼ã‚¹çµ±è¨ˆ
        report.append("â–  ãƒ‘ãƒ¼ã‚¹è©³ç´°çµ±è¨ˆ")
        report.append("â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”")
        report.append("â”‚ ãƒ‡ãƒ¼ã‚¿ç¨®åˆ¥   â”‚ æˆåŠŸ     â”‚ å¤±æ•—     â”‚ æˆåŠŸç‡   â”‚ åˆè¨ˆ     â”‚")
        report.append("â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤")

        for data_type in ['races', 'shutuba', 'horses', 'pedigrees']:
            stats = self.stats['parsing_stats'][data_type]
            total = stats['parsed'] + stats['failed']
            success_rate = (stats['parsed'] / total * 100) if total > 0 else 0
            report.append(
                f"â”‚ {data_type:12s} â”‚ {stats['parsed']:8,} â”‚ "
                f"{stats['failed']:8,} â”‚ {success_rate:7.2f}% â”‚ {total:8,} â”‚"
            )

        report.append("â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜")

        # æ¬ æãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰
        if self.stats['parsing_stats']['missing_fields']:
            report.append("")
            report.append("  æ¬ æãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ (ä¸Šä½15ä»¶):")
            for field, count in self.stats['parsing_stats']['missing_fields'].most_common(15):
                report.append(f"    â€¢ {field:30s}: {count:6,} ä»¶")

        report.append("")

        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹çµ±è¨ˆ
        if self.stats['performance']['execution_stages']:
            report.append("â–  ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹çµ±è¨ˆ")
            for stage, seconds in sorted(self.stats['performance']['execution_stages'].items()):
                minutes = seconds / 60
                hours = minutes / 60

                if hours >= 1:
                    time_str = f"{hours:6.2f} æ™‚é–“ ({seconds:10,.1f} ç§’)"
                elif minutes >= 1:
                    time_str = f"{minutes:6.2f} åˆ†   ({seconds:10,.1f} ç§’)"
                else:
                    time_str = f"{seconds:10,.1f} ç§’"

                report.append(f"  â€¢ {stage:20s}: {time_str}")

            report.append("")

        # ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ
        if self.stats['performance']['scraping_rate']:
            avg_scraping_rate = sum(r for _, r in self.stats['performance']['scraping_rate']) / len(
                self.stats['performance']['scraping_rate'])
            report.append(f"  å¹³å‡ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é€Ÿåº¦: {avg_scraping_rate:.2f} items/ç§’")

        if self.stats['performance']['parsing_rate']:
            avg_parsing_rate = sum(r for _, r in self.stats['performance']['parsing_rate']) / len(
                self.stats['performance']['parsing_rate'])
            report.append(f"  å¹³å‡ãƒ‘ãƒ¼ã‚¹é€Ÿåº¦: {avg_parsing_rate:.2f} items/ç§’")

        report.append("")

        # ã‚¨ãƒ©ãƒ¼è©³ç´°
        if self.stats['errors']:
            report.append("â–  ã‚¨ãƒ©ãƒ¼è©³ç´° (æœ€æ–°20ä»¶)")
            report.append("â”€" * 100)
            for i, error in enumerate(self.stats['errors'][-20:], 1):
                timestamp = error['timestamp'].strftime('%H:%M:%S') if error['timestamp'] else 'N/A'
                report.append(f"  {i:2d}. [{timestamp}] {error['message'][:90]}")
            report.append("")

        # è­¦å‘Šè©³ç´°
        if self.stats['warnings']:
            report.append("â–  è­¦å‘Šè©³ç´° (æœ€æ–°20ä»¶)")
            report.append("â”€" * 100)
            for i, warning in enumerate(self.stats['warnings'][-20:], 1):
                timestamp = warning['timestamp'].strftime('%H:%M:%S') if warning['timestamp'] else 'N/A'
                report.append(f"  {i:2d}. [{timestamp}] {warning['message'][:90]}")
            report.append("")

        # ãƒ‡ãƒ¼ã‚¿å“è³ªå•é¡Œ
        if self.stats['parsing_stats']['data_quality']:
            report.append("â–  ãƒ‡ãƒ¼ã‚¿å“è³ªå•é¡Œ")
            for field, entries in self.stats['parsing_stats']['data_quality'].items():
                report.append(f"  â€¢ {field}: {len(entries):,} ä»¶ã®å•é¡Œ")
            report.append("")

        report.append("=" * 100)
        return "\n".join(report)

    def save_json_report(self, output_path: Path):
        """JSONå½¢å¼ã§ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜"""
        report = {
            'metadata': {
                'log_directory': str(self.log_dir),
                'analysis_datetime': datetime.now().isoformat(),
                'total_entries': self.stats['total_entries'],
            },
            'log_levels': dict(self.stats['by_level']),
            'scraping_stats': {
                k: v if not isinstance(v, Counter) else dict(v)
                for k, v in self.stats['scraping_stats'].items()
            },
            'parsing_stats': {
                k: v if not isinstance(v, (Counter, defaultdict)) else dict(v)
                for k, v in self.stats['parsing_stats'].items()
                if k != 'data_quality'  # é™¤å¤–: Entryã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’å«ã‚€
            },
            'performance': {
                k: v for k, v in self.stats['performance'].items()
                if k not in ['scraping_rate', 'parsing_rate']  # é™¤å¤–: datetimeã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’å«ã‚€
            },
            'error_count': len(self.stats['errors']),
            'warning_count': len(self.stats['warnings']),
        }

        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)

        print(f"ğŸ’¾ JSON ãƒ¬ãƒãƒ¼ãƒˆã‚’ä¿å­˜: {output_path}")

    def analyze(self):
        """åˆ†æå®Ÿè¡Œ"""
        log_files = self.find_log_files()

        if not log_files:
            print("âš ï¸  ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
            return

        print(f"ğŸ“Š {len(log_files)} å€‹ã®ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’åˆ†æã—ã¾ã™\n")

        for log_file in log_files:
            self.analyze_log_file(log_file)

        print("\nâœ… åˆ†æå®Œäº†!\n")


def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    import sys
    from datetime import date

    today = date.today()
    default_log_dir = Path(f"keibaai/data/logs/{today.year}/{today.month:02d}/{today.day:02d}")

    if len(sys.argv) > 1:
        log_dir = Path(sys.argv[1])
    else:
        log_dir = default_log_dir

    print("ğŸ” KeibaAI_v2 æ‹¡å¼µãƒ­ã‚°ã‚¢ãƒŠãƒ©ã‚¤ã‚¶ãƒ¼")
    print(f"ğŸ“ ãƒ­ã‚°ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {log_dir}\n")

    analyzer = AdvancedKeibaLogAnalyzer(log_dir)
    analyzer.analyze()

    # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
    report = analyzer.generate_detailed_report()
    print(report)

    # ä¿å­˜
    output_dir = Path("keibaai/data/logs/analysis")
    output_dir.mkdir(parents=True, exist_ok=True)

    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')

    # ãƒ†ã‚­ã‚¹ãƒˆãƒ¬ãƒãƒ¼ãƒˆ
    txt_output = output_dir / f"advanced_log_analysis_{timestamp}.txt"
    with open(txt_output, 'w', encoding='utf-8') as f:
        f.write(report)
    print(f"ğŸ’¾ è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜: {txt_output}")

    # JSONãƒ¬ãƒãƒ¼ãƒˆ
    json_output = output_dir / f"advanced_log_analysis_{timestamp}.json"
    analyzer.save_json_report(json_output)

    print("\n" + "=" * 100)
    print("ğŸ“ˆ åˆ†æå®Œäº†!")
    print("=" * 100)


if __name__ == "__main__":
    main()
