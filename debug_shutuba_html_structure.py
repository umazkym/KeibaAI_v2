"""
å‡ºé¦¬è¡¨HTMLã®æ§‹é€ ã‚’è©³ç´°ã«èª¿æŸ»ã™ã‚‹ãƒ‡ãƒãƒƒã‚°ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
morning_odds/morning_popularityãŒå–å¾—ã§ããªã„åŸå› ã‚’ç‰¹å®šã™ã‚‹
"""

import sys
from pathlib import Path
from bs4 import BeautifulSoup

def analyze_shutuba_html(file_path):
    """
    å‡ºé¦¬è¡¨HTMLã®æ§‹é€ ã‚’è©³ç´°ã«åˆ†æ
    """
    print("=" * 80)
    print(f"ğŸ” HTMLãƒ•ã‚¡ã‚¤ãƒ«æ§‹é€ åˆ†æ: {Path(file_path).name}")
    print("=" * 80)
    print()

    # HTMLãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
    with open(file_path, 'rb') as f:
        html_bytes = f.read()

    try:
        html_text = html_bytes.decode('euc_jp', errors='replace')
    except:
        html_text = html_bytes.decode('utf-8', errors='replace')

    soup = BeautifulSoup(html_text, 'html.parser')

    # --- å‡ºé¦¬è¡¨ãƒ†ãƒ¼ãƒ–ãƒ«ã®å­˜åœ¨ç¢ºèª ---
    shutuba_table = soup.find('table', class_='Shutuba_Table')

    if not shutuba_table:
        print("âŒ Shutuba_Table ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return

    print("âœ… Shutuba_Table ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸ")
    print()

    # --- HorseListã®è¡Œã‚’å–å¾— ---
    horse_rows = shutuba_table.find_all('tr', class_='HorseList')
    print(f"ğŸ“Š HorseListè¡Œæ•°: {len(horse_rows)}")
    print()

    if not horse_rows:
        print("âŒ HorseListè¡ŒãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return

    # --- æœ€åˆã®1è¡Œã‚’è©³ç´°åˆ†æ ---
    first_row = horse_rows[0]
    print("ğŸ”¬ æœ€åˆã®1é ­ã®HTMLæ§‹é€ ã‚’åˆ†æ:")
    print("-" * 80)

    # ã™ã¹ã¦ã®<td>ã‚’å–å¾—
    cells = first_row.find_all('td')
    print(f"ç·ã‚»ãƒ«æ•°: {len(cells)}")
    print()

    # å„ã‚»ãƒ«ã®å†…å®¹ã¨classå±æ€§ã‚’è¡¨ç¤º
    for i, cell in enumerate(cells):
        cell_classes = cell.get('class', [])
        cell_text = cell.get_text(strip=True)[:50]  # æœ€åˆã®50æ–‡å­—

        print(f"[ã‚»ãƒ« {i}]")
        print(f"  classå±æ€§: {cell_classes}")
        print(f"  ãƒ†ã‚­ã‚¹ãƒˆ: {cell_text}")

        # spanã‚¿ã‚°ã®æœ‰ç„¡ã‚‚ç¢ºèª
        spans = cell.find_all('span')
        if spans:
            print(f"  <span>ã®æ•°: {len(spans)}")
            for j, span in enumerate(spans):
                span_id = span.get('id', '')
                span_text = span.get_text(strip=True)
                print(f"    span[{j}] id='{span_id}' text='{span_text}'")

        print()

    print("-" * 80)
    print()

    # --- ã‚ªãƒƒã‚ºãƒ»äººæ°—ã«é–¢é€£ã—ãã†ãªã‚»ãƒ«ã‚’æ¤œç´¢ ---
    print("ğŸ” ã‚ªãƒƒã‚ºãƒ»äººæ°—ã«é–¢é€£ã—ãã†ãªã‚»ãƒ«ã‚’æ¤œç´¢:")
    print()

    # "Popular"ã‚’å«ã‚€class
    popular_cells = first_row.find_all('td', class_=lambda x: x and 'Popular' in ' '.join(x) if x else False)
    print(f"classä¸­ã«'Popular'ã‚’å«ã‚€ã‚»ãƒ«: {len(popular_cells)}å€‹")
    for i, cell in enumerate(popular_cells):
        print(f"  [{i}] class={cell.get('class')} text='{cell.get_text(strip=True)}'")
    print()

    # "Ninki"ã‚’å«ã‚€class
    ninki_cells = first_row.find_all('td', class_=lambda x: x and 'Ninki' in ' '.join(x) if x else False)
    print(f"classä¸­ã«'Ninki'ã‚’å«ã‚€ã‚»ãƒ«: {len(ninki_cells)}å€‹")
    for i, cell in enumerate(ninki_cells):
        print(f"  [{i}] class={cell.get('class')} text='{cell.get_text(strip=True)}'")
    print()

    # "Txt_R"ã‚’å«ã‚€class
    txt_r_cells = first_row.find_all('td', class_=lambda x: x and 'Txt_R' in ' '.join(x) if x else False)
    print(f"classä¸­ã«'Txt_R'ã‚’å«ã‚€ã‚»ãƒ«: {len(txt_r_cells)}å€‹")
    for i, cell in enumerate(txt_r_cells):
        print(f"  [{i}] class={cell.get('class')} text='{cell.get_text(strip=True)}'")
    print()

    # idå±æ€§ã«"odds"ã‚’å«ã‚€span
    odds_spans = first_row.find_all('span', id=lambda x: x and 'odds' in x if x else False)
    print(f"idä¸­ã«'odds'ã‚’å«ã‚€<span>: {len(odds_spans)}å€‹")
    for i, span in enumerate(odds_spans):
        print(f"  [{i}] id='{span.get('id')}' text='{span.get_text(strip=True)}'")
    print()

    # idå±æ€§ã«"ninki"ã‚’å«ã‚€span
    ninki_spans = first_row.find_all('span', id=lambda x: x and 'ninki' in x if x else False)
    print(f"idä¸­ã«'ninki'ã‚’å«ã‚€<span>: {len(ninki_spans)}å€‹")
    for i, span in enumerate(ninki_spans):
        print(f"  [{i}] id='{span.get('id')}' text='{span.get_text(strip=True)}'")
    print()

    print("=" * 80)
    print("ğŸ’¡ çµæœ:")
    print("=" * 80)

    if not popular_cells and not ninki_cells and not odds_spans and not ninki_spans:
        print("âŒ ã‚ªãƒƒã‚ºãƒ»äººæ°—ã«é–¢é€£ã™ã‚‹è¦ç´ ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
        print()
        print("ğŸ’­ è€ƒãˆã‚‰ã‚Œã‚‹åŸå› :")
        print("   1. 2020å¹´ã®HTMLã«ã¯ã‚ªãƒƒã‚ºæƒ…å ±ãŒå«ã¾ã‚Œã¦ã„ãªã„")
        print("   2. HTMLã®æ§‹é€ ãŒ2025å¹´ç‰ˆã¨å¤§ããç•°ãªã‚‹")
        print("   3. å‡ºé¦¬è¡¨ãƒšãƒ¼ã‚¸ã§ã¯ãªãã€çµæœãƒšãƒ¼ã‚¸ã®å¯èƒ½æ€§")
        print()
        print("ğŸ“ æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³:")
        print("   â†’ ã‚ˆã‚Šæ–°ã—ã„å¹´ä»£ï¼ˆ2024-2025å¹´ï¼‰ã®HTMLãƒ•ã‚¡ã‚¤ãƒ«ã§ãƒ†ã‚¹ãƒˆã™ã‚‹")
        print("   â†’ ã¾ãŸã¯ã€ã“ã®HTMLãƒ•ã‚¡ã‚¤ãƒ«ã®å…¨ä½“æ§‹é€ ã‚’ç¢ºèªã™ã‚‹")
    else:
        print("âœ… ã‚ªãƒƒã‚ºãƒ»äººæ°—ã«é–¢é€£ã™ã‚‹è¦ç´ ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸ")
        print("   â†’ ä¸Šè¨˜ã®è©³ç´°æƒ…å ±ã‚’å…ƒã«ãƒ‘ãƒ¼ã‚µãƒ¼ã‚’èª¿æ•´ã—ã¦ãã ã•ã„")

    print("=" * 80)

def main():
    print()
    print("ğŸ”§ å‡ºé¦¬è¡¨HTMLæ§‹é€ ãƒ‡ãƒãƒƒã‚°ãƒ„ãƒ¼ãƒ«")
    print()

    html_dir = Path(r'keibaai\data\raw\html\shutuba')

    if not html_dir.exists():
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: {html_dir} ãŒå­˜åœ¨ã—ã¾ã›ã‚“")
        return

    # 2020å¹´ã®ãƒ•ã‚¡ã‚¤ãƒ«1ä»¶
    bin_files_2020 = sorted([f for f in html_dir.glob('2020*.bin')])[:1]

    # 2024-2025å¹´ã®ãƒ•ã‚¡ã‚¤ãƒ«1ä»¶
    bin_files_recent = sorted([f for f in html_dir.glob('202[45]*.bin')])[:1]

    files_to_analyze = bin_files_2020 + bin_files_recent

    if not files_to_analyze:
        print("âŒ åˆ†æå¯¾è±¡ã®ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return

    for file_path in files_to_analyze:
        analyze_shutuba_html(file_path)
        print()
        print()

if __name__ == '__main__':
    main()
