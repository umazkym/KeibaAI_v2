"""
Î¼ãƒ¢ãƒ‡ãƒ«ã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–ã‚¹ã‚¯ãƒªãƒ—ãƒˆ (Optunaä½¿ç”¨)

ã€é‡è¦ã€‘ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯é˜²æ­¢ã®ãŸã‚ã®åŸå‰‡:
1. æ™‚ç³»åˆ—åˆ†å‰²: éå»ãƒ‡ãƒ¼ã‚¿ã§å­¦ç¿’ã€æœªæ¥ãƒ‡ãƒ¼ã‚¿ã§æ¤œè¨¼
2. ãƒ¬ãƒ¼ã‚¹å¾Œæƒ…å ±ã®é™¤å¤–: finish_position, win_oddsç­‰ã¯çµ¶å¯¾ã«ä½¿ã‚ãªã„
3. åŒä¸€ãƒ¬ãƒ¼ã‚¹å†…ã®æƒ…å ±: horse_numberã¯ä½¿ãˆã‚‹ï¼ˆãƒ¬ãƒ¼ã‚¹å‰ã«ç¢ºå®šï¼‰
4. 2024å¹´ãƒ‡ãƒ¼ã‚¿ã¯ä¸€åˆ‡ä½¿ã‚ãªã„ï¼ˆæœ€çµ‚æ¤œè¨¼ã®ã¿ï¼‰

å®Ÿè¡Œæ–¹æ³•:
    python optimize_mu_hyperparameters.py --n_trials 50 --timeout 10800
"""

import optuna
from optuna.pruners import MedianPruner
from optuna.samplers import TPESampler
import lightgbm as lgb
import pandas as pd
import numpy as np
from pathlib import Path
from datetime import datetime
import json
import logging
import argparse
from sklearn.model_selection import TimeSeriesSplit
from sklearn.metrics import mean_squared_error, ndcg_score
import warnings
warnings.filterwarnings('ignore')

# ãƒ­ã‚®ãƒ³ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('optuna_optimization.log', encoding='utf-8')
    ]
)

class DataLeakValidator:
    """ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯æ¤œè¨¼ã‚¯ãƒ©ã‚¹"""
    
    FORBIDDEN_COLUMNS = [
        'finish_position', 'finish_time_seconds', 'finish_time_str',
        'win_odds', 'popularity', 'prize_money',
        'margin_seconds', 'margin_str', 'passing_order',
        'last_3f_time', 'time_except_last3f'
    ]
    
    @classmethod
    def validate_features(cls, df, feature_cols):
        """ç‰¹å¾´é‡ã«ãƒ¬ãƒ¼ã‚¹å¾Œæƒ…å ±ãŒå«ã¾ã‚Œã¦ã„ãªã„ã‹æ¤œè¨¼"""
        leaked = [col for col in feature_cols if col in cls.FORBIDDEN_COLUMNS]
        if leaked:
            raise ValueError(f"âŒ ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯æ¤œå‡º: {leaked}")
        logging.info("âœ… ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯æ¤œè¨¼: åˆæ ¼")
        return True

def load_training_data():
    """å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã®ãƒ­ãƒ¼ãƒ‰ï¼ˆ2020-2023å¹´ã®ã¿ã€2024å¹´ã¯é™¤å¤–ï¼‰"""
    logging.info("å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­...")
    
    # ç‰¹å¾´é‡ãƒ‡ãƒ¼ã‚¿
    features_dir = Path('keibaai/data/features/parquet')
    year_partitions = sorted(features_dir.glob('year=*'))
    
    dfs = []
    for year_partition in year_partitions:
        year = int(year_partition.name.split('=')[1])
        # 2024å¹´ã¯æœ€çµ‚æ¤œè¨¼ç”¨ãªã®ã§é™¤å¤–
        if year >= 2024:
            logging.info(f"  å¹´={year}: ã‚¹ã‚­ãƒƒãƒ—ï¼ˆæœ€çµ‚æ¤œè¨¼ç”¨ï¼‰")
            continue
        
        df_year = pd.read_parquet(year_partition)
        dfs.append(df_year)
        logging.info(f"  å¹´={year}: {len(df_year)}è¡Œ")
    
    features_df = pd.concat(dfs, ignore_index=True)
    logging.info(f"ç‰¹å¾´é‡ãƒ‡ãƒ¼ã‚¿: {len(features_df)}è¡Œ")
    
    # race_dateã§æ™‚ç³»åˆ—ã‚½ãƒ¼ãƒˆï¼ˆé‡è¦ï¼ï¼‰
    if 'race_date' in features_df.columns:
        features_df['race_date'] = pd.to_datetime(features_df['race_date'])
        features_df = features_df.sort_values('race_date').reset_index(drop=True)
        logging.info("âœ… æ™‚ç³»åˆ—ã§ã‚½ãƒ¼ãƒˆå®Œäº†")
    
    # ãƒ¬ãƒ¼ã‚¹çµæœãƒ‡ãƒ¼ã‚¿
    races_df = pd.read_parquet('keibaai/data/parsed/parquet/races/races.parquet')
    races_df['race_date'] = pd.to_datetime(races_df['race_date'])
    
    # 2020-2023å¹´ã®ã¿
    races_df = races_df[
        (races_df['race_date'].dt.year >= 2020) & 
        (races_df['race_date'].dt.year < 2024)
    ]
    
    # ãƒãƒ¼ã‚¸
    merge_keys = ['race_id', 'horse_id']
    target_cols = ['finish_position', 'finish_time_seconds']
    
    # ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯é˜²æ­¢: ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå¤‰æ•°ã‚’é™¤å¤–
    exclude_cols = DataLeakValidator.FORBIDDEN_COLUMNS
    cols_to_drop = [c for c in exclude_cols if c in features_df.columns]
    if cols_to_drop:
        logging.warning(f"ç‰¹å¾´é‡ã‹ã‚‰ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå¤‰æ•°ã‚’é™¤å¤–: {cols_to_drop}")
        features_df = features_df.drop(columns=cols_to_drop)
    
    merged = features_df.merge(
        races_df[merge_keys + target_cols],
        on=merge_keys,
        how='inner'
    )
    
    # æ¬ æå€¤é™¤å»
    merged = merged.dropna(subset=target_cols + ['race_id'])
    
    logging.info(f"ãƒãƒ¼ã‚¸å¾Œ: {len(merged)}è¡Œ, {merged['race_id'].nunique()}ãƒ¬ãƒ¼ã‚¹")
    return merged

def get_feature_columns(df):
    """ç‰¹å¾´é‡ã‚«ãƒ©ãƒ ã‚’å–å¾—ï¼ˆãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã‚’é™¤ãï¼‰"""
    exclude = [
        'race_id', 'horse_id', 'horse_name', 'jockey_name', 'trainer_name',
        'race_date', 'year', 'month', 'day',
        'finish_position', 'finish_time_seconds'
    ] + DataLeakValidator.FORBIDDEN_COLUMNS
    
    feature_cols = [col for col in df.columns if col not in exclude]
    
    # å‹ãƒã‚§ãƒƒã‚¯ã®å‰ã«ç¾çŠ¶ã‚’ãƒ­ã‚°å‡ºåŠ›
    logging.info(f"é™¤å¤–å‰ã‚«ãƒ©ãƒ æ•°: {len(df.columns)}")
    logging.info(f"é™¤å¤–å¾Œå€™è£œæ•°: {len(feature_cols)}")
    
    # æ•°å€¤å‹ã®ã¿
    numeric_cols = [col for col in feature_cols if pd.api.types.is_numeric_dtype(df[col])]
    if len(numeric_cols) < len(feature_cols):
        non_numeric = set(feature_cols) - set(numeric_cols)
        logging.warning(f"éæ•°å€¤ã‚«ãƒ©ãƒ ãŒé™¤å¤–ã•ã‚Œã¾ã—ãŸ: {len(non_numeric)}å€‹")
        logging.debug(f"é™¤å¤–ã•ã‚ŒãŸã‚«ãƒ©ãƒ : {non_numeric}")
    
    feature_cols = numeric_cols
    
    # ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯æ¤œè¨¼
    DataLeakValidator.validate_features(df, feature_cols)
    
    logging.info(f"æœ€çµ‚ç‰¹å¾´é‡æ•°: {len(feature_cols)}")
    if len(feature_cols) == 0:
        logging.error("ç‰¹å¾´é‡ãŒ0å€‹ã§ã™ï¼ãƒ‡ãƒ¼ã‚¿å‹ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        logging.info("å…ˆé ­5è¡Œã®ãƒ‡ãƒ¼ã‚¿å‹:")
        logging.info(df.dtypes.head(20))
        raise ValueError("ç‰¹å¾´é‡ãŒã‚ã‚Šã¾ã›ã‚“")
        
    return feature_cols

def time_series_cv_split(df, n_splits=5):
    """
    æ™‚ç³»åˆ—ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³åˆ†å‰²
    """
    # race_dateã§ã‚½ãƒ¼ãƒˆæ¸ˆã¿ã‚’å‰æ
    race_dates = df['race_date'].unique()
    race_dates = sorted(race_dates)
    
    splits = []
    total_races = len(race_dates)
    
    if total_races < n_splits + 2:
        logging.warning(f"ãƒ¬ãƒ¼ã‚¹é–‹å‚¬æ—¥æ•°ãŒå°‘ãªã™ãã¾ã™: {total_races}æ—¥")
    
    for i in range(n_splits):
        # è¨“ç·´æœŸé–“: æœ€åˆã‹ã‚‰ i/n_splits ã®ä½ç½®ã¾ã§
        # æ¤œè¨¼æœŸé–“: i/n_splits ã‹ã‚‰ (i+1)/n_splits ã¾ã§
        # åˆ†å‰²ãƒ­ã‚¸ãƒƒã‚¯ã‚’ä¿®æ­£ã—ã¦ç¢ºå®Ÿã«ãƒ‡ãƒ¼ã‚¿ãŒå…¥ã‚‹ã‚ˆã†ã«ã™ã‚‹
        train_end_idx = int(total_races * (i + 2) / (n_splits + 2))
        val_start_idx = train_end_idx
        val_end_idx = int(total_races * (i + 3) / (n_splits + 2))
        
        if val_end_idx > total_races:
            val_end_idx = total_races
        
        train_dates = race_dates[:train_end_idx]
        val_dates = race_dates[val_start_idx:val_end_idx]
        
        train_idx = df[df['race_date'].isin(train_dates)].index
        val_idx = df[df['race_date'].isin(val_dates)].index
        
        logging.info(f"  Fold {i+1}: Train={len(train_idx)}è¡Œ ({len(train_dates)}æ—¥), Val={len(val_idx)}è¡Œ ({len(val_dates)}æ—¥)")
        
        if len(train_idx) == 0 or len(val_idx) == 0:
            logging.warning(f"  Fold {i+1} ã®ãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™ï¼ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
            continue
            
        splits.append((train_idx, val_idx))
    
    return splits

def objective(trial, df, feature_cols):
    """Optunaç›®çš„é–¢æ•°"""
    
    # ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ææ¡ˆ
    params = {
        'objective': 'regression',
        'metric': 'rmse',
        'verbosity': -1,
        'boosting_type': 'gbdt',
        'num_leaves': trial.suggest_int('num_leaves', 20, 100),
        'max_depth': trial.suggest_int('max_depth', 3, 12),
        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1, log=True),
        'n_estimators': trial.suggest_int('n_estimators', 100, 1000, step=100),
        'min_child_samples': trial.suggest_int('min_child_samples', 10, 100),
        'subsample': trial.suggest_float('subsample', 0.6, 1.0),
        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),
        'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 1.0, log=True),
        'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 1.0, log=True),
        'random_state': 42,
        'n_jobs': 1
    }
    
    # Time Series CV
    cv_splits = time_series_cv_split(df, n_splits=3)  # 3-foldã§é«˜é€ŸåŒ–
    scores = []
    
    if not cv_splits:
        raise ValueError("æœ‰åŠ¹ãªCVåˆ†å‰²ãŒä½œæˆã§ãã¾ã›ã‚“ã§ã—ãŸ")
    
    for fold_idx, (train_idx, val_idx) in enumerate(cv_splits):
        train_df = df.loc[train_idx]
        val_df = df.loc[val_idx]
        
        X_train = train_df[feature_cols]
        y_train = train_df['finish_time_seconds']
        X_val = val_df[feature_cols]
        y_val = val_df['finish_time_seconds']
        
        # ãƒ‡ãƒ¼ã‚¿å½¢çŠ¶ãƒã‚§ãƒƒã‚¯
        if X_train.empty or X_val.empty:
            logging.warning(f"Fold {fold_idx}: ãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™ã€‚X_train={X_train.shape}, X_val={X_val.shape}")
            continue
            
        # LightGBMå­¦ç¿’
        model = lgb.LGBMRegressor(**params)
        model.fit(
            X_train, y_train,
            eval_set=[(X_val, y_val)],
            eval_metric='rmse',
            callbacks=[
                lgb.early_stopping(stopping_rounds=50, verbose=False),
                lgb.log_evaluation(0) # ãƒ­ã‚°å‡ºåŠ›ã‚’æŠ‘åˆ¶
            ]
        )
        
        # äºˆæ¸¬
        y_pred = model.predict(X_val)
        rmse = np.sqrt(mean_squared_error(y_val, y_pred))
        scores.append(rmse)
        
        # Pruning (æ—©æœŸçµ‚äº†)
        trial.report(rmse, fold_idx)
        if trial.should_prune():
            raise optuna.TrialPruned()
    
    if not scores:
        return float('inf')
        
    mean_score = np.mean(scores)
    return mean_score

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('--n_trials', type=int, default=50, help='æœ€é©åŒ–è©¦è¡Œå›æ•°')
    parser.add_argument('--timeout', type=int, default=10800, help='ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ(ç§’)')
    args = parser.parse_args()
    
    logging.info("="*60)
    logging.info("Î¼ãƒ¢ãƒ‡ãƒ« ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–")
    logging.info("="*60)
    
    # ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‰
    df = load_training_data()
    feature_cols = get_feature_columns(df)
    
    # Optunaæœ€é©åŒ–
    study = optuna.create_study(
        direction='minimize',
        sampler=TPESampler(seed=42),
        pruner=MedianPruner(n_startup_trials=5, n_warmup_steps=5)
    )
    
    logging.info(f"\næœ€é©åŒ–é–‹å§‹: {args.n_trials}è©¦è¡Œ, ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ={args.timeout}ç§’")
    
    study.optimize(
        lambda trial: objective(trial, df, feature_cols),
        n_trials=args.n_trials,
        timeout=args.timeout,
        n_jobs=1  # LightGBMãŒä¸¦åˆ—åŒ–ã™ã‚‹ã®ã§1ã§ååˆ†
    )
    
    # çµæœ
    logging.info("\n" + "="*60)
    logging.info("æœ€é©åŒ–å®Œäº†")
    logging.info("="*60)
    logging.info(f"æœ€è‰¯ã‚¹ã‚³ã‚¢ (RMSE): {study.best_value:.6f}")
    logging.info(f"æœ€è‰¯ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿:")
    for key, value in study.best_params.items():
        logging.info(f"  {key}: {value}")
    
    # ä¿å­˜
    output_path = Path('keibaai/configs/optimized_params.json')
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    result = {
        'best_score': study.best_value,
        'best_params': study.best_params,
        'n_trials': len(study.trials),
        'optimization_date': datetime.now().isoformat()
    }
    
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(result, f, indent=2, ensure_ascii=False)
    
    logging.info(f"\nâœ… æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ä¿å­˜: {output_path}")
    
    # æœ€é©åŒ–å±¥æ­´ã®å¯è¦–åŒ–ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
    try:
        import optuna.visualization as vis
        fig1 = vis.plot_optimization_history(study)
        fig1.write_html('optuna_history.html')
        
        fig2 = vis.plot_param_importances(study)
        fig2.write_html('optuna_param_importance.html')
        
        logging.info("ğŸ“Š å¯è¦–åŒ–ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜: optuna_history.html, optuna_param_importance.html")
    except:
        logging.info("å¯è¦–åŒ–ã‚¹ã‚­ãƒƒãƒ—ï¼ˆplotlyãŒå¿…è¦ï¼‰")

if __name__ == '__main__':
    main()
