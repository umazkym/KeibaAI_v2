#!/usr/bin/env python3
"""
KeibaAI_v2 Log Analyzer (Fixed Version)
å®Ÿéš›ã®ãƒ­ã‚°ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã¨æ—¥æœ¬èªãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã«å¯¾å¿œã—ãŸãƒãƒ¼ã‚¸ãƒ§ãƒ³
"""

import re
from pathlib import Path
from datetime import datetime
from collections import defaultdict, Counter
from typing import Dict, List, Optional
import json


class KeibaLogAnalyzer:
    """ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’åˆ†æã™ã‚‹ã‚¯ãƒ©ã‚¹"""

    def __init__(self, log_dir: Path):
        self.log_dir = Path(log_dir)
        self.log_entries = []
        self.stats = {
            'total_entries': 0,
            'by_level': Counter(),
            'by_module': Counter(),
            'errors': [],
            'warnings': [],
            'criticals': [],
            'scraping_stats': {
                'races_scraped': 0,
                'shutuba_scraped': 0,
                'horses_scraped': 0,
                'horses_performance_scraped': 0,
                'pedigrees_scraped': 0,
                'failed_scrapes': 0,
                'http_errors': Counter(),
                'ip_ban_warnings': 0,
            },
            'parsing_stats': {
                'races_parsed': 0,
                'shutuba_parsed': 0,
                'horses_parsed': 0,
                'horses_profile_parsed': 0,
                'horses_performance_parsed': 0,
                'pedigrees_parsed': 0,
                'parse_errors': 0,
                'total_records': Counter(),
            },
            'execution_time': {},
            'file_counts': {},
        }

    def find_log_files(self) -> List[Path]:
        """ãƒ­ã‚°ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®ã™ã¹ã¦ã®ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢"""
        if not self.log_dir.exists():
            print(f"âš ï¸  ãƒ­ã‚°ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã—ã¾ã›ã‚“: {self.log_dir}")
            return []

        log_files = sorted(self.log_dir.glob("*.log"))
        return log_files

    def parse_log_line(self, line: str) -> Optional[Dict]:
        """ãƒ­ã‚°è¡Œã‚’ãƒ‘ãƒ¼ã‚¹ã—ã¦æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ã«å¤‰æ›

        å®Ÿéš›ã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ: YYYY-MM-DD HH:MM:SS,mmm - <module> - <level> - <message>
        ä¾‹: 2025-11-18 09:02:52,233 - __main__ - INFO - ãƒ‡ãƒ¼ã‚¿æ•´å½¢ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’é–‹å§‹ã—ã¾ã™...
        """
        # ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«åä»˜ããƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
        pattern = r'(\d{4}-\d{2}-\d{2}\s+\d{2}:\d{2}:\d{2},\d{3})\s+-\s+([^\s]+)\s+-\s+(\w+)\s+-\s+(.+)'
        match = re.match(pattern, line)

        if match:
            timestamp_str, module, level, message = match.groups()
            try:
                timestamp = datetime.strptime(timestamp_str, '%Y-%m-%d %H:%M:%S,%f')
            except:
                timestamp = None

            return {
                'timestamp': timestamp,
                'module': module,
                'level': level,
                'message': message.strip(),
                'raw': line
            }
        return None

    def analyze_log_file(self, log_file: Path):
        """å˜ä¸€ã®ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’åˆ†æ"""
        print(f"ğŸ“„ åˆ†æä¸­: {log_file.name}")

        try:
            with open(log_file, 'r', encoding='utf-8', errors='replace') as f:
                for line in f:
                    line = line.strip()
                    if not line:
                        continue

                    entry = self.parse_log_line(line)
                    if entry:
                        self.log_entries.append(entry)
                        self.stats['total_entries'] += 1
                        self.stats['by_level'][entry['level']] += 1
                        self.stats['by_module'][entry['module']] += 1

                        # ã‚¨ãƒ©ãƒ¼ã€è­¦å‘Šã€ã‚¯ãƒªãƒ†ã‚£ã‚«ãƒ«ã‚’è¨˜éŒ²
                        if entry['level'] == 'ERROR':
                            self.stats['errors'].append(entry)
                        elif entry['level'] == 'WARNING':
                            self.stats['warnings'].append(entry)
                        elif entry['level'] == 'CRITICAL':
                            self.stats['criticals'].append(entry)

                        # ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°çµ±è¨ˆã‚’æŠ½å‡º
                        self._extract_scraping_stats(entry)

                        # ãƒ‘ãƒ¼ã‚¹çµ±è¨ˆã‚’æŠ½å‡º
                        self._extract_parsing_stats(entry)

                        # å®Ÿè¡Œæ™‚é–“ã¨ãƒ•ã‚¡ã‚¤ãƒ«æ•°ã‚’æŠ½å‡º
                        self._extract_metadata(entry)

        except Exception as e:
            print(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {log_file.name} - {e}")

    def _extract_scraping_stats(self, entry: Dict):
        """ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–¢é€£ã®çµ±è¨ˆã‚’æŠ½å‡º"""
        msg = entry['message']

        # HTTPã‚¨ãƒ©ãƒ¼ï¼ˆCRITICALï¼‰
        if 'HTTP 400' in msg or 'HTTP 403' in msg or 'HTTP 404' in msg:
            status_match = re.search(r'HTTP (\d{3})', msg)
            if status_match:
                status_code = status_match.group(1)
                self.stats['scraping_stats']['http_errors'][status_code] += 1
                self.stats['scraping_stats']['failed_scrapes'] += 1

        # IP BANè­¦å‘Š
        if 'IP BAN' in msg or 'IP BANã®å¯èƒ½æ€§' in msg:
            self.stats['scraping_stats']['ip_ban_warnings'] += 1

        # é–‹å‚¬æ—¥å–å¾—
        if 'å€‹ã®é–‹å‚¬æ—¥ã‚’å–å¾—' in msg:
            count_match = re.search(r'(\d+)å€‹ã®é–‹å‚¬æ—¥', msg)
            if count_match:
                self.stats['scraping_stats']['kaisai_days'] = int(count_match.group(1))

    def _extract_parsing_stats(self, entry: Dict):
        """ãƒ‘ãƒ¼ã‚¹é–¢é€£ã®çµ±è¨ˆã‚’æŠ½å‡ºï¼ˆæ—¥æœ¬èªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã«å¯¾å¿œï¼‰"""
        msg = entry['message']

        # ãƒ¬ãƒ¼ã‚¹çµæœãƒ‘ãƒ¼ã‚¹
        if 'ãƒ¬ãƒ¼ã‚¹çµæœãƒ‘ãƒ¼ã‚¹å®Œäº†' in msg:
            self.stats['parsing_stats']['races_parsed'] += 1
            # è¡Œæ•°ã‚’æŠ½å‡º
            rows_match = re.search(r'(\d+)è¡Œ', msg)
            if rows_match:
                rows = int(rows_match.group(1))
                self.stats['parsing_stats']['total_records']['races'] += rows

        # å‡ºé¦¬è¡¨ãƒ‘ãƒ¼ã‚¹
        if 'å‡ºé¦¬è¡¨ãƒ‘ãƒ¼ã‚¹å®Œäº†' in msg:
            self.stats['parsing_stats']['shutuba_parsed'] += 1
            rows_match = re.search(r'(\d+)è¡Œ', msg)
            if rows_match:
                rows = int(rows_match.group(1))
                self.stats['parsing_stats']['total_records']['shutuba'] += rows

        # é¦¬ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‘ãƒ¼ã‚¹
        if 'é¦¬ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‘ãƒ¼ã‚¹å®Œäº†' in msg:
            self.stats['parsing_stats']['horses_profile_parsed'] += 1

        # é¦¬éå»æˆç¸¾ãƒ‘ãƒ¼ã‚¹
        if 'é¦¬éå»æˆç¸¾ãƒ‘ãƒ¼ã‚¹å®Œäº†' in msg:
            self.stats['parsing_stats']['horses_performance_parsed'] += 1
            races_match = re.search(r'(\d+)ãƒ¬ãƒ¼ã‚¹', msg)
            if races_match:
                races = int(races_match.group(1))
                self.stats['parsing_stats']['total_records']['horse_performance'] += races

        # è¡€çµ±ãƒ‘ãƒ¼ã‚¹
        if 'è¡€çµ±ãƒ‘ãƒ¼ã‚¹å®Œäº†' in msg:
            self.stats['parsing_stats']['pedigrees_parsed'] += 1

        # ãƒ‘ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼
        if entry['level'] == 'ERROR' and ('ãƒ‘ãƒ¼ã‚¹' in msg or 'parse' in msg.lower()):
            self.stats['parsing_stats']['parse_errors'] += 1

    def _extract_metadata(self, entry: Dict):
        """ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«æ•°ã€å®Ÿè¡Œæ™‚é–“ãªã©ï¼‰ã‚’æŠ½å‡º"""
        msg = entry['message']

        # ãƒ•ã‚¡ã‚¤ãƒ«æ•°
        file_count_match = re.search(r'(\d{1,3}(?:,\d{3})*)ä»¶ã®.*(HTML|ãƒ•ã‚¡ã‚¤ãƒ«)', msg)
        if file_count_match:
            count_str = file_count_match.group(1).replace(',', '')
            count = int(count_str)
            if 'ãƒ¬ãƒ¼ã‚¹çµæœ' in msg:
                self.stats['file_counts']['race_files'] = count
            elif 'å‡ºé¦¬è¡¨' in msg:
                self.stats['file_counts']['shutuba_files'] = count
            elif 'é¦¬ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«' in msg:
                self.stats['file_counts']['horse_profile_files'] = count
            elif 'é¦¬éå»æˆç¸¾' in msg:
                self.stats['file_counts']['horse_performance_files'] = count

        # ä¿å­˜å®Œäº†ï¼ˆãƒ¬ã‚³ãƒ¼ãƒ‰æ•°ï¼‰
        save_match = re.search(r'ä¿å­˜å®Œäº†.*\((\d{1,3}(?:,\d{3})*)ãƒ¬ã‚³ãƒ¼ãƒ‰\)', msg)
        if save_match:
            count_str = save_match.group(1).replace(',', '')
            count = int(count_str)
            if 'races' in msg or 'ãƒ¬ãƒ¼ã‚¹çµæœ' in msg:
                self.stats['parsing_stats']['total_records']['races_saved'] = count
            elif 'shutuba' in msg or 'å‡ºé¦¬è¡¨' in msg:
                self.stats['parsing_stats']['total_records']['shutuba_saved'] = count
            elif 'horses' in msg or 'é¦¬æƒ…å ±' in msg:
                self.stats['parsing_stats']['total_records']['horses_saved'] = count
            elif 'performance' in msg or 'éå»æˆç¸¾' in msg:
                self.stats['parsing_stats']['total_records']['horse_performance_saved'] = count
            elif 'pedigree' in msg or 'è¡€çµ±' in msg:
                self.stats['parsing_stats']['total_records']['pedigrees_saved'] = count

    def generate_report(self) -> str:
        """åˆ†æçµæœã®ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ"""
        report = []
        report.append("=" * 80)
        report.append("ğŸ“Š KeibaAI_v2 ãƒ­ã‚°åˆ†æãƒ¬ãƒãƒ¼ãƒˆ (ä¿®æ­£ç‰ˆ)")
        report.append("=" * 80)
        report.append(f"ğŸ“ ãƒ­ã‚°ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {self.log_dir}")
        report.append(f"ğŸ“… åˆ†ææ—¥æ™‚: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report.append("")

        # ç·åˆçµ±è¨ˆ
        report.append("â–  ç·åˆçµ±è¨ˆ")
        report.append(f"  â€¢ ç·ãƒ­ã‚°ã‚¨ãƒ³ãƒˆãƒªæ•°: {self.stats['total_entries']:,}")
        report.append("")

        # ãƒ­ã‚°ãƒ¬ãƒ™ãƒ«åˆ¥
        report.append("â–  ãƒ­ã‚°ãƒ¬ãƒ™ãƒ«åˆ¥é›†è¨ˆ")
        for level in ['DEBUG', 'INFO', 'WARNING', 'ERROR', 'CRITICAL']:
            count = self.stats['by_level'].get(level, 0)
            if count > 0:
                percentage = (count / self.stats['total_entries'] * 100) if self.stats['total_entries'] > 0 else 0
                report.append(f"  â€¢ {level:8s}: {count:8,} ({percentage:5.1f}%)")
        report.append("")

        # ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«åˆ¥ï¼ˆä¸Šä½10ä»¶ï¼‰
        if self.stats['by_module']:
            report.append("â–  ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«åˆ¥é›†è¨ˆ (ä¸Šä½10ä»¶)")
            for module, count in self.stats['by_module'].most_common(10):
                percentage = (count / self.stats['total_entries'] * 100) if self.stats['total_entries'] > 0 else 0
                report.append(f"  â€¢ {module:40s}: {count:8,} ({percentage:5.1f}%)")
            report.append("")

        # ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°çµ±è¨ˆ
        report.append("â–  ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°çµ±è¨ˆ")
        scraping = self.stats['scraping_stats']

        if 'kaisai_days' in scraping:
            report.append(f"  â€¢ é–‹å‚¬æ—¥: {scraping['kaisai_days']:,} æ—¥")

        if scraping['failed_scrapes'] > 0:
            report.append(f"  â€¢ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å¤±æ•—: {scraping['failed_scrapes']:,} ä»¶")

        if scraping['ip_ban_warnings'] > 0:
            report.append(f"  â€¢ âš ï¸  IP BANè­¦å‘Š: {scraping['ip_ban_warnings']:,} å›")

        if scraping['http_errors']:
            report.append("  â€¢ HTTPã‚¨ãƒ©ãƒ¼:")
            for status, count in sorted(scraping['http_errors'].items()):
                error_name = {
                    '400': 'Bad Request',
                    '403': 'Forbidden',
                    '404': 'Not Found',
                    '500': 'Internal Server Error',
                    '503': 'Service Unavailable'
                }.get(status, 'Unknown')
                report.append(f"    - HTTP {status} ({error_name}): {count:,} ä»¶")
        report.append("")

        # ãƒ•ã‚¡ã‚¤ãƒ«æ•°çµ±è¨ˆ
        if self.stats['file_counts']:
            report.append("â–  å‡¦ç†ãƒ•ã‚¡ã‚¤ãƒ«æ•°")
            for file_type, count in self.stats['file_counts'].items():
                file_type_jp = {
                    'race_files': 'ãƒ¬ãƒ¼ã‚¹çµæœ',
                    'shutuba_files': 'å‡ºé¦¬è¡¨',
                    'horse_profile_files': 'é¦¬ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«',
                    'horse_performance_files': 'é¦¬éå»æˆç¸¾',
                }.get(file_type, file_type)
                report.append(f"  â€¢ {file_type_jp}: {count:,} ãƒ•ã‚¡ã‚¤ãƒ«")
            report.append("")

        # ãƒ‘ãƒ¼ã‚¹çµ±è¨ˆ
        report.append("â–  ãƒ‘ãƒ¼ã‚¹çµ±è¨ˆ")
        parsing = self.stats['parsing_stats']
        report.append(f"  â€¢ ãƒ¬ãƒ¼ã‚¹çµæœ: {parsing['races_parsed']:,} ä»¶")
        report.append(f"  â€¢ å‡ºé¦¬è¡¨: {parsing['shutuba_parsed']:,} ä»¶")
        report.append(f"  â€¢ é¦¬ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«: {parsing['horses_profile_parsed']:,} ä»¶")
        report.append(f"  â€¢ é¦¬éå»æˆç¸¾: {parsing['horses_performance_parsed']:,} ä»¶")
        report.append(f"  â€¢ è¡€çµ±: {parsing['pedigrees_parsed']:,} ä»¶")

        if parsing['parse_errors'] > 0:
            report.append(f"  â€¢ ãƒ‘ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼: {parsing['parse_errors']:,} ä»¶")

        # ä¿å­˜æ¸ˆã¿ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°
        if parsing['total_records']:
            report.append("")
            report.append("â–  ä¿å­˜æ¸ˆã¿ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°")
            for data_type, count in parsing['total_records'].items():
                data_type_jp = {
                    'races': 'ãƒ¬ãƒ¼ã‚¹çµæœï¼ˆè¡Œï¼‰',
                    'shutuba': 'å‡ºé¦¬è¡¨ï¼ˆè¡Œï¼‰',
                    'horse_performance': 'é¦¬éå»æˆç¸¾ï¼ˆãƒ¬ãƒ¼ã‚¹ï¼‰',
                    'races_saved': 'ãƒ¬ãƒ¼ã‚¹çµæœï¼ˆæœ€çµ‚ä¿å­˜ï¼‰',
                    'shutuba_saved': 'å‡ºé¦¬è¡¨ï¼ˆæœ€çµ‚ä¿å­˜ï¼‰',
                    'horses_saved': 'é¦¬æƒ…å ±ï¼ˆæœ€çµ‚ä¿å­˜ï¼‰',
                    'horse_performance_saved': 'é¦¬éå»æˆç¸¾ï¼ˆæœ€çµ‚ä¿å­˜ï¼‰',
                    'pedigrees_saved': 'è¡€çµ±ï¼ˆæœ€çµ‚ä¿å­˜ï¼‰',
                }.get(data_type, data_type)
                report.append(f"  â€¢ {data_type_jp}: {count:,} ãƒ¬ã‚³ãƒ¼ãƒ‰")
        report.append("")

        # ã‚¯ãƒªãƒ†ã‚£ã‚«ãƒ«ã‚¨ãƒ©ãƒ¼
        if self.stats['criticals']:
            report.append("â–  ã‚¯ãƒªãƒ†ã‚£ã‚«ãƒ«ã‚¨ãƒ©ãƒ¼ (å…¨ä»¶)")
            report.append("â”€" * 80)
            for i, critical in enumerate(self.stats['criticals'], 1):
                timestamp = critical['timestamp'].strftime('%H:%M:%S') if critical['timestamp'] else 'N/A'
                report.append(f"  {i:2d}. [{timestamp}] {critical['message'][:90]}")
                if len(critical['message']) > 90:
                    report.append(f"      {critical['message'][90:180]}")
            report.append("")

        # ã‚¨ãƒ©ãƒ¼è©³ç´°ï¼ˆæœ€æ–°15ä»¶ï¼‰
        if self.stats['errors']:
            report.append(f"â–  ã‚¨ãƒ©ãƒ¼è©³ç´° (æœ€æ–°15ä»¶ / å…¨{len(self.stats['errors'])}ä»¶)")
            report.append("â”€" * 80)
            for i, error in enumerate(self.stats['errors'][-15:], 1):
                timestamp = error['timestamp'].strftime('%H:%M:%S') if error['timestamp'] else 'N/A'
                report.append(f"  {i:2d}. [{timestamp}] {error['message'][:90]}")
            report.append("")

        # è­¦å‘Šè©³ç´°ï¼ˆæœ€æ–°15ä»¶ï¼‰
        if self.stats['warnings']:
            report.append(f"â–  è­¦å‘Šè©³ç´° (æœ€æ–°15ä»¶ / å…¨{len(self.stats['warnings'])}ä»¶)")
            report.append("â”€" * 80)
            for i, warning in enumerate(self.stats['warnings'][-15:], 1):
                timestamp = warning['timestamp'].strftime('%H:%M:%S') if warning['timestamp'] else 'N/A'
                report.append(f"  {i:2d}. [{timestamp}] {warning['message'][:90]}")
            report.append("")

        report.append("=" * 80)
        return "\n".join(report)

    def save_detailed_report(self, output_path: Path):
        """è©³ç´°ãªJSONå½¢å¼ã®ãƒ¬ãƒãƒ¼ãƒˆã‚’ä¿å­˜"""
        detailed_report = {
            'metadata': {
                'log_directory': str(self.log_dir),
                'analysis_datetime': datetime.now().isoformat(),
                'total_entries': self.stats['total_entries'],
            },
            'log_levels': dict(self.stats['by_level']),
            'modules': dict(self.stats['by_module'].most_common(20)),
            'scraping_stats': {
                k: v if not isinstance(v, Counter) else dict(v)
                for k, v in self.stats['scraping_stats'].items()
            },
            'parsing_stats': {
                k: v if not isinstance(v, Counter) else dict(v)
                for k, v in self.stats['parsing_stats'].items()
            },
            'file_counts': self.stats['file_counts'],
            'error_count': len(self.stats['errors']),
            'warning_count': len(self.stats['warnings']),
            'critical_count': len(self.stats['criticals']),
            'errors': [
                {
                    'timestamp': e['timestamp'].isoformat() if e['timestamp'] else None,
                    'module': e['module'],
                    'message': e['message']
                }
                for e in self.stats['errors']
            ],
            'warnings': [
                {
                    'timestamp': w['timestamp'].isoformat() if w['timestamp'] else None,
                    'module': w['module'],
                    'message': w['message']
                }
                for w in self.stats['warnings']
            ],
            'criticals': [
                {
                    'timestamp': c['timestamp'].isoformat() if c['timestamp'] else None,
                    'module': c['module'],
                    'message': c['message']
                }
                for c in self.stats['criticals']
            ],
        }

        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(detailed_report, f, ensure_ascii=False, indent=2)

        print(f"ğŸ’¾ è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆã‚’ä¿å­˜: {output_path}")

    def analyze(self):
        """ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã®åˆ†æã‚’å®Ÿè¡Œ"""
        log_files = self.find_log_files()

        if not log_files:
            print("âš ï¸  ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
            return

        print(f"ğŸ“Š {len(log_files)} å€‹ã®ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’åˆ†æã—ã¾ã™\n")

        for log_file in log_files:
            self.analyze_log_file(log_file)

        print("\nâœ… åˆ†æå®Œäº†!\n")


def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    import sys
    from datetime import date

    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ãƒ­ã‚°ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªï¼ˆä»Šæ—¥ã®æ—¥ä»˜ï¼‰
    today = date.today()
    default_log_dir = Path(f"keibaai/data/logs/{today.year}/{today.month:02d}/{today.day:02d}")

    # ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã§ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’æŒ‡å®šå¯èƒ½
    if len(sys.argv) > 1:
        log_dir = Path(sys.argv[1])
    else:
        log_dir = default_log_dir

    print(f"ğŸ” KeibaAI_v2 ãƒ­ã‚°ã‚¢ãƒŠãƒ©ã‚¤ã‚¶ãƒ¼ (ä¿®æ­£ç‰ˆ)")
    print(f"ğŸ“ ãƒ­ã‚°ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {log_dir}\n")

    # ã‚¢ãƒŠãƒ©ã‚¤ã‚¶ãƒ¼ã‚’åˆæœŸåŒ–ã—ã¦å®Ÿè¡Œ
    analyzer = KeibaLogAnalyzer(log_dir)
    analyzer.analyze()

    # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
    report = analyzer.generate_report()
    print(report)

    # è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆã‚’JSONå½¢å¼ã§ä¿å­˜
    output_dir = Path("keibaai/data/logs/analysis")
    output_dir.mkdir(parents=True, exist_ok=True)

    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    json_output = output_dir / f"log_analysis_fixed_{timestamp}.json"
    analyzer.save_detailed_report(json_output)

    # ã‚µãƒãƒªãƒ¼ã‚’ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã«ã‚‚ä¿å­˜
    txt_output = output_dir / f"log_analysis_fixed_{timestamp}.txt"
    with open(txt_output, 'w', encoding='utf-8') as f:
        f.write(report)
    print(f"ğŸ’¾ ã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆã‚’ä¿å­˜: {txt_output}")

    print("\n" + "=" * 80)
    print("ğŸ“ˆ åˆ†æå®Œäº†! è©³ç´°ã¯ä¸Šè¨˜ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚")
    print("=" * 80)


if __name__ == "__main__":
    main()
