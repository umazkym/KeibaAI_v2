#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å†é–‹å¯èƒ½ãªã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
é€”ä¸­ã§åœæ­¢ã—ã¦ã‚‚ã€æ—¢ã«ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°æ¸ˆã¿ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¦ç¶šè¡Œå¯èƒ½
ã‚¨ãƒ©ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ã®è‡ªå‹•æ¤œå‡ºã¨å†ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã«å¯¾å¿œ
"""

import logging
import sqlite3
from pathlib import Path
import yaml
from datetime import datetime
import sys
import argparse
import os
from tqdm import tqdm

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’sys.pathã«è¿½åŠ 
project_root = Path(__file__).resolve().parent
sys.path.append(str(project_root / 'keibaai'))

from keibaai.src.modules.preparing import _scrape_html
from keibaai.src.modules.constants import LocalPaths


def load_config():
    """è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ã™ã‚‹"""
    config_path = Path(__file__).resolve().parent / "keibaai" / "configs"
    with open(config_path / "default.yaml", "r", encoding="utf-8") as f:
        default_cfg = yaml.safe_load(f)
    with open(config_path / "scraping.yaml", "r", encoding="utf-8") as f:
        scraping_cfg_raw = yaml.safe_load(f)

    data_root = Path(__file__).resolve().parent / "keibaai" / default_cfg['data_path']
    default_cfg['raw_data_path'] = str(data_root / 'raw')
    default_cfg['parsed_data_path'] = str(data_root / 'parsed')
    default_cfg['database']['path'] = str(data_root / 'metadata' / 'db.sqlite3')
    default_cfg['logging']['log_file'] = str(data_root / 'logs' / '{YYYY}' / '{MM}' / '{DD}' / 'scraping_resumable.log')

    config = {
        "default": default_cfg,
        "scraping": scraping_cfg_raw['scraping']
    }
    return config


def setup_logging(log_path_template: str):
    """ãƒ­ã‚°è¨­å®šã‚’è¡Œã†"""
    now = datetime.now()
    log_path = log_path_template.format(YYYY=now.year, MM=f"{now.month:02}", DD=f"{now.day:02}")
    Path(log_path).parent.mkdir(parents=True, exist_ok=True)

    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
        handlers=[
            logging.FileHandler(log_path, encoding='utf-8'),
            logging.StreamHandler(sys.stdout)
        ]
    )


def get_error_files(conn, error_type: str) -> set:
    """
    ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸIDã‚’å–å¾—

    Args:
        conn: SQLiteæ¥ç¶š
        error_type: ã‚¨ãƒ©ãƒ¼ã‚¿ã‚¤ãƒ— ('race', 'shutuba', 'horse', 'pedigree')

    Returns:
        ã‚¨ãƒ©ãƒ¼IDã®ã‚»ãƒƒãƒˆ
    """
    try:
        cursor = conn.cursor()
        # ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚¨ãƒ©ãƒ¼ã‚‚åŒã˜parse_failuresãƒ†ãƒ¼ãƒ–ãƒ«ã«è¨˜éŒ²ã•ã‚Œã¦ã„ã‚‹æƒ³å®š
        # ã‚‚ã—å°‚ç”¨ãƒ†ãƒ¼ãƒ–ãƒ«ãŒã‚ã‚Œã°ãã‚Œã‚’ä½¿ã†
        cursor.execute('''
            SELECT DISTINCT source_file
            FROM parse_failures
            WHERE parser_name LIKE ?
            ORDER BY failed_ts DESC
        ''', (f'%{error_type}%',))

        error_ids = set()
        for row in cursor.fetchall():
            source_file = row[0]
            # ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰IDã‚’æŠ½å‡º
            file_id = Path(source_file).stem.replace('_perf', '').replace('_profile', '')
            error_ids.add(file_id)

        return error_ids
    except Exception as e:
        logging.warning(f"ã‚¨ãƒ©ãƒ¼IDå–å¾—å¤±æ•— ({error_type}): {e}")
        return set()


def clear_error_record(conn, file_path: str, parser_name: str):
    """
    ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°æˆåŠŸæ™‚ã«ã‚¨ãƒ©ãƒ¼ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’å‰Šé™¤

    Args:
        conn: SQLiteæ¥ç¶š
        file_path: ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        parser_name: ãƒ‘ãƒ¼ã‚µå
    """
    try:
        cursor = conn.cursor()
        cursor.execute('''
            DELETE FROM parse_failures
            WHERE source_file = ? AND parser_name = ?
        ''', (file_path, parser_name))
        conn.commit()

        deleted_count = cursor.rowcount
        if deleted_count > 0:
            logging.debug(f"ã‚¨ãƒ©ãƒ¼ãƒ¬ã‚³ãƒ¼ãƒ‰å‰Šé™¤: {file_path} ({deleted_count}ä»¶)")
    except Exception as e:
        logging.warning(f"ã‚¨ãƒ©ãƒ¼ãƒ¬ã‚³ãƒ¼ãƒ‰å‰Šé™¤å¤±æ•— ({file_path}): {e}")


def get_existing_files(directory: Path, extension: str = '.bin') -> set:
    """
    æ—¢å­˜ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰IDã‚’å–å¾—

    Args:
        directory: ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãƒ‘ã‚¹
        extension: ãƒ•ã‚¡ã‚¤ãƒ«æ‹¡å¼µå­

    Returns:
        æ—¢å­˜IDã®ã‚»ãƒƒãƒˆ
    """
    if not directory.exists():
        return set()

    existing_ids = set()
    for file in directory.glob(f'*{extension}'):
        file_id = file.stem.replace('_perf', '').replace('_profile', '')
        existing_ids.add(file_id)

    return existing_ids


def scrape_phase_races(conn, race_ids: list, skip_existing: bool = True, retry_errors: bool = False):
    """ãƒ•ã‚§ãƒ¼ã‚º0b-1: ãƒ¬ãƒ¼ã‚¹çµæœHTMLã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°"""
    log = logging.getLogger(__name__)
    log.info("\n" + "=" * 80)
    log.info("ã€ãƒ•ã‚§ãƒ¼ã‚º0b-1ã€‘ãƒ¬ãƒ¼ã‚¹çµæœHTMLã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚’é–‹å§‹")
    log.info("=" * 80)

    race_dir = Path(LocalPaths.HTML_RACE_DIR)
    race_dir.mkdir(parents=True, exist_ok=True)

    log.info(f"  â†’ {len(race_ids):,}ä»¶ã®ãƒ¬ãƒ¼ã‚¹IDãŒå¯¾è±¡ã§ã™")

    # ã‚¨ãƒ©ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—
    error_ids = get_error_files(conn, 'race')
    if error_ids:
        log.info(f"  â†’ ã‚¨ãƒ©ãƒ¼å±¥æ­´: {len(error_ids):,}ä»¶ (è‡ªå‹•çš„ã«å†ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å¯¾è±¡ã«å«ã‚ã¾ã™)")

    # æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç¢ºèª
    if skip_existing and not retry_errors:
        existing_ids = get_existing_files(race_dir)
        log.info(f"  â†’ æ—¢ã«ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°æ¸ˆã¿: {len(existing_ids):,}ä»¶")

        # æ–°è¦ID + ã‚¨ãƒ©ãƒ¼ID
        race_ids_to_process = [
            rid for rid in race_ids
            if rid not in existing_ids or rid in error_ids
        ]

        new_count = len([rid for rid in race_ids if rid not in existing_ids])
        error_retry = len([rid for rid in race_ids_to_process if rid in error_ids])
        log.info(f"  â†’ å‡¦ç†å¯¾è±¡: {len(race_ids_to_process):,}ä»¶ (æ–°è¦: {new_count:,}ä»¶, ã‚¨ãƒ©ãƒ¼å†å‡¦ç†: {error_retry:,}ä»¶)")
    elif retry_errors:
        # ã‚¨ãƒ©ãƒ¼IDã®ã¿
        race_ids_to_process = [rid for rid in race_ids if rid in error_ids]
        log.info(f"  â†’ å‡¦ç†å¯¾è±¡: {len(race_ids_to_process):,}ä»¶ (ã‚¨ãƒ©ãƒ¼å†å‡¦ç†ã®ã¿)")
    else:
        race_ids_to_process = race_ids
        log.info(f"  â†’ å‡¦ç†å¯¾è±¡: {len(race_ids_to_process):,}ä»¶ (å…¨ID)")

    if not race_ids_to_process:
        log.info("  â†’ å‡¦ç†å¯¾è±¡ãŒã‚ã‚Šã¾ã›ã‚“ã€‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
        return

    # ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Ÿè¡Œ
    log.info("  â†’ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚’é–‹å§‹ã—ã¾ã™...")
    saved_paths = _scrape_html.scrape_html_race(race_ids_to_process, skip=False)

    # çµæœç¢ºèªã¨çµ±è¨ˆ
    success_count = len(saved_paths)
    error_count = len(race_ids_to_process) - success_count

    # æˆåŠŸã—ãŸãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¨ãƒ©ãƒ¼ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’ã‚¯ãƒªã‚¢
    error_cleared_count = 0
    for race_id in race_ids_to_process:
        file_path = race_dir / f"{race_id}.bin"
        if file_path.exists() and race_id in error_ids:
            clear_error_record(conn, str(file_path), 'results_parser')
            error_cleared_count += 1

    log.info(f"  ğŸ“Š å‡¦ç†çµ±è¨ˆ: æˆåŠŸ {success_count:,}ä»¶ / ã‚¨ãƒ©ãƒ¼ {error_count:,}ä»¶ / ã‚¨ãƒ©ãƒ¼è§£æ¶ˆ {error_cleared_count:,}ä»¶")


def scrape_phase_shutuba(conn, race_ids: list, skip_existing: bool = True, retry_errors: bool = False):
    """ãƒ•ã‚§ãƒ¼ã‚º0b-2: å‡ºé¦¬è¡¨HTMLã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°"""
    log = logging.getLogger(__name__)
    log.info("\n" + "=" * 80)
    log.info("ã€ãƒ•ã‚§ãƒ¼ã‚º0b-2ã€‘å‡ºé¦¬è¡¨HTMLã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚’é–‹å§‹")
    log.info("=" * 80)

    shutuba_dir = Path(LocalPaths.HTML_SHUTUBA_DIR)
    shutuba_dir.mkdir(parents=True, exist_ok=True)

    log.info(f"  â†’ {len(race_ids):,}ä»¶ã®ãƒ¬ãƒ¼ã‚¹IDãŒå¯¾è±¡ã§ã™")

    # ã‚¨ãƒ©ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—
    error_ids = get_error_files(conn, 'shutuba')
    if error_ids:
        log.info(f"  â†’ ã‚¨ãƒ©ãƒ¼å±¥æ­´: {len(error_ids):,}ä»¶ (è‡ªå‹•çš„ã«å†ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å¯¾è±¡ã«å«ã‚ã¾ã™)")

    # æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç¢ºèª
    if skip_existing and not retry_errors:
        existing_ids = get_existing_files(shutuba_dir)
        log.info(f"  â†’ æ—¢ã«ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°æ¸ˆã¿: {len(existing_ids):,}ä»¶")

        shutuba_ids_to_process = [
            rid for rid in race_ids
            if rid not in existing_ids or rid in error_ids
        ]

        new_count = len([rid for rid in race_ids if rid not in existing_ids])
        error_retry = len([rid for rid in shutuba_ids_to_process if rid in error_ids])
        log.info(f"  â†’ å‡¦ç†å¯¾è±¡: {len(shutuba_ids_to_process):,}ä»¶ (æ–°è¦: {new_count:,}ä»¶, ã‚¨ãƒ©ãƒ¼å†å‡¦ç†: {error_retry:,}ä»¶)")
    elif retry_errors:
        shutuba_ids_to_process = [rid for rid in race_ids if rid in error_ids]
        log.info(f"  â†’ å‡¦ç†å¯¾è±¡: {len(shutuba_ids_to_process):,}ä»¶ (ã‚¨ãƒ©ãƒ¼å†å‡¦ç†ã®ã¿)")
    else:
        shutuba_ids_to_process = race_ids
        log.info(f"  â†’ å‡¦ç†å¯¾è±¡: {len(shutuba_ids_to_process):,}ä»¶ (å…¨ID)")

    if not shutuba_ids_to_process:
        log.info("  â†’ å‡¦ç†å¯¾è±¡ãŒã‚ã‚Šã¾ã›ã‚“ã€‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
        return

    log.info("  â†’ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚’é–‹å§‹ã—ã¾ã™...")
    saved_paths = _scrape_html.scrape_html_shutuba(shutuba_ids_to_process, skip=False)

    success_count = len(saved_paths)
    error_count = len(shutuba_ids_to_process) - success_count

    error_cleared_count = 0
    for race_id in shutuba_ids_to_process:
        file_path = shutuba_dir / f"{race_id}.bin"
        if file_path.exists() and race_id in error_ids:
            clear_error_record(conn, str(file_path), 'shutuba_parser')
            error_cleared_count += 1

    log.info(f"  ğŸ“Š å‡¦ç†çµ±è¨ˆ: æˆåŠŸ {success_count:,}ä»¶ / ã‚¨ãƒ©ãƒ¼ {error_count:,}ä»¶ / ã‚¨ãƒ©ãƒ¼è§£æ¶ˆ {error_cleared_count:,}ä»¶")


def scrape_phase_horses(conn, horse_ids: list, skip_existing: bool = True, retry_errors: bool = False):
    """ãƒ•ã‚§ãƒ¼ã‚º2-1: é¦¬ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«ãƒ»æˆç¸¾HTMLã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°"""
    log = logging.getLogger(__name__)
    log.info("\n" + "=" * 80)
    log.info("ã€ãƒ•ã‚§ãƒ¼ã‚º2-1ã€‘é¦¬ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«ãƒ»æˆç¸¾HTMLã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚’é–‹å§‹")
    log.info("=" * 80)

    horse_dir = Path(LocalPaths.HTML_HORSE_DIR)
    horse_dir.mkdir(parents=True, exist_ok=True)

    log.info(f"  â†’ {len(horse_ids):,}é ­ã®é¦¬IDãŒå¯¾è±¡ã§ã™")

    # ã‚¨ãƒ©ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—
    error_ids = get_error_files(conn, 'horse')
    if error_ids:
        log.info(f"  â†’ ã‚¨ãƒ©ãƒ¼å±¥æ­´: {len(error_ids):,}ä»¶ (è‡ªå‹•çš„ã«å†ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å¯¾è±¡ã«å«ã‚ã¾ã™)")

    # æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç¢ºèªï¼ˆãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã§åˆ¤å®šï¼‰
    if skip_existing and not retry_errors:
        existing_ids = set()
        for file in horse_dir.glob('*_profile.bin'):
            horse_id = file.stem.replace('_profile', '')
            existing_ids.add(horse_id)

        log.info(f"  â†’ æ—¢ã«ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°æ¸ˆã¿: {len(existing_ids):,}é ­")

        horse_ids_to_process = [
            hid for hid in horse_ids
            if hid not in existing_ids or hid in error_ids
        ]

        new_count = len([hid for hid in horse_ids if hid not in existing_ids])
        error_retry = len([hid for hid in horse_ids_to_process if hid in error_ids])
        log.info(f"  â†’ å‡¦ç†å¯¾è±¡: {len(horse_ids_to_process):,}é ­ (æ–°è¦: {new_count:,}é ­, ã‚¨ãƒ©ãƒ¼å†å‡¦ç†: {error_retry:,}é ­)")
    elif retry_errors:
        horse_ids_to_process = [hid for hid in horse_ids if hid in error_ids]
        log.info(f"  â†’ å‡¦ç†å¯¾è±¡: {len(horse_ids_to_process):,}é ­ (ã‚¨ãƒ©ãƒ¼å†å‡¦ç†ã®ã¿)")
    else:
        horse_ids_to_process = horse_ids
        log.info(f"  â†’ å‡¦ç†å¯¾è±¡: {len(horse_ids_to_process):,}é ­ (å…¨ID)")

    if not horse_ids_to_process:
        log.info("  â†’ å‡¦ç†å¯¾è±¡ãŒã‚ã‚Šã¾ã›ã‚“ã€‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
        return

    log.info("  â†’ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚’é–‹å§‹ã—ã¾ã™...")
    saved_paths = _scrape_html.scrape_html_horse(horse_ids_to_process, skip=False)

    success_count = len(saved_paths) // 2  # profile + perf ã§2ãƒ•ã‚¡ã‚¤ãƒ«
    error_count = len(horse_ids_to_process) - success_count

    error_cleared_count = 0
    for horse_id in horse_ids_to_process:
        profile_path = horse_dir / f"{horse_id}_profile.bin"
        if profile_path.exists() and horse_id in error_ids:
            clear_error_record(conn, str(profile_path), 'horse_info_parser')
            error_cleared_count += 1

    log.info(f"  ğŸ“Š å‡¦ç†çµ±è¨ˆ: æˆåŠŸ {success_count:,}é ­ / ã‚¨ãƒ©ãƒ¼ {error_count:,}é ­ / ã‚¨ãƒ©ãƒ¼è§£æ¶ˆ {error_cleared_count:,}é ­")


def scrape_phase_pedigrees(conn, horse_ids: list, skip_existing: bool = True, retry_errors: bool = False):
    """ãƒ•ã‚§ãƒ¼ã‚º2-2: è¡€çµ±HTMLã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°"""
    log = logging.getLogger(__name__)
    log.info("\n" + "=" * 80)
    log.info("ã€ãƒ•ã‚§ãƒ¼ã‚º2-2ã€‘è¡€çµ±HTMLã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚’é–‹å§‹")
    log.info("=" * 80)

    ped_dir = Path(LocalPaths.HTML_PED_DIR)
    ped_dir.mkdir(parents=True, exist_ok=True)

    log.info(f"  â†’ {len(horse_ids):,}é ­ã®é¦¬IDãŒå¯¾è±¡ã§ã™")

    # ã‚¨ãƒ©ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—
    error_ids = get_error_files(conn, 'pedigree')
    if error_ids:
        log.info(f"  â†’ ã‚¨ãƒ©ãƒ¼å±¥æ­´: {len(error_ids):,}ä»¶ (è‡ªå‹•çš„ã«å†ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å¯¾è±¡ã«å«ã‚ã¾ã™)")

    # æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç¢ºèª
    if skip_existing and not retry_errors:
        existing_ids = get_existing_files(ped_dir)
        log.info(f"  â†’ æ—¢ã«ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°æ¸ˆã¿: {len(existing_ids):,}é ­")

        ped_ids_to_process = [
            hid for hid in horse_ids
            if hid not in existing_ids or hid in error_ids
        ]

        new_count = len([hid for hid in horse_ids if hid not in existing_ids])
        error_retry = len([hid for hid in ped_ids_to_process if hid in error_ids])
        log.info(f"  â†’ å‡¦ç†å¯¾è±¡: {len(ped_ids_to_process):,}é ­ (æ–°è¦: {new_count:,}é ­, ã‚¨ãƒ©ãƒ¼å†å‡¦ç†: {error_retry:,}é ­)")
    elif retry_errors:
        ped_ids_to_process = [hid for hid in horse_ids if hid in error_ids]
        log.info(f"  â†’ å‡¦ç†å¯¾è±¡: {len(ped_ids_to_process):,}é ­ (ã‚¨ãƒ©ãƒ¼å†å‡¦ç†ã®ã¿)")
    else:
        ped_ids_to_process = horse_ids
        log.info(f"  â†’ å‡¦ç†å¯¾è±¡: {len(ped_ids_to_process):,}é ­ (å…¨ID)")

    if not ped_ids_to_process:
        log.info("  â†’ å‡¦ç†å¯¾è±¡ãŒã‚ã‚Šã¾ã›ã‚“ã€‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
        return

    log.info("  â†’ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚’é–‹å§‹ã—ã¾ã™ï¼ˆSeleniumä½¿ç”¨ã€æ™‚é–“ãŒã‹ã‹ã‚Šã¾ã™ï¼‰...")
    saved_paths = _scrape_html.scrape_html_ped(ped_ids_to_process, skip=False)

    success_count = len(saved_paths)
    error_count = len(ped_ids_to_process) - success_count

    error_cleared_count = 0
    for horse_id in ped_ids_to_process:
        file_path = ped_dir / f"{horse_id}.bin"
        if file_path.exists() and horse_id in error_ids:
            clear_error_record(conn, str(file_path), 'pedigree_parser')
            error_cleared_count += 1

    log.info(f"  ğŸ“Š å‡¦ç†çµ±è¨ˆ: æˆåŠŸ {success_count:,}é ­ / ã‚¨ãƒ©ãƒ¼ {error_count:,}é ­ / ã‚¨ãƒ©ãƒ¼è§£æ¶ˆ {error_cleared_count:,}é ­")


def main():
    """å†é–‹å¯èƒ½ãªã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³"""
    parser = argparse.ArgumentParser(description='å†é–‹å¯èƒ½ãªã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³')
    parser.add_argument('--from-date', type=str, required=True,
                        help='é–‹å§‹æ—¥ (YYYY-MM-DDå½¢å¼)')
    parser.add_argument('--to-date', type=str, required=True,
                        help='çµ‚äº†æ—¥ (YYYY-MM-DDå½¢å¼)')
    parser.add_argument('--force-rescrape', action='store_true',
                        help='æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å«ã‚ã¦å…¨ä»¶å†ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯æ–°è¦ãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿å‡¦ç†ï¼‰')
    parser.add_argument('--retry-errors', action='store_true',
                        help='ã‚¨ãƒ©ãƒ¼IDã®ã¿å†ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ï¼ˆä»–ã®IDã¯ã‚¹ã‚­ãƒƒãƒ—ï¼‰')
    parser.add_argument('--phase', type=str, default='all',
                        choices=['0a', '0b', '1', '2', 'all'],
                        help='å®Ÿè¡Œã™ã‚‹ãƒ•ã‚§ãƒ¼ã‚º (0a: é–‹å‚¬æ—¥/ãƒ¬ãƒ¼ã‚¹ID, 0b: ãƒ¬ãƒ¼ã‚¹/å‡ºé¦¬è¡¨, 1: é¦¬ID, 2: é¦¬ãƒ‡ãƒ¼ã‚¿/è¡€çµ±, all: å…¨ã¦)')
    args = parser.parse_args()

    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯ã‚¹ã‚­ãƒƒãƒ—ãƒ¢ãƒ¼ãƒ‰ï¼ˆæ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†ã—ãªã„ï¼‰
    skip_existing = not args.force_rescrape

    cfg = load_config()
    setup_logging(cfg["default"]["logging"]["log_file"])

    log = logging.getLogger(__name__)
    log.info("=" * 80)
    log.info("å†é–‹å¯èƒ½ãªã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’é–‹å§‹ã—ã¾ã™...")
    log.info(f"æœŸé–“: {args.from_date} ï½ {args.to_date}")
    if args.retry_errors:
        log.info(f"ãƒ¢ãƒ¼ãƒ‰: ã‚¨ãƒ©ãƒ¼IDã®ã¿å†ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°")
    elif args.force_rescrape:
        log.info(f"ãƒ¢ãƒ¼ãƒ‰: å…¨ä»¶å†ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°")
    else:
        log.info(f"ãƒ¢ãƒ¼ãƒ‰: æ–°è¦IDã®ã¿å‡¦ç† (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ)")
    log.info(f"ãƒ•ã‚§ãƒ¼ã‚º: {args.phase}")
    log.info("=" * 80)

    db_path = Path(cfg["default"]["database"]["path"])
    db_path.parent.mkdir(parents=True, exist_ok=True)
    conn = sqlite3.connect(db_path)

    try:
        # --- ãƒ•ã‚§ãƒ¼ã‚º0a: é–‹å‚¬æ—¥ã¨ãƒ¬ãƒ¼ã‚¹IDã®å–å¾— ---
        if args.phase in ['0a', 'all']:
            log.info("\n" + "=" * 80)
            log.info("ã€ãƒ•ã‚§ãƒ¼ã‚º0aã€‘é–‹å‚¬æ—¥ã¨ãƒ¬ãƒ¼ã‚¹IDã®å–å¾—ã‚’é–‹å§‹")
            log.info("=" * 80)

            kaisai_dates = _scrape_html.scrape_kaisai_date(args.from_date, args.to_date)
            log.info(f"  âœ… {len(kaisai_dates)}å€‹ã®é–‹å‚¬æ—¥ã‚’å–å¾—ã—ã¾ã—ãŸ")

            if not kaisai_dates:
                log.warning("  âš ï¸ é–‹å‚¬æ—¥ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚å‡¦ç†ã‚’ä¸­æ–­ã—ã¾ã™ã€‚")
                return

            race_ids = _scrape_html.scrape_race_id_list(kaisai_dates)
            log.info(f"  âœ… {len(race_ids)}å€‹ã®ãƒ¬ãƒ¼ã‚¹IDã‚’å–å¾—ã—ã¾ã—ãŸ")

            if not race_ids:
                log.warning("  âš ï¸ ãƒ¬ãƒ¼ã‚¹IDãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚å‡¦ç†ã‚’ä¸­æ–­ã—ã¾ã™ã€‚")
                return
        else:
            # æ—¢å­˜ã®ãƒ¬ãƒ¼ã‚¹IDãƒªã‚¹ãƒˆã‚’èª­ã¿è¾¼ã‚€
            import pandas as pd
            race_id_file = Path("race_id_list.csv")
            if race_id_file.exists():
                race_ids = pd.read_csv(race_id_file)['race_id'].astype(str).tolist()
                log.info(f"  â†’ æ—¢å­˜ã®ãƒ¬ãƒ¼ã‚¹IDãƒªã‚¹ãƒˆã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ: {len(race_ids)}ä»¶")
            else:
                log.error("  âŒ race_id_list.csv ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ãƒ•ã‚§ãƒ¼ã‚º0aã‚’å…ˆã«å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
                return

        # --- ãƒ•ã‚§ãƒ¼ã‚º0b: ãƒ¬ãƒ¼ã‚¹çµæœã¨å‡ºé¦¬è¡¨ã®å–å¾— ---
        if args.phase in ['0b', 'all']:
            scrape_phase_races(conn, race_ids, skip_existing=skip_existing, retry_errors=args.retry_errors)
            scrape_phase_shutuba(conn, race_ids, skip_existing=skip_existing, retry_errors=args.retry_errors)

        # --- ãƒ•ã‚§ãƒ¼ã‚º1: é¦¬IDã®åé›† ---
        if args.phase in ['1', 'all']:
            log.info("\n" + "=" * 80)
            log.info("ã€ãƒ•ã‚§ãƒ¼ã‚º1ã€‘é¦¬IDã®åé›†ã‚’é–‹å§‹")
            log.info("=" * 80)

            raw_race_dir = Path(cfg["default"]["raw_data_path"]) / "html" / "race"
            if not raw_race_dir.exists():
                log.error(f"  âŒ ãƒ¬ãƒ¼ã‚¹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã—ã¾ã›ã‚“: {raw_race_dir}")
                return

            horse_ids = _scrape_html.extract_horse_ids_from_html(str(raw_race_dir))
            log.info(f"  âœ… {len(horse_ids)}é ­ã®ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªé¦¬IDã‚’å–å¾—ã—ã¾ã—ãŸ")

            if not horse_ids:
                log.warning("  âš ï¸ é¦¬IDãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
                return

            horse_ids = list(horse_ids)
        else:
            # æ—¢å­˜ã®é¦¬IDãƒªã‚¹ãƒˆã‚’èª­ã¿è¾¼ã‚€
            horse_id_file = Path("horse_id_list.txt")
            if horse_id_file.exists():
                with open(horse_id_file, 'r') as f:
                    horse_ids = [line.strip() for line in f if line.strip()]
                log.info(f"  â†’ æ—¢å­˜ã®é¦¬IDãƒªã‚¹ãƒˆã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ: {len(horse_ids)}é ­")
            else:
                log.warning("  âš ï¸ horse_id_list.txt ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
                horse_ids = []

        # --- ãƒ•ã‚§ãƒ¼ã‚º2: é¦¬ãƒ‡ãƒ¼ã‚¿ã¨è¡€çµ±ã®å–å¾— ---
        if args.phase in ['2', 'all'] and horse_ids:
            scrape_phase_horses(conn, horse_ids, skip_existing=skip_existing, retry_errors=args.retry_errors)
            scrape_phase_pedigrees(conn, horse_ids, skip_existing=skip_existing, retry_errors=args.retry_errors)

        log.info("\n" + "=" * 80)
        log.info("âœ… ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ãŒæ­£å¸¸ã«çµ‚äº†ã—ã¾ã—ãŸ")
        log.info("=" * 80)

    except KeyboardInterrupt:
        log.warning("\n" + "=" * 80)
        log.warning("âš ï¸ ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã‚ˆã‚‹ä¸­æ–­ã‚’æ¤œå‡ºã—ã¾ã—ãŸ")
        log.warning("æ¬¡å›å®Ÿè¡Œæ™‚ã«ãã®ã¾ã¾å†å®Ÿè¡Œã™ã‚‹ã¨ã€ã“ã“ã‹ã‚‰å†é–‹ã§ãã¾ã™:")
        log.warning(f"  python run_scraping_resumable.py --from-date {args.from_date} --to-date {args.to_date} --phase {args.phase}")
        log.warning("=" * 80)
    except Exception as e:
        log.error(f"âŒ ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®å®Ÿè¡Œä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}", exc_info=True)
    finally:
        conn.close()
        log.info("ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šã‚’ã‚¯ãƒ­ãƒ¼ã‚ºã—ã¾ã—ãŸ")


if __name__ == "__main__":
    main()
