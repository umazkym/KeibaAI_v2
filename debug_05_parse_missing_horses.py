#!/usr/bin/env python3
"""
ãƒ‡ãƒãƒƒã‚°ã‚¹ã‚¯ãƒªãƒ—ãƒˆ05: ä¸è¶³ã—ã¦ã„ã‚‹é¦¬ãƒ‡ãƒ¼ã‚¿ã®å·®åˆ†ãƒ‘ãƒ¼ã‚¹

races.parquet ã«ç™»å ´ã™ã‚‹6,601é ­ã®ã†ã¡ã€
horses.parquet ã«å­˜åœ¨ã—ãªã„5,601é ­åˆ†ã‚’è¿½åŠ ãƒ‘ãƒ¼ã‚¹ã—ã¾ã™ã€‚

å‡¦ç†å†…å®¹:
1. å·®åˆ†é¦¬IDãƒªã‚¹ãƒˆã‚’ä½œæˆï¼ˆ5,601é ­ï¼‰
2. è©²å½“ã™ã‚‹HTMLãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢
3. é¦¬ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«ãƒ»è¡€çµ±ãƒ»éå»æˆç¸¾ã‚’ãƒ‘ãƒ¼ã‚¹
4. æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã¨çµåˆã—ã¦ä¿å­˜
"""

import pandas as pd
import numpy as np
from pathlib import Path
from datetime import datetime
import sys
import logging

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’sys.pathã«è¿½åŠ 
project_root = Path(__file__).resolve().parent
sys.path.append(str(project_root / "keibaai"))

from keibaai.src.modules.parsers import horse_info_parser, pedigree_parser
from keibaai.src import pipeline_core
import sqlite3

# ãƒ­ã‚®ãƒ³ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)


def get_missing_horse_ids():
    """racesã«ç™»å ´ã™ã‚‹ãŒhorsesã«ãªã„é¦¬IDã‚’æŠ½å‡º"""
    print("=" * 80)
    print("ğŸ” ä¸è¶³ã—ã¦ã„ã‚‹é¦¬IDã®ç‰¹å®š")
    print("=" * 80)

    # races.parquet ã‹ã‚‰å…¨é¦¬ID
    races_path = Path("keibaai/data/parsed/parquet/races/races.parquet")
    df_races = pd.read_parquet(races_path)
    races_horse_ids = set(df_races['horse_id'].unique())
    print(f"\nraces.parquet ã®é¦¬IDæ•°: {len(races_horse_ids):,}")

    # horses.parquet ã‹ã‚‰æ—¢å­˜é¦¬ID
    horses_path = Path("keibaai/data/parsed/parquet/horses/horses.parquet")
    if horses_path.exists():
        df_horses = pd.read_parquet(horses_path)
        existing_horse_ids = set(df_horses['horse_id'].unique())
        print(f"horses.parquet ã®é¦¬IDæ•°: {len(existing_horse_ids):,}")
    else:
        existing_horse_ids = set()
        print("horses.parquet ãŒå­˜åœ¨ã—ã¾ã›ã‚“ï¼ˆå…¨é ­ãŒå¯¾è±¡ï¼‰")

    # å·®åˆ†
    missing_horse_ids = races_horse_ids - existing_horse_ids
    print(f"\nğŸ“Š ä¸è¶³ã—ã¦ã„ã‚‹é¦¬IDæ•°: {len(missing_horse_ids):,}")

    if missing_horse_ids:
        print(f"\nã‚µãƒ³ãƒ—ãƒ«ï¼ˆæœ€åˆã®10é ­ï¼‰:")
        for horse_id in sorted(missing_horse_ids)[:10]:
            print(f"  - {horse_id}")

    return sorted(missing_horse_ids), existing_horse_ids


def find_html_files(missing_horse_ids):
    """ä¸è¶³ã—ã¦ã„ã‚‹é¦¬ã®HTMLãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢"""
    print("\n" + "=" * 80)
    print("ğŸ“ HTMLãƒ•ã‚¡ã‚¤ãƒ«ã®æ¤œç´¢")
    print("=" * 80)

    html_dir = Path("keibaai/data/raw/html/horse")
    ped_dir = Path("keibaai/data/raw/html/ped")

    profile_files = []
    perf_files = []
    ped_files = []

    print(f"\næ¤œç´¢å¯¾è±¡: {len(missing_horse_ids):,} é ­")

    # ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«ãƒ»éå»æˆç¸¾ãƒ•ã‚¡ã‚¤ãƒ«
    for horse_id in missing_horse_ids:
        profile_file = html_dir / f"{horse_id}_profile.bin"
        perf_file = html_dir / f"{horse_id}_perf.bin"

        if profile_file.exists():
            profile_files.append(str(profile_file))
        if perf_file.exists():
            perf_files.append(str(perf_file))

    # è¡€çµ±ãƒ•ã‚¡ã‚¤ãƒ«
    for horse_id in missing_horse_ids:
        ped_file = ped_dir / f"{horse_id}.bin"
        if ped_file.exists():
            ped_files.append(str(ped_file))

    print(f"\nâœ… è¦‹ã¤ã‹ã£ãŸãƒ•ã‚¡ã‚¤ãƒ«:")
    print(f"  ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«: {len(profile_files):,} ä»¶")
    print(f"  éå»æˆç¸¾: {len(perf_files):,} ä»¶")
    print(f"  è¡€çµ±: {len(ped_files):,} ä»¶")

    missing_profile = len(missing_horse_ids) - len(profile_files)
    if missing_profile > 0:
        print(f"\nâš ï¸ ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«HTMLãŒè¦‹ã¤ã‹ã‚‰ãªã„é¦¬: {missing_profile:,} é ­")

    return profile_files, perf_files, ped_files


def parse_horse_profiles(profile_files):
    """é¦¬ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«ã‚’ãƒ‘ãƒ¼ã‚¹"""
    print("\n" + "=" * 80)
    print("ğŸ´ é¦¬ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«ã®ãƒ‘ãƒ¼ã‚¹")
    print("=" * 80)

    all_horses_data = []
    errors = []

    # DBãƒ€ãƒŸãƒ¼æ¥ç¶šï¼ˆã‚¨ãƒ©ãƒ¼ãƒ­ã‚°ç”¨ï¼‰
    db_path = Path("keibaai/data/metadata/db.sqlite3")
    db_path.parent.mkdir(parents=True, exist_ok=True)
    conn = sqlite3.connect(db_path)

    total = len(profile_files)
    for i, html_file in enumerate(profile_files, 1):
        if i % 100 == 0:
            print(f"  å‡¦ç†ä¸­: {i:,} / {total:,} ({i/total*100:.1f}%)")

        try:
            data = pipeline_core.parse_with_error_handling(
                str(html_file),
                "horse_info_parser",
                horse_info_parser.parse_horse_profile,
                conn
            )

            if data and 'horse_id' in data and data['horse_id']:
                if not data.get('_is_empty', False):
                    all_horses_data.append(data)
        except Exception as e:
            errors.append((html_file, str(e)))
            logger.warning(f"ãƒ‘ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼: {html_file} - {e}")

    conn.close()

    print(f"\nâœ… ãƒ‘ãƒ¼ã‚¹å®Œäº†: {len(all_horses_data):,} é ­")
    if errors:
        print(f"âš ï¸ ã‚¨ãƒ©ãƒ¼: {len(errors)} ä»¶")

    return pd.DataFrame(all_horses_data) if all_horses_data else pd.DataFrame()


def parse_pedigrees(ped_files):
    """è¡€çµ±ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ‘ãƒ¼ã‚¹"""
    print("\n" + "=" * 80)
    print("ğŸŒ³ è¡€çµ±ãƒ‡ãƒ¼ã‚¿ã®ãƒ‘ãƒ¼ã‚¹")
    print("=" * 80)

    all_pedigrees_df = []
    errors = []

    db_path = Path("keibaai/data/metadata/db.sqlite3")
    conn = sqlite3.connect(db_path)

    total = len(ped_files)
    for i, html_file in enumerate(ped_files, 1):
        if i % 100 == 0:
            print(f"  å‡¦ç†ä¸­: {i:,} / {total:,} ({i/total*100:.1f}%)")

        try:
            df = pipeline_core.parse_with_error_handling(
                str(html_file),
                "pedigree_parser",
                pedigree_parser.parse_pedigree_html,
                conn
            )

            if df is not None and not df.empty:
                all_pedigrees_df.append(df)
        except Exception as e:
            errors.append((html_file, str(e)))
            logger.warning(f"ãƒ‘ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼: {html_file} - {e}")

    conn.close()

    if all_pedigrees_df:
        final_df = pd.concat(all_pedigrees_df, ignore_index=True)
        print(f"\nâœ… ãƒ‘ãƒ¼ã‚¹å®Œäº†: {len(final_df):,} è¡Œï¼ˆ{final_df['horse_id'].nunique():,} é ­ï¼‰")
    else:
        final_df = pd.DataFrame()
        print("\nâš ï¸ ãƒ‘ãƒ¼ã‚¹ã§ããŸãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")

    if errors:
        print(f"âš ï¸ ã‚¨ãƒ©ãƒ¼: {len(errors)} ä»¶")

    return final_df


def parse_horse_performance(perf_files):
    """é¦¬éå»æˆç¸¾ã‚’ãƒ‘ãƒ¼ã‚¹"""
    print("\n" + "=" * 80)
    print("ğŸ“Š é¦¬éå»æˆç¸¾ã®ãƒ‘ãƒ¼ã‚¹")
    print("=" * 80)

    all_perf_df = []
    errors = []

    db_path = Path("keibaai/data/metadata/db.sqlite3")
    conn = sqlite3.connect(db_path)

    total = len(perf_files)
    for i, html_file in enumerate(perf_files, 1):
        if i % 100 == 0:
            print(f"  å‡¦ç†ä¸­: {i:,} / {total:,} ({i/total*100:.1f}%)")

        try:
            df = pipeline_core.parse_with_error_handling(
                str(html_file),
                "horse_performance_parser",
                horse_info_parser.parse_horse_performance,
                conn
            )

            if df is not None and not df.empty:
                all_perf_df.append(df)
        except Exception as e:
            errors.append((html_file, str(e)))
            logger.warning(f"ãƒ‘ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼: {html_file} - {e}")

    conn.close()

    if all_perf_df:
        final_df = pd.concat(all_perf_df, ignore_index=True)
        print(f"\nâœ… ãƒ‘ãƒ¼ã‚¹å®Œäº†: {len(final_df):,} è¡Œï¼ˆ{final_df['horse_id'].nunique():,} é ­ï¼‰")
    else:
        final_df = pd.DataFrame()
        print("\nâš ï¸ ãƒ‘ãƒ¼ã‚¹ã§ããŸãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")

    if errors:
        print(f"âš ï¸ ã‚¨ãƒ©ãƒ¼: {len(errors)} ä»¶")

    return final_df


def merge_and_save(new_horses_df, new_pedigrees_df, new_performance_df):
    """æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã¨çµåˆã—ã¦ä¿å­˜"""
    print("\n" + "=" * 80)
    print("ğŸ’¾ ãƒ‡ãƒ¼ã‚¿ã®çµåˆã¨ä¿å­˜")
    print("=" * 80)

    base_dir = Path("keibaai/data/parsed/parquet")

    # --- horses.parquet ã®æ›´æ–° ---
    horses_path = base_dir / "horses" / "horses.parquet"
    if horses_path.exists():
        existing_horses = pd.read_parquet(horses_path)
        print(f"\næ—¢å­˜ horses.parquet: {len(existing_horses):,} é ­")
        print(f"æ–°è¦ãƒ‡ãƒ¼ã‚¿: {len(new_horses_df):,} é ­")

        # çµåˆ
        combined_horses = pd.concat([existing_horses, new_horses_df], ignore_index=True)
        # é‡è¤‡æ’é™¤ï¼ˆæœ€æ–°ã‚’å„ªå…ˆï¼‰
        combined_horses = combined_horses.drop_duplicates(subset='horse_id', keep='last')
        print(f"çµåˆå¾Œï¼ˆé‡è¤‡æ’é™¤ï¼‰: {len(combined_horses):,} é ­")
    else:
        combined_horses = new_horses_df
        print(f"\næ–°è¦ä½œæˆ horses.parquet: {len(combined_horses):,} é ­")

    combined_horses.to_parquet(horses_path, index=False)
    print(f"âœ… ä¿å­˜å®Œäº†: {horses_path}")

    # --- pedigrees.parquet ã®æ›´æ–° ---
    pedigrees_path = base_dir / "pedigrees" / "pedigrees.parquet"
    if pedigrees_path.exists() and not new_pedigrees_df.empty:
        existing_pedigrees = pd.read_parquet(pedigrees_path)
        print(f"\næ—¢å­˜ pedigrees.parquet: {len(existing_pedigrees):,} è¡Œ")
        print(f"æ–°è¦ãƒ‡ãƒ¼ã‚¿: {len(new_pedigrees_df):,} è¡Œ")

        # çµåˆ
        combined_pedigrees = pd.concat([existing_pedigrees, new_pedigrees_df], ignore_index=True)
        # é‡è¤‡æ’é™¤
        combined_pedigrees = combined_pedigrees.drop_duplicates(
            subset=['horse_id', 'ancestor_id', 'generation'],
            keep='last'
        )
        print(f"çµåˆå¾Œï¼ˆé‡è¤‡æ’é™¤ï¼‰: {len(combined_pedigrees):,} è¡Œ")
    else:
        combined_pedigrees = new_pedigrees_df
        print(f"\næ–°è¦ä½œæˆ pedigrees.parquet: {len(combined_pedigrees):,} è¡Œ")

    if not combined_pedigrees.empty:
        combined_pedigrees.to_parquet(pedigrees_path, index=False)
        print(f"âœ… ä¿å­˜å®Œäº†: {pedigrees_path}")

    # --- horses_performance.parquet ã®ä½œæˆ/æ›´æ–° ---
    if not new_performance_df.empty:
        perf_dir = base_dir / "horses_performance"
        perf_dir.mkdir(parents=True, exist_ok=True)
        perf_path = perf_dir / "horses_performance.parquet"

        if perf_path.exists():
            existing_perf = pd.read_parquet(perf_path)
            print(f"\næ—¢å­˜ horses_performance.parquet: {len(existing_perf):,} è¡Œ")
            print(f"æ–°è¦ãƒ‡ãƒ¼ã‚¿: {len(new_performance_df):,} è¡Œ")

            combined_perf = pd.concat([existing_perf, new_performance_df], ignore_index=True)
            combined_perf = combined_perf.drop_duplicates(
                subset=['horse_id', 'race_date', 'race_number'],
                keep='last'
            )
            print(f"çµåˆå¾Œï¼ˆé‡è¤‡æ’é™¤ï¼‰: {len(combined_perf):,} è¡Œ")
        else:
            combined_perf = new_performance_df
            print(f"\næ–°è¦ä½œæˆ horses_performance.parquet: {len(combined_perf):,} è¡Œ")

        # ãƒ‡ãƒ¼ã‚¿å‹æœ€é©åŒ–
        int_columns = ['race_number', 'head_count', 'bracket_number', 'horse_number',
                      'finish_position', 'popularity', 'horse_weight', 'horse_weight_change']
        for col in int_columns:
            if col in combined_perf.columns:
                combined_perf[col] = combined_perf[col].astype('Int64')

        combined_perf.to_parquet(perf_path, index=False)
        print(f"âœ… ä¿å­˜å®Œäº†: {perf_path}")


def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    print("=" * 80)
    print("ğŸ” KeibaAI_v2 ä¸è¶³é¦¬ãƒ‡ãƒ¼ã‚¿ã®å·®åˆ†ãƒ‘ãƒ¼ã‚¹")
    print(f"â° å®Ÿè¡Œæ™‚åˆ»: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 80)

    # ã‚¹ãƒ†ãƒƒãƒ—1: ä¸è¶³ã—ã¦ã„ã‚‹é¦¬IDã‚’ç‰¹å®š
    missing_horse_ids, existing_horse_ids = get_missing_horse_ids()

    if not missing_horse_ids:
        print("\nâœ… ã™ã¹ã¦ã®é¦¬ãƒ‡ãƒ¼ã‚¿ãŒæƒã£ã¦ã„ã¾ã™ï¼")
        return

    # ã‚¹ãƒ†ãƒƒãƒ—2: HTMLãƒ•ã‚¡ã‚¤ãƒ«æ¤œç´¢
    profile_files, perf_files, ped_files = find_html_files(missing_horse_ids)

    if not profile_files and not ped_files:
        print("\nâš ï¸ ãƒ‘ãƒ¼ã‚¹å¯¾è±¡ã®HTMLãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
        return

    # ã‚¹ãƒ†ãƒƒãƒ—3: ãƒ‘ãƒ¼ã‚¹å®Ÿè¡Œ
    print("\n" + "=" * 80)
    print("âš™ï¸ ãƒ‘ãƒ¼ã‚¹å‡¦ç†é–‹å§‹")
    print("=" * 80)

    new_horses_df = parse_horse_profiles(profile_files) if profile_files else pd.DataFrame()
    new_pedigrees_df = parse_pedigrees(ped_files) if ped_files else pd.DataFrame()
    new_performance_df = parse_horse_performance(perf_files) if perf_files else pd.DataFrame()

    # ã‚¹ãƒ†ãƒƒãƒ—4: ä¿å­˜
    if not new_horses_df.empty or not new_pedigrees_df.empty:
        merge_and_save(new_horses_df, new_pedigrees_df, new_performance_df)
    else:
        print("\nâš ï¸ ãƒ‘ãƒ¼ã‚¹ã§ããŸãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")

    print("\n" + "=" * 80)
    print("ğŸ“‹ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—")
    print("=" * 80)
    print("""
ğŸ’¡ å·®åˆ†ãƒ‘ãƒ¼ã‚¹å®Œäº†ï¼æ¬¡ã¯ä»¥ä¸‹ã‚’å®Ÿè¡Œã—ã¾ã™:

1. ãƒ™ãƒ¼ã‚¹ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’å†ä½œæˆï¼ˆè¡€çµ±æƒ…å ±ã‚’å«ã‚€ï¼‰
   â†’ python debug_04_create_analysis_base_table.py

2. è¡€çµ±æƒ…å ±ã®æ¬ æç‡ã‚’ç¢ºèª
   â†’ 91% â†’ æ•°%ã«æ”¹å–„ã•ã‚Œã¦ã„ã‚‹ã¯ãš

3. FeatureEngine ã§ç‰¹å¾´é‡ç”Ÿæˆ
   â†’ python keibaai/src/features/generate_features.py
    """)

    print("=" * 80)
    print("âœ… ãƒ‡ãƒãƒƒã‚°ã‚¹ã‚¯ãƒªãƒ—ãƒˆ05å®Œäº†")
    print("=" * 80)


if __name__ == "__main__":
    main()
