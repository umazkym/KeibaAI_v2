# スクレイピング手法の.bin 形式への移行ガイド

## 概要

このドキュメントでは、既存のスクレイピング手法を`.html`形式から`.bin`形式に移行する手順を説明します。

## ファイル形式の変更

### 旧形式（バージョン付き）

- race: `race_202305020811_20251111T102824+0900_sha256=c90e86f1.html`
- horse: `horse_2017102294_20251111T091410+0900_sha256=1a2b3c4d.html`

### 新形式（シンプル）

- race: `202305020811.bin`
- horse: `2017102294_profile.bin`, `2017102294_perf.bin`
- ped: `2017102294.bin`
- shutuba: `202305020811.bin`

## 実装手順

### 1. ファイルのバックアップ

```bash
# 既存のファイルをバックアップ
cp src/modules/preparing/_requests_utils.py src/modules/preparing/_requests_utils.py.bak
cp src/utils/data_utils.py src/utils/data_utils.py.bak
cp src/run_scraping_pipeline_local.py src/run_scraping_pipeline_local.py.bak
```

### 2. 新しいファイルの配置

```bash
# 新規作成したファイルを配置
cp _scrape_html.py src/modules/preparing/_scrape_html.py
cp run_scraping_pipeline_local_v2.py src/run_scraping_pipeline_local.py
cp _requests_utils_v2.py src/modules/preparing/_requests_utils.py
cp data_utils_v2.py src/utils/data_utils.py
```

### 3. 依存関係の確認

必要なパッケージがインストールされているか確認：

```python
# requirements.txt に追加
selenium>=4.0.0
webdriver-manager>=3.8.0
```

### 4. 使用方法

#### 基本的な使用例

```python
from src.modules.preparing import _scrape_html

# 開催日を取得
kaisai_dates = _scrape_html.scrape_kaisai_date("2023-01-01", "2023-01-31")

# レースIDを取得
race_ids = _scrape_html.scrape_race_id_list(kaisai_dates)

# レース結果を取得（.bin形式で保存）
race_paths = _scrape_html.scrape_html_race(race_ids)

# 馬情報を取得（プロフィールと成績を分離）
horse_paths = _scrape_html.scrape_html_horse(horse_id_list)

# 血統情報を取得（Selenium使用）
ped_paths = _scrape_html.scrape_html_ped(horse_id_list)
```

## 主な変更点

### 1. BAN 回避策の強化

```python
# User-Agentのローテーション
USER_AGENTS = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36...",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36...",
    "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36..."
]

# ランダムな待機時間
MIN_SLEEP_SECONDS = 2.5
MAX_SLEEP_SECONDS = 5.0

# HTTP 400エラー時の長時間待機
HTTP_400_SLEEP_SECONDS = 60
```

### 2. Selenium の自動化検出回避

```python
# 自動化フラグの隠蔽
options.add_argument("--disable-blink-features=AutomationControlled")
options.add_experimental_option("excludeSwitches", ["enable-automation"])
driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
```

### 3. 馬の過去成績（AJAX）の取得

```python
# AJAX APIへのアクセス
ajax_url = 'https://db.netkeiba.com/horse/ajax_horse_results.html'
headers = {
    "Referer": f"https://db.netkeiba.com/horse/{horse_id}",
    "X-Requested-With": "XMLHttpRequest"
}
response = session.get(ajax_url, params={'id': horse_id}, headers=headers)
```

## 注意事項

1. **既存データとの互換性**

   - 既存の`.html`ファイルは削除せずに残しておくことを推奨
   - パーサーは両方の形式（`.bin`と`.html`）に対応しているため、段階的な移行が可能

2. **キャッシュ戦略**

   - 現在の実装では簡易版のキャッシュ（既存ファイルのスキップ）のみ
   - 完全なキャッシュ戦略（最終更新日時の管理）は今後実装予定

3. **エラーハンドリング**
   - ネットワークエラーやパースエラーは適切にログに記録
   - メタデータ DB にエラー情報も保存

## トラブルシューティング

### Selenium 関連のエラー

```bash
# ChromeDriverが見つからない場合
pip install webdriver-manager

# ヘッドレスモードでエラーが出る場合
# prepare_chrome_driver(headless=False) に変更して確認
```

### IP BAN された場合

- HTTP 400 エラーが頻発する場合は、IP がブロックされている可能性
- より長い待機時間を設定するか、プロキシの使用を検討

## 今後の改善点

1. **キャッシュ戦略の完全実装**

   - `horse_cache_log.csv`による更新管理
   - 差分取得の最適化

2. **並列処理の実装**

   - 複数の WebDriver インスタンスによる並列取得
   - スレッドプールによる効率化

3. **プロキシサポート**

   - 複数 IP からのアクセスによる BAN 回避

4. **設定の外部化**
   - `configs/scraping.yaml`からの詳細設定読み込み
